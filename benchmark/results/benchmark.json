{
  "entry_summaries": [
    {
      "raw_input": "Title: What do I test out / run first?\nID: t3_1kexdgy\nSummary:   \n               Just got her in the mail. Haven't had a chance\n            to put her in yet.    submitted by\n              \n            /u/Recurrents   [link]\n              [comments]\n            \nImageDescription: \nComment:       Just got her in the mail. Haven't had a chance to put her in yet.    submitted by    /u/Recurrents   [link]   [comments] \nComment: First run home. Preferably safely. \nComment: llama 3.2 1b \nComment: Bro is loaded. How many kidneys did you sell for that?! \nComment: LLAMA 405B Q.000016 \nComment: all the new qwen 3 models \nComment: sexy ass card \nComment: Old School Runescape \nComment: Are they selling those already? \nComment: Download cuda and make sure your pytorch is the cuda version \nComment: Can it run Crysis? \nComment: You bought it just to benchmark it, didn't you? \nComment: Hello Kitty Island Adventures, butters would be proud of you. \nComment: Would you mind sharing or DMing retailer info? I don‚Äôt have a preferred vendor and am curious on your experience. \nComment: Llama 3.3 70b at 8-bit. Would be interesting to see how many tokens per second gives. \nComment: Try Super Mario Bros ü•∏ \nComment: you don't need it. gimme that. \nComment: Rtx pro 6000 is 96Gb it is beast. Without pro is 48gb. I really want to know how many FOPS it is. Or the t/s for a deepseek 70B or largest model it can fit. \nComment: Get some silly concurrency going on qwen 3 32b awq and run the aider benchmark. \nComment: Flux to generate pics of your dream Audi. Find out your use case and try some models that fit. I was first impressed by GLM 4 in one shot coding, but it fails to use other tools. Mistral small is my daily driver currently. It's even fluent in most languages. \nComment: That‚Äôs some expensive computer hardware. Congratulations. \nComment: That‚Äôs our serial number now \nComment: Your power connectors. \nComment: i cant imagine spending that much money on a gpu with that power connector \nComment: Houston we have lift off  https://preview.redd.it/v3z4prno2wye1.png?width=780\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=6a6156b3fc0818b93b0459a14c86a0e0dd1d70d7 \nComment: https://preview.redd.it/5bnvabxayvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=9516acddbdda888267887c823c70c25db1ba8c6e New card installed! \nComment: Is it better then a h100 performance wise? i know the vram is slightly bigger. \nComment: Quake I \nComment: Everything. In all seriousness, I would reaaally like to see the benchmarks on that thing \nComment: Old school runescape \nComment: Cancer research. \nComment: OT, but run 3Dmark and confirm if it really is faster in games than the 5090 (for once in the history of workstation cards). \nComment: dude you are so lucky congrats!! run every qwen 3 model and make videos! i hear you stream, how about a live stream using llama.cpp and testing out models, or lm studio. this card is so awesome üòç \nComment: Something like Gemma 3 27B/Mistral small-3/Qwen 3 32B with maximum context size? \nComment: Qwen3 and don‚Äôt look back \nComment: Qwen 30B A3B q8 has something around 30 GB file size. Should run very fast and have plent of room for context. \nComment: Bios \nComment: Where did you buy it from? \nComment: About $12,000 to $16,000 for the 48gb vram editions here .. not sure we can get the 96gb \nComment: What CPU are you pairing with? Linux? \nComment: https://preview.redd.it/glj9rjmk9vye1.jpeg?width=1280\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=64d6eac13d0a6aaed4b500953bfd300dcea46322 \nComment: ü•∫ü•πüò≠ \nComment: Wan 2.1 fp 32 model \nComment: Crysis. \nComment: Haha I thought it had a plaid pattern printed on it üòÖ \nComment: Dude so cheesed, could've even wait to get home \nComment: Hey, I was looking to buy one as well, how much did you pay and how long did it take to arrive. They are releasing so many cards these days I get confused. \nComment: How much \nComment: what version is it? Max‚ÄìQ? Workstation edition? Etc‚Ä¶ \nComment: GTA V \nComment: Grounding strap. \nComment: Crysis \nComment: Plex Media Server. But make sure to hack your drivers. \nComment: Mate, share some benchmarks!  I‚Äôm about ready to pull the trigger on one too, but the price gouging here is insane. They‚Äôre still selling Ampere A6000s for 6‚Äì7K AUD, and the Ada version is going for as much as 12K.  Instead of dropping prices on the older cards, they‚Äôre just marking up the new Blackwell ones way above MSRP. The server variant of this exact card is already sitting at 17K AUD (~11K USD)‚Äîabsolute piss take tbh. \nComment: Image and clip generation \nComment: I think I'll stream getting some LLMs and comfyui up tomorrow and the next few days. give a follow if you want to be notified https://twitch.tv/faustcircuits \nComment: Get that unsloth 235B Qwen3 model at Q2K_XL. It should fit. Q2 is the most efficient size when it comes to benchmark score to size ratio according to unsloths documentation. It should be fast AF too since only 22B active parameters.  \nComment: Nice! Still waiting for mine. Can you let me know if you are able to disable ECC or not? \nComment: what Audi is that? S4? \nComment: Nice. Run stuff and share stats! Would be cool to see. \nComment: https://preview.redd.it/ems9w2z6yvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=76b13f186be7cb783727c000bda533c92c1e8c56 here is the old card lol \n",
      "results": {
        "title": "What do I test out / run first?",
        "id": "t3_1kexdgy",
        "overview": "This post features a user who recently received a high-end GPU (likely an RTX Pro 6000 or similar) and is seeking recommendations for initial tests. The discussion revolves around running large language models (LLMs) like Llama 3.2, Qwen 3, and Mistral, with technical details about VRAM requirements (e.g., 'LLAMA 405B Q.000016', 'Qwen 3 32B AWQ'), benchmarking considerations ('tokens per second', 'FOPS'), and hardware compatibility ('CUDA version', 'ECC disable'). Comments also mention specific use cases like 'Crysis' gaming benchmarks, '3DMark', and AI workloads such as 'image generation' and 'coding tasks'. However, the post itself does not announce any new technology or model release‚Äîonly user suggestions for testing procedures.",
        "comment_overview": "The community is highly engaged in suggesting LLMs and workloads to test the GPU, with enthusiasm for benchmarks and performance comparisons. Users debate model efficiency ('Q2K_XL', 'A3B q8'), hardware limitations ('96GB vs 48GB VRAM'), and practical applications ('Cancer research', 'Plex Media Server'). Some comments highlight concerns about pricing ('price gouging', '17K AUD server variant') and technical hurdles ('disabling ECC'). While the discussion contains technical depth, it remains speculative and focused on user experiences rather than novel developments.",
        "link": "",
        "relevance": "",
        "is_relevant": false,
        "thumbnail_url": ""
      },
      "processing_time_ms": 19348
    },
    {
      "raw_input": "Title: Visa is looking for vibe coders - thoughts?\nID: t3_1keolh9\nSummary:   \n                \n            submitted by    /u/eastwindtoday\n              [link]\n              [comments]\n            \nImageDescription: ## Image Analysis: Visa \"Vibe Coder\" Job Posting Detail\n\nThe image presents a screenshot of text, likely extracted from a job posting by Visa. The post is soliciting candidates with specific technical skills, framed as \"vibe coders.\" It appears to be a section detailing desired experience.\n\nThe text is formatted as a bulleted list, outlining requirements and preferred qualifications. Key technologies explicitly mentioned include:\n\n*   **Vector Databases:** Pinecone, ChromaDB are named. This indicates a focus on semantic search and retrieval using vector embeddings.\n*   **Embedding Models:** The posting specifically requests experience with embedding models, suggesting the role involves working with representations of data for similarity comparisons.\n*   **Containerization:** Docker is explicitly listed, indicating a need for experience in packaging and deploying applications within containers. Kubernetes is also mentioned, implying familiarity with container orchestration at scale.\n*   **Frontend Tools:** Lovable and V0 are listed, suggesting a preference for modern frontend development tools. The specific nature of these tools isn't clear without further context, but they likely relate to component libraries or UI frameworks.\n*   **Data Science:** Experience in Data Science is considered a \"big plus,\" suggesting the role may involve data analysis or model building.\n*   **Problem Solving:** Strong problem-solving skills are explicitly requested, highlighting the need for analytical thinking and adaptability.\n\nThe bottom of the image contains a blurred section with non-English characters, which are illegible. A small Reddit logo is visible in the bottom right corner, indicating the source of the screenshot is likely a Reddit post. The \"technical\" flair tag further confirms the discussion revolves around technical aspects of the job posting.\n\nOverall, this excerpt points to a role requiring expertise in modern data infrastructure (vector databases, embeddings), DevOps practices (Docker, Kubernetes) and frontend development. The \"vibe coder\" framing suggests a potentially innovative or experimental team within Visa, focused on leveraging these technologies\nComment:         submitted by    /u/eastwindtoday   [link]   [comments] \nComment: The way it's written with the other requirements makes me think that they want a programmer able to write vibe coding tools, not specifically hiring a vibe coder for programming. \nComment: \"Background in Data Science is a big plus\" - Be prepared to be payed less than a proper data scientist. \nComment: Dumb. Vibe coding is a bit ill-defined but what I see is a niche where people like product managers can use AI to build tiny tools that would be hard to get into a product team's priority list. In particular what I saw was using Zapier which is a user-friendly automation tool that lets you run snippets of Python or js inside workflows and now lets you generate them with AI. Great way to build little utilities without a developer but also very narrow, low stakes applications that aren't critical path. This job req wants familiarity with vector DBs and containers which means they want an actual experienced software engineer. Someone who can use a coding assistant but probably doesn't need one. That's not vibe coding. \nComment: Vibe coding 2025 is same as slapping an untested, ill designed, 700-lines-a-function Python script on anything and calling it a day in the 2015s Edit: You scrolled down on some requirements tho https://www.visa.ca/fr_ca/jobs/REF061638W \u0026gt; Strong proficiency with Python, FastAPI and PostgreSQL for backend development. \nComment: These days, stupid HRs are stuffing the job requirements sections with as many tech buzzwords as possible, regardless of whether these buzzwords actually reflect the technologies used in the company. I feel like \"vibe coding\" may be one of those buzzwords. \nComment: now exactly nobody will know why credit card applications failed \nComment: Maybe we should all \"vibe-pay\" our Visa card bills. \nComment: Click click, sold all visa stocks instantly \nComment: I for one am happy my CC transactions are handled by vibed up systems. \nComment: they are starting up an ml intership for students next summer in poland too \nComment: I don't even know what to think. My general reaction of most job posts is to assume the companily already has someone they're gonna hire, and the position is only being posted as a formality and to make it appear that the company has fair hiring practices (even though requirements weed out people that are perfectly capable of being trained). \nComment: I know some folks working with such teams. They have ambitions to build vibe coding tools (like cursor) but without the dependency on letting data go out of the org (from a financial reg pov). Visa‚Äôs CTO is one of the highest paid CTOs in the US, and they are building agents replicating (gamma.ai, OpenAI deep research, Agentic Commerce) to start with. \nComment: They are even Vibe-using Caps. That's so Vibe. It gives me Bad Vibes. \nComment: That job posting is clear as mud. Title is an obvious mismatch for the skillset. \nComment: well I'm glad none of my credit cards are visa...  \nComment: what could possibly go wrong? \nComment: If they are listing the tech stacks that should be enough. \nComment: At least they're not requiring minimum 15 years of experience \nComment: Lmao \nComment: AI generated job posting ? :p \nComment: Vibe coding doesn't work. It takes me 10-20 prompts to generate anything useful with gemini 2.5.  Sometimes 30+ prompts even with explicit instructions. \nComment: And I thought it was a big deal 15 years ago when I talked NASA into letting me put ‚Äúauto-generated‚Äù guidance and control code into a satellite. That particular NASA center had never done such a thing, and I had to pop their cherry on it (as a contractor). Partly because a colleague of mine was doing the same thing on a bigger project, and he really wanted to be able to say his project wasn‚Äôt the first. The ‚Äúauto-generation‚Äù step is just compiling MatLab down into C code, and from there is just standard C compilation on the target environment. But the MatLab becomes the source code, in this case. And you have to version-control the auto-generation configuration as well.  Honestly, with all the other shit we had to deal with on that program, just telling an AI to ‚Äúwrite some code that might look like spacecraft guidance and control software, no particular requirements because we‚Äôre don‚Äôt have the money to actually test the hardware before launch, and most of it is going to fail anyway‚Äù would have been such a stress-saver. That‚Äôs a good vibe. \nComment: As someone already posted, these are not the full requirements, but another thing also:  Vibe-coding is expensive. Requests can quickly become $1-2 a piece. Over the course of a month, you could easily rack up $500+ if you're using it liberally. Now consider that you're already paying a regular salary, put this on top, and it becomes kind of unsustainable.  The money has to come from somewhere, and it's most probably the base pay. \nComment: I don't know much about developers as vibe coders. But, a lot of project managers \u0026amp; UI UX folks and product managers are starting to learn these tools (lovable, cursor, etc.,) and will roll out the basic screens for evaluation.  Also, this will start getting back to actual developers on justifying their timelines, complexity justifications, etc., This is mostly as nobody gives a fuck about code maintainability, reusability, tech debt, security, etc,.  Other side to this coin is 90% of code written by \u0026lt;5 years experienced devs are not following these standards anyways ü§∑‚Äç‚ôÇÔ∏è 90% of products from start-ups and other IT companies never see the light anyways ü§¶‚Äç‚ôÇÔ∏èü§£ \n",
      "results": {
        "title": "Visa's 'Vibe Coder' Job Posting Sparks Debate Over AI-Driven Development Trends",
        "id": "t3_1keolh9",
        "overview": "Visa's job posting for 'vibe coders' has ignited discussions about the evolving role of AI in software development. The listing explicitly requires expertise in vector databases (Pinecone, ChromaDB), embedding models, containerization (Docker/Kubernetes), and frontend tools like Lovable/V0. While the term 'vibe coder' remains ambiguously defined, the technical requirements suggest a focus on modern AI infrastructure: semantic search systems, scalable DevOps pipelines, and AI-assisted development workflows. The mention of Python, FastAPI, and PostgreSQL further indicates a backend engineering role, raising questions about whether the title aligns with actual responsibilities. Notably, the posting highlights 'strong problem-solving skills' alongside AI tools, implying a hybrid approach where human oversight remains critical. This reflects broader industry trends toward integrating AI assistants into software development, though the practicality of 'vibe coding'‚Äîas described in comments‚Äîremains debated.",
        "comment_overview": "The community reaction mixes skepticism and curiosity. Many users questioned the legitimacy of 'vibe coder' as a role, with some arguing it's a buzzword-laden title masking traditional software engineering requirements. Critics highlighted the disconnect between AI-assisted development and the posting's emphasis on containerization, data science, and complex backend systems. Others speculated that Visa might be exploring AI-driven tooling for internal workflows, potentially mirroring projects like Cursor or Gamma.ai. A recurring concern was the potential for AI tooling to depress salaries, with one commenter noting that 'background in Data Science is a big plus' could signal lower compensation. The discussion also touched on the broader implications of AI in software development, with some recalling past challenges of 'auto-generated' code and others joking about the absurdity of 'vibe-paying' credit card bills.",
        "link": "",
        "relevance": "",
        "is_relevant": true,
        "thumbnail_url": ""
      },
      "processing_time_ms": 18101
    },
    {
      "raw_input": "Title: UI-Tars-1.5 reasoning never fails to entertain me.\nID: t3_1keo3te\nSummary:   \n             \n             7B\n            parameter computer use agent.   \n            submitted by   \n            /u/Impressive_Half_2819   [link]\n              [comments]\n            \nImageDescription: ## Image Analysis: UI-Tars-1.5 Reasoning Interface\n\nThe image displays a screenshot of a user interface (UI) seemingly built for interacting with a Large Language Model (LLM), specifically identified as \"UI-Tars-1.5\". The UI is presented in a light theme, resembling a macOS application window.\n\n**Key Components:**\n\n1. **LLM Provider and Model Selection:** A prominent section labeled \"LLM Provider and Model\" allows the user to select a pre-defined LLM or specify a \"Custom model‚Ä¶\". A dropdown menu is visible, currently displaying the selected custom model: ‚ÄúByteDance-Seed/UI-Tars-1.5-7B‚Äù. Below this is a text input field labeled \"Provider Base URL\", likely used to specify the API endpoint for accessing the chosen model.\n\n2. **UI-TARS Thoughts:** A text box titled \"UI-TARS Thoughts\" displays the LLM's reasoning process. The displayed text indicates a conversational context where the user asked about Google‚Äôs purpose, and UI-Tars is responding with a verbose explanation. The LLM states it's attempting to explain the purpose of Google and is about to initiate a search for a repository, suggesting an agent-like behavior with tool use (web searching). The text includes downward pointing arrows (\"‚Üì\") indicating further output is likely being generated.\n\n3. **Textbox Input:** A \"Textbox\" field at the bottom of the interface is labeled ‚ÄúAsk me to perform tasks in a virtual macOS environment‚Äù. This suggests the LLM has capabilities beyond simple text generation, potentially including interacting with a simulated macOS environment.\n\n**Technical Observations:**\n\n*   The model name \"ByteDance-Seed/UI-Tars-1.5-7B\" indicates a 7 billion parameter model developed by ByteDance, likely hosted on Hugging Face or a similar platform.\n*   The UI design suggests an\nComment:       7B parameter computer use agent.    submitted by    /u/Impressive_Half_2819   [link]   [comments] \nComment: What's more important here is the model used - ByteDance-Seed/UI-TARS-1.5-7B the model which it is meant to be used with, so how did you make it work? Because last time I checked I haven't seen that model being converted to GGUF format, nor having vision support added into llama.cpp for it. \nComment: I guess: https://github.com/trycua/cua \nComment: When you train a model to use computers for humans and do the tiresome ToS reading, but it can't be bothered to do it either \nComment: Most probably trained on Gen-Z data. \nComment: https://preview.redd.it/4ignwtxrhuye1.jpeg?width=1079\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=e5d30270b51854c061aebdd502448897c63fed18 \nComment: On one hand, I guess I'd like the language model to read language on my behalf - on the other hand I wouldn't want the model to decide the cookies policy warrants user review or some other distraction so maybe skipping it is for the best after all. It does seem reading the pop-up falls within the scope of accessing the site to search for a repository \nComment: Try out yourself using cu/a! \nComment: I mean, fair \nComment: tiktok ai getting lazy \nComment: TARS, would you set your attention span setting to 8 for me? \nComment: It‚Äôs the defaut personality? \n",
      "results": {
        "title": "UI-Tars-1.5 reasoning never fails to entertain me.",
        "id": "t3_1keo3te",
        "overview": "This post showcases a custom user interface (UI) for interacting with ByteDance's 7B-parameter language model, 'UI-Tars-1.5'. The UI enables users to select LLM providers, input custom model URLs, and observe the model's 'reasoning process' in a dedicated textbox. Notably, the interface suggests agent-like capabilities, such as initiating web searches and simulating interactions with a macOS environment. Technical details reveal the model is likely hosted on Hugging Face, with community speculation about its training data (possibly 'Gen-Z data') and compatibility with tools like llama.cpp. The 7B parameter count aligns with mid-sized LLMs, but the UI's novel features‚Äîsuch as visible 'thought processes' and environment interaction‚Äîhighlight a push toward more transparent and functional LLM interfaces. While no benchmarks are provided, the integration of tool-use capabilities (e.g., web searching) suggests practical applications for task automation.",
        "comment_overview": "The community is intrigued by the model's apparent agent-like behavior but raises technical questions. Users debate whether ByteDance-Seed/UI-Tars-1.5-7B has been adapted to GGUF format for local execution, noting that llama.cpp support is missing. Others humorously speculate about the model's 'Gen-Z data' training and its ability to avoid tedious tasks like reading ToS agreements. A link to a GitHub repository (cua) sparks interest in testing the interface, while discussions about 'attention span settings' reflect playful engagement. Critics question the model's practicality without vision support or performance metrics, but the post underscores growing interest in customizable LLM interfaces that bridge text generation and actionable automation.",
        "link": "",
        "relevance": "",
        "is_relevant": true,
        "thumbnail_url": ""
      },
      "processing_time_ms": 16506
    },
    {
      "raw_input": "Title: Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS,\n            Android, Mac, Windows)\nID: t3_1kepuli\nSummary:   \n             \n             Hey\n            LocalLlama! We've started publishing open-source model\n            performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS,\n            Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+\n            soon. We‚Äôre doing this because perf metrics determine the viability\n            of shipping models in apps to users (no end-user wants crashing/slow AI features that\n            hog up their specific device). Although benchmarks get posted in\n            threads here and there, we feel like a more consolidated and standardized hub should\n            probably exist. We figured we'd kickstart this since we\n            already maintain this benchmarking infra/tooling at RunLocal for our enterprise\n            customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and\n            TFLite to date, so a few things are still WIP for GGUF support. \n            Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill\n            Tokens=512, Num Generation Tokens=128). GGUFs\n            are from Unsloth üêê Qwen3\n            GGUF benchmarks on laptops Qwen3\n            GGUF benchmarks on phones You can see more of the benchmark\n            data for Qwen3 here.\n            We realize there are so many variables (devices, backends, etc.) that interpreting the\n            data is currently harder than it should be. We'll work on that!\n            You can also see benchmarks for a few other models here. If\n            you want to see benchmarks for any others, feel free to request them and we‚Äôll try to\n            publish ASAP! Lastly, you can run your own benchmarks on our devices\n            for free (limited to some degree to avoid our devices melting!). This\n            free/public version is a bit of a frankenstein fork of our enterprise product, so any\n            benchmarks you run would be private to your account. But if there's interest, we\n            can add a way for you to also publish them so that the public benchmarks aren‚Äôt\n            bottlenecked by us.  It‚Äôs still very early days for us with this, so\n            please let us know what would make it better/cooler for the community: https://edgemeter.runlocal.ai/public/pipelines\n            To more on-device AI in production! üí™ https://i.redd.it/aev5rgjazsye1.gif\n               submitted by    /u/intofuture \n             [link]\n              [comments]\n            \nImageDescription: ## Image Description: Qwen3 Performance Benchmarks\n\nThe image presents a detailed performance comparison of the Qwen3 language model across approximately 50 different devices, categorized by backend (CPU, Metal, and Android). The data is structured in a tabular format.\n\n**Table Structure:**\n\nThe table has columns representing:\n*   **Device:** Lists various devices, including iPhone models (e.g., iPhone 15 Pro Max, iPhone 14), iPad models, Mac devices (e.g., M3 Max, M2 Pro), Windows laptops (e.g., Surface Laptop 5), and Android phones (e.g., Pixel 8 Pro, Samsung Galaxy S24 Ultra).\n*   **Backend Count:** Indicates the number of model layers processed by each backend (CPU or GPU).\n*   **RAM:** Specifies the RAM usage in GB.\n*   **Toks/s:**  Represents tokens per second, a key metric for inference speed.\n*   **Time (ms):**  Indicates the latency in milliseconds, likely for generating a specific number of tokens.\n*   **RAM (mb):** RAM usage in megabytes, likely a redundant column duplicating the \"RAM\" information.\n\n**Data Observations:**\n\n*   **Backends:** The majority of devices utilize either CPU or Metal (Apple's GPU framework). Android devices also show performance data.\n*   **Performance Variation:** Significant variation in Toks/s is observed across devices. High-end Macs (M3 Max) demonstrate the highest Toks/s values, exceeding 600. iPhones and iPads show varying performance depending on the model (e.g., iPhone 15 Pro Max at ~382 Toks/s). Android devices generally show lower performance than comparable Apple silicon devices.\n*   **RAM Usage:** RAM usage correlates with backend count; higher layer counts generally require more RAM.  \n\nExternal URL Summaries:\n- https://huggingface.co/collections/unsloth/qwen3-680edabfb790c8c34a242f95: \n\n\n\nThe article introduces the Qwen3 model collection by unsloth on Hugging Face, featuring various versions of Qwen3 with different sizes (from 0.6B to 235B parameters) and formats, including GGUF, 4-bit, and 16-bit safetensors. It highlights models with up to 128K context length, dynamic 4-bit safetensors for fine-tuning and deployment, and FP8 versions. Each model includes details like update dates, download counts, and stars.\nComment:       Hey LocalLlama! We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon. We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device). Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist. We figured we'd kickstart this since we already maintain this benchmarking infra/tooling at RunLocal for our enterprise customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and TFLite to date, so a few things are still WIP for GGUF support.  Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill Tokens=512, Num Generation Tokens=128). GGUFs are from Unsloth üêê Qwen3 GGUF benchmarks on laptops Qwen3 GGUF benchmarks on phones You can see more of the benchmark data for Qwen3 here. We realize there are so many variables (devices, backends, etc.) that interpreting the data is currently harder than it should be. We'll work on that! You can also see benchmarks for a few other models here. If you want to see benchmarks for any others, feel free to request them and we‚Äôll try to publish ASAP! Lastly, you can run your own benchmarks on our devices for free (limited to some degree to avoid our devices melting!). This free/public version is a bit of a frankenstein fork of our enterprise product, so any benchmarks you run would be private to your account. But if there's interest, we can add a way for you to also publish them so that the public benchmarks aren‚Äôt bottlenecked by us.  It‚Äôs still very early days for us with this, so please let us know what would make it better/cooler for the community: https://edgemeter.runlocal.ai/public/pipelines To more on-device AI in production! üí™ https://i.redd.it/aev5rgjazsye1.gif    submitted by    /u/intofuture   [link]   [comments] \nComment: Iphone 16's Metal performance is pretty impressive for 1.6b-q8. But I do wonder why q8's performance is faster than q4 in that particular setup. \nComment: It‚Äôs interesting to see that performance in m4 is pretty similar in both cpu and gpu \nComment: There's one edge factor you missed - on Metal backend when you get OOM you get completely wrong results. For example on Qwen3 8B Q4 your results are like this: - MacBook Pro M1, 8GB = 99232.83tok/s prefill, 2133.70tok/s generation - MacBook Pro M3, 8GB = 90508.66tok/s prefill, 2507.50tok/s generation If you wouldn't get OOM the correct results for that model should be around ~100-150tok/s prefill and ~10tok/s generation. Additionally, all results for RAM usage on Apple silicon \u0026amp; Metal are not correct. In terms of your UX/UI there's tons of stuff that should be improved. but to not make this into very long post I'll write about biggest problems that can be fixed rather easily. First, add option to hide columns, there's too much redundant information that should be possible to hide with just couple of clicks. Second, decide on some naming scheme for components and stick with it. https://preview.redd.it/8idjxghteuye1.png?width=91\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=12d1d9317495b745768b9dd012288be1eb804964 I would suggest to get rid of 'Apple'/'Bionic' names altogether - it just adds to complexity and cognitive load to a table that is already very dense. There is no non-Apple M1 in Macbooks or non-Bionic A12 in iPad, so you don't need to clarify that much in a first place and additionally this page is aimed at technical people. Exact same problem with Samsung/Google vs Snapdragon. Third, if both CPU and Metal failed don't create two entries. Table is 2x longer than it should be with results that are non-comparable to anything. Just combine it into one entry. \nComment: How to run on metal on iphone 16 pro? I have pocketpal app and how to switch from cpu to metal? \nComment: if i'm reading this correctly the load time on cpu is better than gpu/metal for macbook pro but the gpu/metal is less memory intensive? also metal perf on iphone 16 is pretty impressive. \nComment: How do I run this on Android? Rn it just crashes \nComment: Why is Q8 faster than Q4??? \nComment: For laptops, is vulkan using the igpu ? \nComment: according to this data on iphone 16 you have 24 t/s on Q8 and 22 t/s on Q4 why so tiny models? \nComment: The iPhone 16e is listed to have the A18 Pro SoC but it actually has the A18.  https://preview.redd.it/h1dx2hgphvye1.png?width=623\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c2f8794bf27c6a9042b81040ebefa09873eae989 \n",
      "results": {
        "title": "Qwen3 Performance Benchmarks Across 50+ Devices (iOS, Android, Mac, Windows)",
        "id": "t3_1kepuli",
        "overview": "This post introduces an open-source initiative to benchmark Qwen3's performance across ~50 devices, focusing on tokens per second (toks/s), RAM utilization, and backend configurations (CPU, Metal, Android). The data highlights significant performance variations: high-end Macs (M3 Max) achieve \u003e600 tok/s, while iPhones and iPads show lower but consistent results. Android devices lag behind Apple silicon counterparts, likely due to hardware/OS optimizations. The project emphasizes the importance of device-specific metrics for on-device AI deployment, noting that slow or resource-heavy models risk user dissatisfaction. Technical details include GGUF format benchmarks from Unsloth, with plans to expand support for Core ML, ONNX, and TFLite. The initiative also allows public users to run limited benchmarks via a 'frankenstein fork' of their enterprise tooling, though data interpretation remains challenging due to variable backend configurations and redundant metrics like RAM (GB) vs. RAM (MB).",
        "comment_overview": "The community praises the initiative's ambition but raises technical concerns. Users note anomalies like Q8 models outperforming Q4 variants on iPhones, questioning quantization efficiency. A critical comment highlights Metal backend OOM (out-of-memory) issues causing incorrect results, while others criticize redundant data columns and unclear device naming conventions (e.g., 'Apple' vs. 'Bionic'). Practical questions about switching to Metal on iOS and Android crashes underscore usability gaps. Despite this, the project is seen as a valuable step toward standardized on-device AI benchmarks, with suggestions for UI improvements like column toggling and unified device naming.",
        "link": "",
        "relevance": "",
        "is_relevant": true,
        "thumbnail_url": ""
      },
      "processing_time_ms": 22537
    },
    {
      "raw_input": "Title: Qwen 30B A3B performance degradation with KV quantization\nID: t3_1kewkno\nSummary: \n            I came\n            across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4\n            that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M\n            quantization. I tried to replicate locally but could I was not able,\n            model sometimes entered in a repetition loop even with dry sampling or came to wrong\n            conclusion after generating lots of thinking tokens. I was using\n            Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested\n            Bartowski Q5_K_S but it had no improvement. The model didn't entered in any\n            repetition loop but generated lots of thinking tokens without finding any\n            solution. Then I saw that sunpazed didn't used KV\n            quantization and tried the same: boom! First time right. It worked\n            with Q5_K_S and also with Q4_K_XL For who wants more details I leave\n            here a gist https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef\n            Do you have any report of performance degradation with long generations on\n            Qwen3 30B A3B and KV quantization?   \n            submitted by   \n            /u/fakezeta   [link]\n              [comments]\nImageDescription: \n\nExternal URL Summaries:\n- https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4: \n\n\n\nThe article discusses the performance of Qwen3 30B A3B (4_0 quant) in solving a cipher problem first introduced in the OpenAI o1-preview technical paper. The model successfully solved the cipher in 5 minutes, significantly faster than QwQ, which took 32 minutes. The article includes detailed logs of the model's initialization and execution on an Apple M4 Pro GPU, highlighting its technical specifications and efficiency.\nComment: I came across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4 that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization. I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens. I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens without finding any solution. Then I saw that sunpazed didn't used KV quantization and tried the same: boom! First time right. It worked with Q5_K_S and also with Q4_K_XL For who wants more details I leave here a gist https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef Do you have any report of performance degradation with long generations on Qwen3 30B A3B and KV quantization?    submitted by    /u/fakezeta   [link]   [comments]\nComment: What KV quant level were you using? IMO on llama.cpp you shouldn‚Äôt push it past Q8_0. Q4_0 cache quant tanks quality in any model and especially models that heavily leverage GQA.  \nComment: I have one rule: I always test ALL new models without flash attention and with full 16bit KV cache. \nComment: Which KV quantization are you using? Don't have time to run this test right now, but I usually use -ctk q8_0 -ctv q5_1 (requires -DGGML_CUDA_FA_ALL_QUANTS=on) \nComment: Ive only tried KV quantization once and saw that any amount of it makes models super dumb. Not sure why anybody uses it tbh \nComment: Which quantization did you use initially? \nComment: Interested here since I'm running a q6 \nComment: Use these parameters: Thinking Mode Settings: Temperature = 0.6 Min_P = 0.0 Top_P = 0.95 TopK = 20 Non-Thinking Mode Settings: Temperature = 0.7  Min_P = 0.0 Top_P = 0.8 TopK = 20 \nComment: Could you please tell us how to disable KV cache quantisation? I'd also like to check the difference. What is the difference in the amount of memory used with KV running at fp16 in comparison with regular q4? \nComment: I‚Äôm confused. Isn‚Äôt K_M KV quantization? And yet you said Qwen 30b solved the rest with Q4 K_M? \nComment: Of course  Cache should always be fp16 even Q8 has degradation. Only flash attention is ok...ish ( as is fp16 ) \n",
      "results": {
        "title": "Qwen 30B A3B Performance Degradation with KV Quantization",
        "id": "t3_1kewkno",
        "overview": "A user reported significant performance issues with Qwen 30B A3B when using KV (key-value) quantization, particularly with Q4_K_M and Unsloth Q4_K_XL formats. The model exhibited repetition loops or incorrect solutions during long-generation tasks like solving the OpenAI cipher test, whereas disabling KV quantization (e.g., using Q5_K_S or Q4_K_XL without KV) resolved the issues. Technical details from Gists show that KV quantization reduces memory usage but sacrifices precision, leading to degraded reasoning capabilities. The discussion highlights the tradeoff between efficiency and accuracy in quantized models, with users noting that FP16 KV caches maintain quality but consume more memory. This underscores the importance of quantization choices for tasks requiring sustained logical reasoning.",
        "comment_overview": "The community emphasized caution with KV quantization, with users warning that Q4_0 cache quantization severely impacts model quality, especially for architectures relying on GQA (Grouped Query Attention). Many recommended avoiding KV quantization altogether or using Q8_0 for balance. Discussions highlighted conflicting experiences: some users found KV quantization 'super dumb,' while others noted that FP16 KV caches are essential for maintaining performance. A recurring theme was the need to test models with full 16-bit KV caches for critical tasks, contrasting with the tradeoffs of lower-precision quantization for efficiency.",
        "link": "",
        "relevance": "",
        "is_relevant": true,
        "thumbnail_url": ""
      },
      "processing_time_ms": 15114
    },
    {
      "raw_input": "Title: QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.\nID: t3_1kenk4f\nSummary:   \n               All models are from Bartowski - q4km version\n            Test only HTML frontend. My assessment lauout quality from 0\n            to 10 Prompt \"Generate a beautiful\n            website for Steve's pc repair using a single html script.\"\n            QwQ 32b - 3/10 - poor layout but ..works , very\n            basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b\n            Qwen 3 32b - 6/10 - much better looks but still not too\n            complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b272399ba8140785feb429a196ecc5173\n            GLM-4-32b 9/10 - looks insanely good , quality layout like\n            sonnet 3.7 easily - 1500+ code lines https://preview.redd.it/3zj2lr2ahsye1.png?width=2469\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964\n            GLM-4-32b is insanely good for html code frontend. I say\n            that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most.\n            Other coding language like python , c , c++ or any other quality\n            of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on\n            the seme level but for html and JavaScript ... is GREAT.\n               submitted by   \n            /u/Healthy-Nebula-3603   [link]\n              [comments]\n            \nImageDescription: ## Image Description: QwQ, Qwen \u0026 GLM-4 HTML Coding Comparison\n\nThe image presents a screenshot of a webpage likely generated by Large Language Models (LLMs) ‚Äì specifically, QwQ 32b, Qwen 3 32b, and GLM-4-32B ‚Äì as indicated by the post title. The webpage's content appears to be a basic HTML structure demonstrating each model‚Äôs ability to generate code for a simple \"Our Services\" section.\n\nThe page is divided into three distinct sections, each presumably representing the output from one of the LLMs. Each section contains a heading \"Our Services\" followed by two sub-sections: ‚ÄúRepair Shop Repair‚Äù and ‚ÄúVirus Removal‚Äù. \n\nEach sub-section contains a brief description. \"Repair Shop Repair\" states: ‚ÄúRepair any part of all computer components.‚Äù  ‚ÄúVirus Removal‚Äù describes: \"Complete malware and virus removal.\"\n\nThe core of the comparison lies in how each model structures the HTML code for these sections. While visually similar, subtle differences likely exist in the tags used (e.g., `\u003cdiv\u003e`, `\u003cp\u003e`, `\u003cspan\u003e`), class names, and overall code formatting. The image doesn‚Äôt show the actual HTML source code; it only displays the rendered output.\n\nNotably, a large block of seemingly random characters (non-English script) is present under both sub-sections. This appears to be a failure case or an artifact of the LLM generation process, potentially indicating issues with handling non-ASCII characters or a hallucination. The repetition of this character block suggests it's not intended content and likely represents an error in the model‚Äôs output.\n\nThe overall UI is minimalistic, with a white background and simple text formatting. The focus is clearly on the generated HTML content rather than any elaborate design elements, reinforcing the purpose of this comparison. The image is a direct visual representation of the LLM's ability to\nComment:   \n               All models are from Bartowski - q4km version\n            Test only HTML frontend. My assessment lauout quality from 0\n            to 10 Prompt \"Generate a beautiful\n            website for Steve's pc repair using a single html script.\"\n            QwQ 32b - 3/10 - poor layout but ..works , very\n            basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b\n            Qwen 3 32b - 6/10 - much better looks but still not too\n            complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b272399ba8140785feb429a196ecc5173\n            GLM-4-32b 9/10 - looks insanely good , quality layout like\n            sonnet 3.7 easily - 1500+ code lines https://preview.redd.it/3zj2lr2ahsye1.png?width=2469\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964\n            GLM-4-32b is insanely good for html code frontend. I say\n            that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most.\n            Other coding language like python , c , c++ or any other quality\n            of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on\n            the seme level but for html and JavaScript ... is GREAT.\n               submitted by   \n            /u/Healthy-Nebula-3603   [link]\n              [comments]\n            \nComment: Yeah, I\n            have also tried to generate webpages with a couple of models, like GLM-4, Qwen3, Phi-4\n            Reasoning, etc. GLM-4 is so far the clear winner at these tasks. It's a gem in\n            my model collection. \nComment: I've created a list of one-shot generated HTML\n            pages using different models, big and small. https://blog.kekepower.com/ai/\n            \nComment: Ironically, so far userscript (javascript): Qwen 3 32B\n            \u0026gt; GLM-4-0414. Don't get me wrong. I love GLM-4-0414, but it feels like it\n            lacked the required understanding for my particular requests that Qwen 3 32B understood\n            well. \nComment: Whats the\n            temp? Did you rerun multiple times? \nComment: Why not\n            try a slightly more complex task? E.g. a mini-game? \n            Create a single-HTML-page game using Babylon.js where the player controls a\n            ship and moves about the open sea exploring islands to find treasure. A single, small\n            map with 3 islands of which only one has the treasure is enough\n             In the first reply, the camera works, but WASD\n            didn't. I copied-pasted the errors from the console a couple times and WASD\n            works now. It looks terrible, but I guess that's expected without external\n            assets. https://preview.redd.it/lf5foomq1tye1.png?width=1288\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b02ed407b3fbef31bf6d83aae5491cacaa93e150\n            Edit: This is my AWQ quant of Qwen-32B. \nComment: GLM falls\n            flat on its face when I try to continue developing after the first prompt.\n            It feels like a model trained (very well) for one-shots\n            \nComment: Try\n            UIGEN-T2 for html generation! There's also a react model. \n            https://huggingface.co/Tesslate/UIGEN-T2\n            \nComment: Would be\n            interesting to see how these results compare to the recent Tesslate/UIGEN-T2-7B.\n            It's a tuned version of Qwen 2.5 Coder 7B specifically for UI\n            generation. \nComment: What\n            quants what context window size? Ollama default size will kill QWQ reasoning if you\n            don‚Äôt know how to set it up properly. \nComment: Still\n            kind of freaked out that smaller Qwen 3 models are probably as good at website\n            development as I was as a teenager. And a damn sight quicker too.\n            \nComment: Thanks\n            for posting the eval. Would be curious to see the prompt used as well.\n            \nComment: Interesting. I ran the same input on QwQ with these\n            settings: Temp: 0.6 Top p: 0.95 Min p: 0.0 Top k: 40 And quite a bit different\n            output. Output: https://pastebin.com/Ntc8QQfH\n            \nComment: GPT 4.1\n            is clearly the winner here in my opinion as well as claude sonnet 3.7\n            \n",
      "results": {
        "title": "QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison",
        "id": "t3_1kenk4f",
        "overview": "This post presents a detailed comparison of three large language models‚ÄîQwQ 32b, Qwen 3 32b, and GLM-4-32B‚Äîfocusing on their ability to generate HTML code for a 'Steve's PC Repair' website. The evaluation uses a standardized prompt and assesses layout quality on a 0-10 scale, along with code complexity metrics. GLM-4-32B scores highest (9/10) with a 1,500+ line HTML output featuring a 'sonnet 3.7-level' layout, while QwQ 32b struggles with basic structure (3/10) and Qwen 3 32b shows moderate improvement (6/10). The analysis highlights GLM-4-32B's specialization in frontend development, with the author noting its HTML/JS capabilities far exceed other languages like Python or C++. Technical details include quantization (q4km version), code line counts, and explicit comparisons to other models like Phi-4 and Claude Sonnet 3.7. The results underscore the importance of task-specific model tuning, as GLM-4-32B's performance suggests it may be optimized for structured code generation tasks.",
        "comment_overview": "The community reaction is mixed but largely acknowledges GLM-4-32B's superiority in HTML generation. Users confirm its 'gem-like' performance for frontend tasks, though some note limitations in JavaScript complexity and multi-step reasoning. Discussions reveal skepticism about the 'one-shot' nature of the tests, with requests for more complex tasks like game development using Babylon.js. Critics point out GLM-4-32B's struggles with iterative development and non-English character handling, while others highlight Qwen 3 32b's better reasoning in specific scenarios. The thread also sparks interest in alternative tools like UIGEN-T2 and raises questions about quantization settings affecting model performance. Overall, the post sparks productive debate about model specialization and the need for task-specific benchmarks.",
        "link": "",
        "relevance": "",
        "is_relevant": true,
        "thumbnail_url": ""
      },
      "processing_time_ms": 23247
    },
    {
      "raw_input": "Title: LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!\nID: t3_1keoint\nSummary:   \n               You\n            can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B\n            and 30B) mainline\n            llama.cpp just got a boost for fully offloaded Qwen3 MoE (single\n            expert) tl;dr; I highly recommend\n            doing a git pull and re-building your\n            ik_llama.cpp or llama.cpp repo to take\n            advantage of recent major performance improvements just released. The\n            friendly competition between these amazing projects is producing delicious fruit for the\n            whole GGUF loving r/LocalLLaMA community! If\n            you have enough VRAM to fully offload and already have an existing\n            \"normal\" quant of Qwen3 MoE then you'll get a little more\n            speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take\n            advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork!\n            Details I spent yesterday compiling and running benhmarks\n            on the newest versions of both ik_llama.cpp and\n            mainline llama.cpp.\n            For those that don't know, ikawrakow was an early contributor to\n            mainline llama.cpp working on important features that have since trickled down into\n            ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my\n            understanding) the ik_llama.cpp fork was built and has a number\n            of interesting features including SotA iqN_k quantizations that\n            pack in a lot of quality for the size while retaining good speed performance. (These new\n            quants are not available in ollma, lmstudio, koboldcpp,\n            etc.) A few recent PRs made by ikawrakow to\n            ik_llama.cpp and by JohannesGaessler to mainline have\n            boosted performance across the board and especially on CUDA with\n            Flash Attention implementations for Grouped Query Attention (GQA) models and also\n            Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B\n            releases! References  ikawrakow/ik_llama.cpp/pull/370\n                submitted by    /u/VoidAlchemy \n             [link]\n              [comments]\n            \nImageDescription: The image presents two line charts comparing the performance of different implementations of LLaMA (likely Large Language Model Meta AI) inference, specifically focusing on throughput versus KV Cache size. The post title indicates improvements in both \"ik\" and \"mainline llama.cpp\".\n\n**Chart 1 (Left):** This chart plots ‚ÄúThroughput (tokens/s)‚Äù on the y-axis against \"KV Cache Size (N_KV)\" on the x-axis, ranging from 0 to approximately 30,000. Four lines are displayed, each representing a different configuration or version of the inference engine. The colors used are red, orange, green and blue. All four lines show a general downward trend ‚Äì as the KV Cache size increases, throughput decreases. The red and orange lines are very close together for most of the range, indicating similar performance.  The green line is consistently lower than red and orange, while the blue line exhibits a steeper decline in throughput as KV Cache size grows. The initial throughput values (at low KV cache sizes) are around 40-50 tokens/s for all configurations.\n\n**Chart 2 (Right):** This chart mirrors the structure of the first, also plotting ‚ÄúThroughput (tokens/s)‚Äù against \"KV Cache Size (N_KV)\". However, the y-axis scale is different, ranging from approximately 5 to 10 tokens/s. This suggests the data represents a different model size or a more demanding inference scenario than Chart 1.  Again, four lines are shown in red, orange, green and blue. The throughput values are lower overall compared to Chart 1.  The red and orange lines remain close, but the green line is significantly lower. The blue line exhibits a very rapid decline in throughput with increasing KV Cache size, falling below the other lines.\n\n**Overall Interpretation:** The charts demonstrate that increasing the KV Cache size impacts throughput negatively, as expected.  \n\nExternal URL Summaries:\n- https://github.com/ikawrakow/ik_llama.cpp: \n\n\n\n**Summary:**  \n*ik_llama.cpp* is a fork of *llama.cpp* focused on enhancing CPU and hybrid GPU/CPU performance, featuring new state-of-the-art quantization methods, Bitnet support, and optimizations like MLA, FlashMLA, and fused MoE operations. It includes numerous updates from April to May 2025, such as improved DeepSeek performance, faster token generation, and support for models like LLaMA-3-Nemotron, Qwen3, GLM-4, and Gemma3. The project emphasizes flexibility with tensor overrides, quantization customization, and detailed documentation via PRs and a wiki. It is licensed under MIT.\nComment:       You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B) mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert) tl;dr; I highly recommend doing a git pull and re-building your ik_llama.cpp or llama.cpp repo to take advantage of recent major performance improvements just released. The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving r/LocalLLaMA community! If you have enough VRAM to fully offload and already have an existing \"normal\" quant of Qwen3 MoE then you'll get a little more speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork! Details I spent yesterday compiling and running benhmarks on the newest versions of both ik_llama.cpp and mainline llama.cpp. For those that don't know, ikawrakow was an early contributor to mainline llama.cpp working on important features that have since trickled down into ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my understanding) the ik_llama.cpp fork was built and has a number of interesting features including SotA iqN_k quantizations that pack in a lot of quality for the size while retaining good speed performance. (These new quants are not available in ollma, lmstudio, koboldcpp, etc.) A few recent PRs made by ikawrakow to ik_llama.cpp and by JohannesGaessler to mainline have boosted performance across the board and especially on CUDA with Flash Attention implementations for Grouped Query Attention (GQA) models and also Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B releases! References  ikawrakow/ik_llama.cpp/pull/370     submitted by    /u/VoidAlchemy   [link]   [comments] \nComment: I'm currently running ik_llama.cpp with Qwen3-235B-A22 on a Xeon E5-2680v4, that's a 10 year old CPU with 128GB ddr4 memory, and a single RTX3090. I'm getting 7 tok/s generation, very usable if you don't use reasoning. BTW the server is multi-GPU but ik_llama.cpp just crash trying to use multiple-gpus, but I don't think it would improve speed a lot, as the CPU is always the bottleneck. \nComment: Could you explain how to read your pictures? I see orange plot below red plot, so ik_llama.cpp is slower than llama.cpp? \nComment: Can you post some of the commands you use for the benchmarks? I want to tinker to see what is best for my use case \nComment: Oh, just updated. My rig is busy for running deepseek \u0026amp; ik_llama (1 week jobs). I will update after that :) \nComment: Maybe GGUF will now give same speed as MLX on Mac devices \nComment: I have a 3090. Doesn't this say it's slower, not faster? \nComment: https://preview.redd.it/0zroyhg1qsye1.png?width=3404\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b3e55128b1aac3f6d2ddfbd22597b9cd6d7dd02c In my limited testing you probably want to go with ik_llama.cpp for fully offloaded non-MoE models like the recent GLM-4 which is crazy efficient on kv-cache VRAM usage due to its GQA design. \nComment: I just pulled and rebuilt and I'm now actually going about 15 tps slower. My previous build was from about a week ago, and I was getting an eval time of about 54 tps. Now I'm only getting 39 tokens per second, so pretty significant drop. I just downloaded the latest unsloth model I'm running on 2 3090s, using this command: ``` .\\bin\\Release\\llama-server.exe -m C:\\shared-drive\\llm_models\\unsloth-2-Qwen3-30B-A3B-128K-Q8_0.gguf --host 0.0.0.0 --ctx-size 50000 --n-predict 10000 --jinja --tensor-split 14,14 --top_k 20 --min_p 0.0 --top_p 0.8 --flash-attn --n-gpu-layers 9999 --threads 24 ``` Prompt: \"tell me a 2 paragraph story\" \nComment: How close is llamacpp to vLLM and exllama now? \nComment: Seems like it is related to CUDA only, so I guess only for people with Nvidia cards and not folks on Apple Silicon and others. \n",
      "results": {
        "title": "LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!",
        "id": "t3_1keoint",
        "overview": "The llama.cpp ecosystem has seen significant performance improvements, particularly for Qwen3 MoE models (235B and 30B). Mainline llama.cpp now benefits from CUDA optimizations with Flash Attention implementations for Grouped Query Attention (GQA) and Mixture of Experts (MoE) architectures, while the ik_llama.cpp fork introduces state-of-the-art iqN_k quantizations that balance model quality and speed. These updates are critical for users leveraging hybrid CPU+GPU offloading or seeking smaller model footprints without sacrificing performance. The mainline version sees speed gains for fully offloaded MoE models, whereas ik_llama.cpp excels in hybrid workloads and offers unique quantization schemes not available in other tools like Ollama or KoboLD. Benchmarks show throughput degradation with larger KV caches, but the optimizations mitigate this impact, especially for GQA-designed models like GLM-4.",
        "comment_overview": "The community is divided but enthusiastic. Some users report significant speed gains, while others note regressions after updates, highlighting the complexity of optimization trade-offs. Discussions center on interpreting benchmark charts, with confusion about relative performance between ik_llama.cpp and mainline versions. Practical concerns include multi-GPU stability issues in ik_llama.cpp and the need for detailed benchmarking commands. There's curiosity about cross-platform compatibility, particularly for Apple Silicon users, as the CUDA-specific optimizations limit benefits for non-NVIDIA hardware. The thread underscores the importance of community-driven testing and the competitive innovation driving these projects forward.",
        "link": "",
        "relevance": "",
        "is_relevant": true,
        "thumbnail_url": ""
      },
      "processing_time_ms": 18018
    },
    {
      "raw_input": "Title: Apparently shipping AI platforms is a thing now as per this post from the Qwen X\n            account\nID: t3_1kebauw\nSummary:   \n                \n            submitted by    /u/MushroomGecko\n              [link]\n              [comments]\n            \nImageDescription: ## Image Analysis: Qwen X Shipping Platform Post\n\nThe image appears to be a promotional graphic, likely intended for social media, advertising the availability of shipping for AI platforms ‚Äì specifically referencing Qwen X. The dominant visual element is two cartoon characters: a light-brown sloth and a brown bear.\n\nThe **sloth** is depicted with large, expressive eyes and clasped hands, suggesting anticipation or excitement. It‚Äôs positioned on the left side of the image.\n\nThe **bear** is shown offering a red flower to the sloth. It's wearing a plain white t-shirt and has a slightly more reserved expression. A small, stylized lightbulb icon is positioned near the bear‚Äôs chest, potentially symbolizing intelligence or innovation.\n\nThe background is a clean white space. \n\nNotably, there's a large block of text present at the bottom of the image. This text is unreadable as it appears to be composed entirely of non-English characters, likely a garbled string or placeholder. This suggests the image may have been generated with some automated tool and the text wasn‚Äôt properly rendered or is irrelevant to the core message.\n\nThe overall aesthetic is playful and approachable, likely targeting a broader audience unfamiliar with the technical complexities of AI platform deployment. The use of cartoon characters aims to humanize the process and make it seem less daunting.  The flower offering could symbolize a gift or ease of use ‚Äì \"shipping\" being presented as something pleasant.\n\nThe context from the post title (\"Apparently shipping AI platforms is a thing now...\") suggests this image is meant to highlight Qwen X's capability to provide readily deployable AI models or services, rather than focusing on the technical details of the platform itself.\nComment:         submitted by    /u/MushroomGecko   [link]   [comments] \nComment: Ah yes, the AI romcom. \nComment: theres lore now? \nComment: -Ok, lets see, i want to make love to you-. -But...wait. no no \nComment: What do you mean by \"shipping AI platforms\"? \nComment: Can't wait for the competitions to see which model writes the best fan fiction \nComment: Should AI companies also create their own VTuber characters that use their models? \nComment: Qwen guy looks mean. His expression is like he's never felt love. \nComment: I ship it! \nComment: And those two ship often \nComment: Hell yeah \nComment: Has anyone noticed better performance for similar quants of the same model by different creators? \nComment: Oh no \nComment: Love the contribution to the OSS these guys make!! Pure awesomeness \nComment: ü§¶üèª‚Äç‚ôÇÔ∏è \nComment: Anyone feeding into this just entirely lacks a social life, why are some of you like this? \n",
      "results": {
        "title": "Apparently shipping AI platforms is a thing now as per this post from the Qwen X account",
        "id": "t3_1kebauw",
        "overview": "This post appears to be a promotional image from the Qwen X account, featuring cartoon characters (a sloth and bear) with ambiguous messaging about 'shipping AI platforms.' The image contains non-English placeholder text and a playful aesthetic, suggesting it may be a lighthearted or meme-like announcement. However, no technical details about Qwen X's capabilities, infrastructure, or deployment processes are provided. The phrase 'shipping AI platforms' is likely a pun on the term 'shipping' (as in delivering products) rather than a technical reference to model deployment. The post lacks concrete information about performance metrics, architecture, or novel techniques, making it difficult to assess its technical significance.",
        "comment_overview": "The comments section is dominated by humorous, meme-like reactions to the post. Users are joking about 'AI romcoms,' 'shipping' as a romantic concept, and the absurdity of 'shipping AI platforms.' Some comments speculate about fictional lore or fanfiction potential, while others mock the post's lack of technical substance. A few users mention Qwen X's open-source contributions, but the overall sentiment is dismissive of the post's seriousness. The discussion highlights the community's skepticism toward vague or marketing-driven AI announcements.",
        "link": "",
        "relevance": "",
        "is_relevant": false,
        "thumbnail_url": ""
      },
      "processing_time_ms": 12594
    },
    {
      "raw_input": "Title: IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models\nID: t3_1kedu0d\nSummary:   \n                \n            submitted by   \n            /u/ab2377   [link]\n              [comments]\n            \nImageDescription: \n\nExternal URL Summaries:\n- https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek: \n\n\n\nIBM has introduced the **Granite 4.0 Tiny Preview**, a compact and efficient language model in its upcoming Granite 4.0 family, designed for consumer-grade hardware. Despite being partially trained, it matches the performance of larger models while using significantly less memory. The model uses a **hybrid Mamba-2/Transformer architecture**, combining the efficiency of Mamba with the precision of transformers, and features a **fine-grained mixture of experts (MoE)** framework. It supports long contexts (up to 128K tokens) and multiple concurrent sessions on modest GPUs. The preview is available on Hugging Face, with full enterprise support expected later this summer. The model's **NoPE (no positional encoding)** approach allows flexible context lengths, and IBM plans to continue training it for improved performance.\nComment:   \n                \n            submitted by   \n            /u/ab2377   [link]\n              [comments]\n            \nComment: Hope they\n            can release a larger one like 30b-a3b  \nComment: so a new\n            architecture, more moe goodness  \"Whereas prior generations\n            of Granite LLMs utilized a conventional transformer architecture, all models in the\n            Granite 4.0 family utilize a new hybrid Mamba-2/Transformer architecture, marrying the\n            speed and efficiency of Mamba with the precision of transformer-based self-attention.\n            Granite 4.0 Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts\n            (MoE) model, with 7B total parameters and only 1B active parameters at inference\n            time. Many of the innovations informing the Granite 4 architecture\n            arose from IBM Research‚Äôs collaboration with the original Mamba creators on Bamba, an\n            experimental open source hybrid model whose successor (Bamba v2) was released earlier\n            this week.\" \nComment: Please\n            look here: https://huggingface.co/ibm-granite/granite-4.0-tiny-preview/discussions/2\n            gabegoodhart IBM\n            Granite org 1\n            day ago Since this model is hot-off-the-press, we\n            don't have inference support in llama.cpp yet.\n            I'm actively working on it, but since this is one of the first major models\n            using a hybrid-recurrent architecture, there are a number of in-flight architectural\n            changes in the codebase that need to all meet up to get this supported. We'll\n            keep you posted! gabegoodhart IBM\n            Granite org 1\n            day ago We definitely expect the model quality to improve\n            beyond this preview. So far, this preview checkpoint has been trained on ~2.5T tokens,\n            but it will continue to train up to ~15T tokens before final release.\n            \nComment: i hope we\n            can see some larger models too! I really want them to scale those more experimental\n            architectures and see where it leads. I think there is huge potential in combining\n            attention with hidden state models. attention to understand context, hidden state to\n            think ahead, remember key information etc. \nComment: Read the\n            full thing. It‚Äôs worth it. \nComment: Holy,\n            this actually looks really good. IBM might actually be able to catch up with Alibaba\n            with this one. \nComment: Neat but\n            unless folks really start working to help add support for mamba architectures to\n            llama.cpp it'll be dead on arrival. It would be great to see\n            the folks at /u/IBM step up and help out\n            llama.cpp to support things like this. \nComment: \n            The Granite 4.0 architecture uses no positional encoding (NoPE). Our testing\n            demonstrates convincingly that this has had no adverse effect on long-context\n            performance.   This is interesting. Are there any\n            papers that explain why this still works? \nComment: Looking\n            very promising... \nComment: Is IBM\n            going to be the silent winner? It‚Äôs impressive that their tiny model is 8b MOE and\n            likely to perform at the same level as their previous dense 8b: Granite 4.0\n            Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts (MoE) model,\n            with 7B total parameters and only 1B active parameters at inference time.\n            I hope their efforts attempt to improve in https://fiction.live/stories/Fiction-liveBench-April-6-2025/oQdzQvKHw8JyXbN87\n            and not just passkey testing. \nComment: I'm just a dreamer without much background in\n            ML stuff. Can anyone with sense comment on how likely it is we'll ever see\n            something that might be so efficient it'll run well on CPU? I mean, this model\n            already sounds pretty exciting from an efficiency perspective. Wondering if\n            we've exhausted architectural changes that would e.g. reduce memory bandwidth\n            requirements \nComment: ‚ÄúWe‚Äôre\n            excited to continue pre-training Granite 4.0 Tiny, given such promising results so early\n            in the process. We‚Äôre also excited to apply our learnings from post-training Granite\n            3.3, particularly with regard to reasoning capabilities and complex instruction\n            following, to the new models. Like its predecessors in Granite 3.2 and Granite 3.3,\n            Granite 4.0 Tiny Preview offers toggleablethinking on andthinking off functionality\n            (though its reasoning-focused post-training is very much incomplete).‚Äù\n            I hope some of this involves interacting with fictional text in a creative\n            fashion: scene summaries, character profiles, plot outlining, hypothetical change\n            impacts - books are great datasets just like large code bases and just need a good set\n            of training data ‚Äî use Guttenberg public domain books that are modernized with AI and\n            then create training around the above elements. \nComment: Now if\n            only we could get IBM to sell a version of their AI card to the public\n            \nComment: ibm doing\n            better work than meta theyre surprisingly becoming a big player in open source (for\n            small models) \nComment: I wonder\n            what is prompt processing speed for semi-recurrent stuff compared to transformers.\n            Transformers have fantastic prompt processing speed like 1000t/s easy even on crap like\n            3060, but slow down during token generation as context grows. This seems to be the other\n            way around, slow PP but fast TG. I might be completely\n            wrong. \nComment: Large\n            datasets: all of Harry Potter series asking questions like, what would have to change in\n            the series for Harry to end up with Hermione or for Voldemort to win. It‚Äôs a series\n            everyone knows fairly well and requires details in the story and the story\n            whole. \nComment: I\n            remember seeing this model a few days ago. There's no gguf so i cant try it out.\n            I guess there's not a lot of interest in this moe or it's not currently\n            possibly to make ggufs for it at the moment.  Webui stopped working\n            for me last year after i updated it and I've never been able to get it working\n            right since then, so been using lm studio appimages. That program runs everything good\n            for me but only runs ggufs. \nComment: Yall need\n            to learn Transformers and stop hating on llama.cpp \n",
      "results": {
        "title": "IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models",
        "id": "t3_1kedu0d",
        "overview": "IBM's Granite 4.0 Tiny Preview introduces a groundbreaking hybrid architecture combining Mamba-2 and Transformer components, enabling efficient large-context processing on consumer hardware. The model employs a fine-grained mixture-of-experts (MoE) framework with 7B total parameters but only 1B active during inference, achieving performance comparable to larger models while reducing memory usage. Key innovations include IBM's NoPE (no positional encoding) approach, which allows flexible context lengths up to 128K tokens without sacrificing long-context performance. The preview model, trained on ~2.5T tokens, will continue training up to 15T tokens before final release. Its hybrid architecture stems from IBM Research's collaboration with Mamba creators on Bamba, with Bamba v2 released concurrently. The model is available on Hugging Face, though inference support in popular frameworks like llama.cpp is still under development.",
        "comment_overview": "The community is enthusiastic about IBM's architectural innovations, with many comparing the potential of hybrid Mamba-2/Transformer models to established players like Alibaba. Discussions highlight excitement over the MoE efficiency (7B parameters vs 1B active) and NoPE's implications for context flexibility. However, some users express concern about limited inference tooling support (e.g., lack of GGUF format) and the need for broader framework compatibility. Others speculate about future scaling possibilities and creative applications like fictional text interaction. A recurring theme is anticipation for enterprise releases later this summer, alongside technical curiosity about the model's reasoning capabilities and training data specifics.",
        "link": "",
        "relevance": "",
        "is_relevant": true,
        "thumbnail_url": ""
      },
      "processing_time_ms": 15626
    },
    {
      "raw_input": "Title: Qwen3 no reasoning vs Qwen2.5\nID: t3_1kegrce\nSummary: \n            It seems\n            evident that Qwen3 with reasoning beats Qwen2.5. But I wonder if the Qwen3 dense models\n            with reasoning turned off also outperforms Qwen2.5. Essentially what I am wondering is\n            if the improvements mostly come from the reasoning.    submitted by    /u/No-Bicycle-132\n              [link]\n              [comments]\nImageDescription: \nComment: It seems evident that Qwen3 with reasoning beats Qwen2.5. But I wonder if the Qwen3 dense models with reasoning turned off also outperforms Qwen2.5. Essentially what I am wondering is if the improvements mostly come from the reasoning.    submitted by    /u/No-Bicycle-132   [link]   [comments]\nComment: On aider polyglot there was a huge boost from 16.4% of qwen 2.5 coder whole to qwen 3 32B (no thinking) achieving 45.8% being ahead of gpt 4.5. \nComment: Don't stop at wondering. Why don't you test it and share your result with us? \nComment: https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/ \nComment: Depends on the task. For code autocomplete Qwen/Qwen3-14B-AWQ nothink is awful. I like Qwen2.5-coder:14b. Additionally: some quants might be broken. \nComment: The 2.5 coders are better at complex one shots. 3.0 seems to generalize better and retains logic over a multiturn edit. My work involves updating lots of legacy Fortran and cobol that is written with very specific formatting and comment practices. 3.0 is the first open model that can run reasonably at 48gb vram that can reliably port my code. Also I think, for coding one shot diffs, reasoning turned off produces better results. \nComment: I have compared them far too little to be able to draw a serious conclusion, but from the very few comparisons I have made in coding, Qwen3 (no thinking) outputs better code, more accordingly to the prompt, than Qwen2.5. \nComment: The short answer is it entirely depends on your use case. In my limited testing, their overall performance was pretty close, with Qwen 3 probably being better overall.  I know the benchmarks say otherwise, but when translating Japanese to English, I found Qwen 2.5 to sound more natural.  However, when summarizing short stories, Qwen 2.5 dissected the story like a technical manual, whereas Qwen 3 wrote (or tried to write) in the tone of the original story.  Qwen 3 seems to lose less when quantized than Qwen 2.5. I was shocked at how well Qwen 3 32b functioned even down to IQ2 (except for factual retrieval which as usual takes a big hit). Coding, logical puzzles, and problem-solving seemed like a toss up. They both did it with more or less the same success; although, enabling reason will likely give Qwen 3 the edge. \nComment: Yes from what I have seen for apples to apples. But the 2.5 coding models will probably still hold tier own vs regular 3 models with thinking off. \nComment: They do. Qwen3 8b outperforms 7b 2.5; at least because of that extra 1b. \n",
      "results": {
        "title": "Qwen3 no reasoning vs Qwen2.5: Are reasoning improvements the main driver?",
        "id": "t3_1kegrce",
        "overview": "This discussion centers on whether Qwen3's performance gains over Qwen2.5 stem primarily from its reasoning capabilities or other architectural changes. The user questions if Qwen3's dense models with reasoning disabled still outperform Qwen2.5, suggesting the improvements might be tied to reasoning mechanisms. Comments highlight specific benchmarks: Qwen3-32B (no thinking) achieved 45.8% on a coding benchmark vs. Qwen2.5's 16.4%, while Qwen3-14B-AWQ showed mixed results in code autocomplete tasks. Some users note Qwen2.5-coder:14B excels in complex one-shot coding, but Qwen3-14B-AWQ produces more prompt-aligned code. Technical details include quantization effects (e.g., Qwen3-32B functioning at IQ2) and task-specific performance differences, such as Qwen2.5's natural Japanese-to-English translation vs. Qwen3's story-toned summarization. The debate underscores how reasoning modules, model size, and quantization impact real-world applications.",
        "comment_overview": "The community is split on Qwen3's advantages, with users emphasizing task-specific tradeoffs. While some praise Qwen3's generalization and quantization resilience, others prefer Qwen2.5 for coding precision and specific language tasks. Critics note that Qwen3's reasoning mode often outperforms Qwen2.5, but 'no thinking' variants show mixed results. A recurring theme is that Qwen3's 32B model outperforms Qwen2.5's 14B in benchmarks, but smaller models like 8B vs. 7B show size-driven gains rather than reasoning improvements. Users also warn about potential quantization issues and the importance of testing on use-case-specific datasets.",
        "link": "",
        "relevance": "",
        "is_relevant": true,
        "thumbnail_url": ""
      },
      "processing_time_ms": 17709
    }
  ],
  "image_summaries": [
    {
      "image_url": "https://preview.redd.it/gefvhv84qsye1.png?width=640\u0026crop=smart\u0026auto=webp\u0026s=235b3e1de1b7df4bd1bc1f7519f84b5259303d05",
      "image_description": "## Image Analysis: Visa \"Vibe Coder\" Job Posting Detail\n\nThe image presents a screenshot of text, likely extracted from a job posting by Visa. The post is soliciting candidates with specific technical skills, framed as \"vibe coders.\" It appears to be a section detailing desired experience.\n\nThe text is formatted as a bulleted list, outlining requirements and preferred qualifications. Key technologies explicitly mentioned include:\n\n*   **Vector Databases:** Pinecone, ChromaDB are named. This indicates a focus on semantic search and retrieval using vector embeddings.\n*   **Embedding Models:** The posting specifically requests experience with embedding models, suggesting the role involves working with representations of data for similarity comparisons.\n*   **Containerization:** Docker is explicitly listed, indicating a need for experience in packaging and deploying applications within containers. Kubernetes is also mentioned, implying familiarity with container orchestration at scale.\n*   **Frontend Tools:** Lovable and V0 are listed, suggesting a preference for modern frontend development tools. The specific nature of these tools isn't clear without further context, but they likely relate to component libraries or UI frameworks.\n*   **Data Science:** Experience in Data Science is considered a \"big plus,\" suggesting the role may involve data analysis or model building.\n*   **Problem Solving:** Strong problem-solving skills are explicitly requested, highlighting the need for analytical thinking and adaptability.\n\nThe bottom of the image contains a blurred section with non-English characters, which are illegible. A small Reddit logo is visible in the bottom right corner, indicating the source of the screenshot is likely a Reddit post. The \"technical\" flair tag further confirms the discussion revolves around technical aspects of the job posting.\n\nOverall, this excerpt points to a role requiring expertise in modern data infrastructure (vector databases, embeddings), DevOps practices (Docker, Kubernetes) and frontend development. The \"vibe coder\" framing suggests a potentially innovative or experimental team within Visa, focused on leveraging these technologies",
      "title": "Visa is looking for vibe coders - thoughts?",
      "entry_id": "t3_1keolh9",
      "processing_time_ms": 36764
    },
    {
      "image_url": "https://preview.redd.it/627wnr5emsye1.jpeg?width=640\u0026crop=smart\u0026auto=webp\u0026s=b896f5165e878160c1e104137518ab1d80b3addc",
      "image_description": "## Image Analysis: UI-Tars-1.5 Reasoning Interface\n\nThe image displays a screenshot of a user interface (UI) seemingly built for interacting with a Large Language Model (LLM), specifically identified as \"UI-Tars-1.5\". The UI is presented in a light theme, resembling a macOS application window.\n\n**Key Components:**\n\n1. **LLM Provider and Model Selection:** A prominent section labeled \"LLM Provider and Model\" allows the user to select a pre-defined LLM or specify a \"Custom model‚Ä¶\". A dropdown menu is visible, currently displaying the selected custom model: ‚ÄúByteDance-Seed/UI-Tars-1.5-7B‚Äù. Below this is a text input field labeled \"Provider Base URL\", likely used to specify the API endpoint for accessing the chosen model.\n\n2. **UI-TARS Thoughts:** A text box titled \"UI-TARS Thoughts\" displays the LLM's reasoning process. The displayed text indicates a conversational context where the user asked about Google‚Äôs purpose, and UI-Tars is responding with a verbose explanation. The LLM states it's attempting to explain the purpose of Google and is about to initiate a search for a repository, suggesting an agent-like behavior with tool use (web searching). The text includes downward pointing arrows (\"‚Üì\") indicating further output is likely being generated.\n\n3. **Textbox Input:** A \"Textbox\" field at the bottom of the interface is labeled ‚ÄúAsk me to perform tasks in a virtual macOS environment‚Äù. This suggests the LLM has capabilities beyond simple text generation, potentially including interacting with a simulated macOS environment.\n\n**Technical Observations:**\n\n*   The model name \"ByteDance-Seed/UI-Tars-1.5-7B\" indicates a 7 billion parameter model developed by ByteDance, likely hosted on Hugging Face or a similar platform.\n*   The UI design suggests an",
      "title": "UI-Tars-1.5 reasoning never fails to entertain me.",
      "entry_id": "t3_1keo3te",
      "processing_time_ms": 30867
    },
    {
      "image_url": "https://preview.redd.it/l59qu1gxysye1.png?width=961\u0026format=png\u0026auto=webp\u0026s=381abad7b25e1d719265826441b51aa50177d143",
      "image_description": "## Image Description: Qwen3 Performance Benchmarks\n\nThe image presents a detailed performance comparison of the Qwen3 language model across approximately 50 different devices, categorized by backend (CPU, Metal, and Android). The data is structured in a tabular format.\n\n**Table Structure:**\n\nThe table has columns representing:\n*   **Device:** Lists various devices, including iPhone models (e.g., iPhone 15 Pro Max, iPhone 14), iPad models, Mac devices (e.g., M3 Max, M2 Pro), Windows laptops (e.g., Surface Laptop 5), and Android phones (e.g., Pixel 8 Pro, Samsung Galaxy S24 Ultra).\n*   **Backend Count:** Indicates the number of model layers processed by each backend (CPU or GPU).\n*   **RAM:** Specifies the RAM usage in GB.\n*   **Toks/s:**  Represents tokens per second, a key metric for inference speed.\n*   **Time (ms):**  Indicates the latency in milliseconds, likely for generating a specific number of tokens.\n*   **RAM (mb):** RAM usage in megabytes, likely a redundant column duplicating the \"RAM\" information.\n\n**Data Observations:**\n\n*   **Backends:** The majority of devices utilize either CPU or Metal (Apple's GPU framework). Android devices also show performance data.\n*   **Performance Variation:** Significant variation in Toks/s is observed across devices. High-end Macs (M3 Max) demonstrate the highest Toks/s values, exceeding 600. iPhones and iPads show varying performance depending on the model (e.g., iPhone 15 Pro Max at ~382 Toks/s). Android devices generally show lower performance than comparable Apple silicon devices.\n*   **RAM Usage:** RAM usage correlates with backend count; higher layer counts generally require more RAM.  ",
      "title": "Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS,\n            Android, Mac, Windows)",
      "entry_id": "t3_1kepuli",
      "processing_time_ms": 31189
    },
    {
      "image_url": "https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026format=png\u0026auto=webp\u0026s=f65d811c4859178fe80cbcb50312217ba5591c5b",
      "image_description": "## Image Description: QwQ, Qwen \u0026 GLM-4 HTML Coding Comparison\n\nThe image presents a screenshot of a webpage likely generated by Large Language Models (LLMs) ‚Äì specifically, QwQ 32b, Qwen 3 32b, and GLM-4-32B ‚Äì as indicated by the post title. The webpage's content appears to be a basic HTML structure demonstrating each model‚Äôs ability to generate code for a simple \"Our Services\" section.\n\nThe page is divided into three distinct sections, each presumably representing the output from one of the LLMs. Each section contains a heading \"Our Services\" followed by two sub-sections: ‚ÄúRepair Shop Repair‚Äù and ‚ÄúVirus Removal‚Äù. \n\nEach sub-section contains a brief description. \"Repair Shop Repair\" states: ‚ÄúRepair any part of all computer components.‚Äù  ‚ÄúVirus Removal‚Äù describes: \"Complete malware and virus removal.\"\n\nThe core of the comparison lies in how each model structures the HTML code for these sections. While visually similar, subtle differences likely exist in the tags used (e.g., `\u003cdiv\u003e`, `\u003cp\u003e`, `\u003cspan\u003e`), class names, and overall code formatting. The image doesn‚Äôt show the actual HTML source code; it only displays the rendered output.\n\nNotably, a large block of seemingly random characters (non-English script) is present under both sub-sections. This appears to be a failure case or an artifact of the LLM generation process, potentially indicating issues with handling non-ASCII characters or a hallucination. The repetition of this character block suggests it's not intended content and likely represents an error in the model‚Äôs output.\n\nThe overall UI is minimalistic, with a white background and simple text formatting. The focus is clearly on the generated HTML content rather than any elaborate design elements, reinforcing the purpose of this comparison. The image is a direct visual representation of the LLM's ability to",
      "title": "QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.",
      "entry_id": "t3_1kenk4f",
      "processing_time_ms": 31183
    },
    {
      "image_url": "https://preview.redd.it/3bwwfd4epsye1.png?width=3404\u0026format=png\u0026auto=webp\u0026s=adbb0bce2c13bc560499b0d3459329d16d0a3291",
      "image_description": "The image presents two line charts comparing the performance of different implementations of LLaMA (likely Large Language Model Meta AI) inference, specifically focusing on throughput versus KV Cache size. The post title indicates improvements in both \"ik\" and \"mainline llama.cpp\".\n\n**Chart 1 (Left):** This chart plots ‚ÄúThroughput (tokens/s)‚Äù on the y-axis against \"KV Cache Size (N_KV)\" on the x-axis, ranging from 0 to approximately 30,000. Four lines are displayed, each representing a different configuration or version of the inference engine. The colors used are red, orange, green and blue. All four lines show a general downward trend ‚Äì as the KV Cache size increases, throughput decreases. The red and orange lines are very close together for most of the range, indicating similar performance.  The green line is consistently lower than red and orange, while the blue line exhibits a steeper decline in throughput as KV Cache size grows. The initial throughput values (at low KV cache sizes) are around 40-50 tokens/s for all configurations.\n\n**Chart 2 (Right):** This chart mirrors the structure of the first, also plotting ‚ÄúThroughput (tokens/s)‚Äù against \"KV Cache Size (N_KV)\". However, the y-axis scale is different, ranging from approximately 5 to 10 tokens/s. This suggests the data represents a different model size or a more demanding inference scenario than Chart 1.  Again, four lines are shown in red, orange, green and blue. The throughput values are lower overall compared to Chart 1.  The red and orange lines remain close, but the green line is significantly lower. The blue line exhibits a very rapid decline in throughput with increasing KV Cache size, falling below the other lines.\n\n**Overall Interpretation:** The charts demonstrate that increasing the KV Cache size impacts throughput negatively, as expected.  ",
      "title": "LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!",
      "entry_id": "t3_1keoint",
      "processing_time_ms": 31327
    },
    {
      "image_url": "https://preview.redd.it/fjze9by1yoye1.jpeg?width=640\u0026crop=smart\u0026auto=webp\u0026s=7e0eb0e11cb1eca743ebbe8cd75e69b96fffd960",
      "image_description": "## Image Analysis: Qwen X Shipping Platform Post\n\nThe image appears to be a promotional graphic, likely intended for social media, advertising the availability of shipping for AI platforms ‚Äì specifically referencing Qwen X. The dominant visual element is two cartoon characters: a light-brown sloth and a brown bear.\n\nThe **sloth** is depicted with large, expressive eyes and clasped hands, suggesting anticipation or excitement. It‚Äôs positioned on the left side of the image.\n\nThe **bear** is shown offering a red flower to the sloth. It's wearing a plain white t-shirt and has a slightly more reserved expression. A small, stylized lightbulb icon is positioned near the bear‚Äôs chest, potentially symbolizing intelligence or innovation.\n\nThe background is a clean white space. \n\nNotably, there's a large block of text present at the bottom of the image. This text is unreadable as it appears to be composed entirely of non-English characters, likely a garbled string or placeholder. This suggests the image may have been generated with some automated tool and the text wasn‚Äôt properly rendered or is irrelevant to the core message.\n\nThe overall aesthetic is playful and approachable, likely targeting a broader audience unfamiliar with the technical complexities of AI platform deployment. The use of cartoon characters aims to humanize the process and make it seem less daunting.  The flower offering could symbolize a gift or ease of use ‚Äì \"shipping\" being presented as something pleasant.\n\nThe context from the post title (\"Apparently shipping AI platforms is a thing now...\") suggests this image is meant to highlight Qwen X's capability to provide readily deployable AI models or services, rather than focusing on the technical details of the platform itself.",
      "title": "Apparently shipping AI platforms is a thing now as per this post from the Qwen X\n            account",
      "entry_id": "t3_1kebauw",
      "processing_time_ms": 28132
    }
  ],
  "web_content_summaries": [
    {
      "url": "https://huggingface.co/collections/unsloth/qwen3-680edabfb790c8c34a242f95",
      "original_content": "Qwen's new Qwen3 models. In Unsloth Dynamic 2.0, GGUF, 4-bit and 16-bit Safetensor formats. Includes 128K Context Length variants.                       unsloth/Qwen3-30B-A3B-GGUF                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       250k            ‚Ä¢       167                 Note    Run Qwen3 with our Dynamic GGUFs:               unsloth/Qwen3-32B-GGUF                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       120k            ‚Ä¢       54                                unsloth/Qwen3-235B-A22B-GGUF                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       71.3k            ‚Ä¢       48                                unsloth/Qwen3-14B-GGUF                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       77.9k            ‚Ä¢       37                                unsloth/Qwen3-8B-GGUF                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       61.8k            ‚Ä¢       39                                unsloth/Qwen3-4B-GGUF                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       55.5k            ‚Ä¢       30                                unsloth/Qwen3-1.7B-GGUF                  Text Generation    ‚Ä¢        Updated      8 days ago          ‚Ä¢       26.9k            ‚Ä¢       16                                unsloth/Qwen3-0.6B-GGUF                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       51.1k            ‚Ä¢       40                                unsloth/Qwen3-30B-A3B-128K-GGUF                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       46.6k            ‚Ä¢       40                 Note    128K Context Length GGUFs:               unsloth/Qwen3-235B-A22B-128K-GGUF                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       304k            ‚Ä¢       26                                unsloth/Qwen3-32B-128K-GGUF                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       19.8k            ‚Ä¢       18                                unsloth/Qwen3-14B-128K-GGUF                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       9.5k            ‚Ä¢       10                                unsloth/Qwen3-8B-128K-GGUF                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       14.6k            ‚Ä¢       12                                unsloth/Qwen3-4B-128K-GGUF                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       14.7k            ‚Ä¢       17                                unsloth/Qwen3-32B-unsloth-bnb-4bit                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       9.47k            ‚Ä¢       5                 Note    Dynamic 4-bit safetensors to fine-tune/deploy:               unsloth/Qwen3-14B-unsloth-bnb-4bit                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       60.9k            ‚Ä¢       5                                unsloth/Qwen3-8B-unsloth-bnb-4bit                     Updated      3 days ago          ‚Ä¢       36.5k            ‚Ä¢       2                                unsloth/Qwen3-4B-unsloth-bnb-4bit                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       47.1k            ‚Ä¢       2                                unsloth/Qwen3-1.7B-unsloth-bnb-4bit                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       38.9k            ‚Ä¢       3                                unsloth/Qwen3-0.6B-unsloth-bnb-4bit                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       24.4k            ‚Ä¢       5                                unsloth/Qwen3-32B                     Updated      3 days ago          ‚Ä¢       3.67k            ‚Ä¢       6                 Note    Original 16-bit (Instruct):               unsloth/Qwen3-30B-A3B                     Updated      3 days ago          ‚Ä¢       5.05k            ‚Ä¢       12                                unsloth/Qwen3-14B                     Updated      3 days ago          ‚Ä¢       4.25k            ‚Ä¢       7                                unsloth/Qwen3-8B                  Text Generation    ‚Ä¢        Updated      3 days ago          ‚Ä¢       8.84k            ‚Ä¢       4                                unsloth/Qwen3-4B                     Updated      3 days ago          ‚Ä¢       8.95k            ‚Ä¢       5                                unsloth/Qwen3-1.7B                     Updated      3 days ago          ‚Ä¢       6.96k            ‚Ä¢       4                                unsloth/Qwen3-0.6B                     Updated      3 days ago          ‚Ä¢       11.7k            ‚Ä¢       6                                unsloth/Qwen3-32B-bnb-4bit                     Updated      3 days ago          ‚Ä¢       6.22k            ‚Ä¢       4                 Note    BitsandBytes 4-bit safetensors:               unsloth/Qwen3-30B-A3B-bnb-4bit                     Updated      3 days ago          ‚Ä¢       10.2k            ‚Ä¢       7                                unsloth/Qwen3-14B-bnb-4bit                     Updated      3 days ago          ‚Ä¢       1.94k            ‚Ä¢       2                                unsloth/Qwen3-8B-bnb-4bit                     Updated      3 days ago          ‚Ä¢       2.23k                                            unsloth/Qwen3-4B-bnb-4bit                     Updated      3 days ago          ‚Ä¢       3.25k            ‚Ä¢       1                                unsloth/Qwen3-0.6B-bnb-4bit                     Updated      3 days ago          ‚Ä¢       1.91k                                            unsloth/Qwen3-1.7B-bnb-4bit                     Updated      3 days ago          ‚Ä¢       890            ‚Ä¢       2                                unsloth/Qwen3-14B-Base-unsloth-bnb-4bit                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       2.05k            ‚Ä¢       1                                unsloth/Qwen3-8B-Base-unsloth-bnb-4bit                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       1.26k                             Note    (Base) - Our Dynamic 4-bit safetensors to fine-tune/deploy:                 unsloth/Qwen3-4B-Base-unsloth-bnb-4bit                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       2.08k                                            unsloth/Qwen3-1.7B-Base-unsloth-bnb-4bit                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       632            ‚Ä¢       1                                unsloth/Qwen3-0.6B-Base-unsloth-bnb-4bit                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       2.26k                                            unsloth/Qwen3-30B-A3B-Base                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       38            ‚Ä¢       1                 Note    (Base) Original 16-bit:               unsloth/Qwen3-14B-Base                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       165                                            unsloth/Qwen3-8B-Base                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       990                                            unsloth/Qwen3-1.7B-Base                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       872            ‚Ä¢       1                                unsloth/Qwen3-4B-Base                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       3.98k                                            unsloth/Qwen3-0.6B-Base                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       1.5k            ‚Ä¢       1                                unsloth/Qwen3-30B-A3B-Base-bnb-4bit                         Note    (Base) BitsandBytes 4-bit safetensors:               unsloth/Qwen3-14B-Base-bnb-4bit                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       132                                            unsloth/Qwen3-8B-Base-bnb-4bit                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       141                                            unsloth/Qwen3-4B-Base-bnb-4bit                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       203                                            unsloth/Qwen3-1.7B-Base-bnb-4bit                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       282                                            unsloth/Qwen3-0.6B-Base-bnb-4bit                  Text Generation    ‚Ä¢        Updated      4 days ago          ‚Ä¢       416                                            unsloth/Qwen3-32B-FP8                  Text Generation    ‚Ä¢        Updated      6 days ago          ‚Ä¢       244            ‚Ä¢       1                 Note    FP8 versions:               unsloth/Qwen3-14B-FP8                  Text Generation    ‚Ä¢        Updated      6 days ago          ‚Ä¢       1.98k            ‚Ä¢       1                                unsloth/Qwen3-8B-FP8                  Text Generation    ‚Ä¢        Updated      6 days ago          ‚Ä¢       128                                            unsloth/Qwen3-4B-FP8                  Text Generation    ‚Ä¢        Updated      6 days ago          ‚Ä¢       97                                            unsloth/Qwen3-1.7B-FP8                  Text Generation    ‚Ä¢        Updated      6 days ago          ‚Ä¢       40                                            unsloth/Qwen3-0.6B-FP8                  Text Generation    ‚Ä¢        Updated      6 days ago          ‚Ä¢       51",
      "summary": "\n\n\n\nThe article introduces the Qwen3 model collection by unsloth on Hugging Face, featuring various versions of Qwen3 with different sizes (from 0.6B to 235B parameters) and formats, including GGUF, 4-bit, and 16-bit safetensors. It highlights models with up to 128K context length, dynamic 4-bit safetensors for fine-tuning and deployment, and FP8 versions. Each model includes details like update dates, download counts, and stars.",
      "title": "Qwen3 - a unsloth Collection",
      "entry_id": "t3_1kepuli",
      "processing_time_ms": 16485
    },
    {
      "url": "https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4",
      "original_content": "% ./build/bin/llama-cli -m ../models/qwen/qwen3-30b-a3b/Qwen_Qwen3-30B-A3B-Q4_K_M.gguf -c 20000                                                                      build: 5229 (07c2e2f7) with Apple clang version 16.0.0 (clang-1600.0.26.6) for arm64-apple-darwin24.1.0                                        main: llama backend init                                        main: load the model and apply lora adapter, if any                                        llama_model_load_from_file_impl: using device Metal (Apple M4 Pro) - 36863 MiB free                                        llama_model_loader: loaded meta data with 41 key-value pairs and 579 tensors from ../models/qwen/qwen3-30b-a3b/Qwen_Qwen3-30B-A3B-Q4_K_M.gguf (version GGUF V3 (latest))                                        llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.                                        llama_model_loader: - kv   0:                       general.architecture str              = qwen3moe                                        llama_model_loader: - kv   1:                               general.type str              = model                                        llama_model_loader: - kv   2:                               general.name str              = Qwen3 30B A3B                                        llama_model_loader: - kv   3:                           general.basename str              = Qwen3                                        llama_model_loader: - kv   4:                         general.size_label str              = 30B-A3B                                        llama_model_loader: - kv   5:                            general.license str              = apache-2.0                                        llama_model_loader: - kv   6:                       general.license.link str              = https://huggingface.co/Qwen/Qwen3-30B...                                        llama_model_loader: - kv   7:                   general.base_model.count u32              = 1                                        llama_model_loader: - kv   8:                  general.base_model.0.name str              = Qwen3 30B A3B Base                                        llama_model_loader: - kv   9:          general.base_model.0.organization str              = Qwen                                        llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/Qwen/Qwen3-30B...                                        llama_model_loader: - kv  11:                               general.tags arr[str,1]       = [\"text-generation\"]                                        llama_model_loader: - kv  12:                       qwen3moe.block_count u32              = 48                                        llama_model_loader: - kv  13:                    qwen3moe.context_length u32              = 32768                                        llama_model_loader: - kv  14:                  qwen3moe.embedding_length u32              = 2048                                        llama_model_loader: - kv  15:               qwen3moe.feed_forward_length u32              = 6144                                        llama_model_loader: - kv  16:              qwen3moe.attention.head_count u32              = 32                                        llama_model_loader: - kv  17:           qwen3moe.attention.head_count_kv u32              = 4                                        llama_model_loader: - kv  18:                    qwen3moe.rope.freq_base f32              = 1000000.000000                                        llama_model_loader: - kv  19:  qwen3moe.attention.layer_norm_rms_epsilon f32              = 0.000001                                        llama_model_loader: - kv  20:                 qwen3moe.expert_used_count u32              = 8                                        llama_model_loader: - kv  21:              qwen3moe.attention.key_length u32              = 128                                        llama_model_loader: - kv  22:            qwen3moe.attention.value_length u32              = 128                                        llama_model_loader: - kv  23:                      qwen3moe.expert_count u32              = 128                                        llama_model_loader: - kv  24:        qwen3moe.expert_feed_forward_length u32              = 768                                        llama_model_loader: - kv  25:                       tokenizer.ggml.model str              = gpt2                                        llama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = qwen2                                        llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"\u0026\", \"'\", ...                                        llama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...                                        llama_model_loader: - kv  29:                      tokenizer.ggml.merges arr[str,151387]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...                                        llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 151645                                        llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 151643                                        llama_model_loader: - kv  32:                tokenizer.ggml.bos_token_id u32              = 151643                                        llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = false                                        llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '\u003c|im_start|\u003e...                                        llama_model_loader: - kv  35:               general.quantization_version u32              = 2                                        llama_model_loader: - kv  36:                          general.file_type u32              = 15                                        llama_model_loader: - kv  37:                      quantize.imatrix.file str              = /models_out/Qwen3-30B-A3B-GGUF/Qwen_Q...                                        llama_model_loader: - kv  38:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt                                        llama_model_loader: - kv  39:             quantize.imatrix.entries_count i32              = 384                                        llama_model_loader: - kv  40:              quantize.imatrix.chunks_count i32              = 209                                        llama_model_loader: - type  f32:  241 tensors                                        llama_model_loader: - type q8_0:   48 tensors                                        llama_model_loader: - type q4_K:  193 tensors                                        llama_model_loader: - type q5_K:   48 tensors                                        llama_model_loader: - type q6_K:   49 tensors                                        print_info: file format = GGUF V3 (latest)                                        print_info: file type   = Q4_K - Medium                                        print_info: file size   = 17.35 GiB (4.88 BPW)                                         load: special tokens cache size = 26                                        load: token to piece cache size = 0.9311 MB                                        print_info: arch             = qwen3moe                                        print_info: vocab_only       = 0                                        print_info: n_ctx_train      = 32768                                        print_info: n_embd           = 2048                                        print_info: n_layer          = 48                                        print_info: n_head           = 32                                        print_info: n_head_kv        = 4                                        print_info: n_rot            = 128                                        print_info: n_swa            = 0                                        print_info: n_swa_pattern    = 1                                        print_info: n_embd_head_k    = 128                                        print_info: n_embd_head_v    = 128                                        print_info: n_gqa            = 8                                        print_info: n_embd_k_gqa     = 512                                        print_info: n_embd_v_gqa     = 512                                        print_info: f_norm_eps       = 0.0e+00                                        print_info: f_norm_rms_eps   = 1.0e-06                                        print_info: f_clamp_kqv      = 0.0e+00                                        print_info: f_max_alibi_bias = 0.0e+00                                        print_info: f_logit_scale    = 0.0e+00                                        print_info: f_attn_scale     = 0.0e+00                                        print_info: n_ff             = 6144                                        print_info: n_expert         = 128                                        print_info: n_expert_used    = 8                                        print_info: causal attn      = 1                                        print_info: pooling type     = 0                                        print_info: rope type        = 2                                        print_info: rope scaling     = linear                                        print_info: freq_base_train  = 1000000.0                                        print_info: freq_scale_train = 1                                        print_info: n_ctx_orig_yarn  = 32768                                        print_info: rope_finetuned   = unknown                                        print_info: ssm_d_conv       = 0                                        print_info: ssm_d_inner      = 0                                        print_info: ssm_d_state      = 0                                        print_info: ssm_dt_rank      = 0                                        print_info: ssm_dt_b_c_rms   = 0                                        print_info: model type       = 30B.A3B                                        print_info: model params     = 30.53 B                                        print_info: general.name     = Qwen3 30B A3B                                        print_info: n_ff_exp         = 768                                        print_info: vocab type       = BPE                                        print_info: n_vocab          = 151936                                        print_info: n_merges         = 151387                                        print_info: BOS token        = 151643 '\u003c|endoftext|\u003e'                                        print_info: EOS token        = 151645 '\u003c|im_end|\u003e'                                        print_info: EOT token        = 151645 '\u003c|im_end|\u003e'                                        print_info: PAD token        = 151643 '\u003c|endoftext|\u003e'                                        print_info: LF token         = 198 'ƒä'                                        print_info: FIM PRE token    = 151659 '\u003c|fim_prefix|\u003e'                                        print_info: FIM SUF token    = 151661 '\u003c|fim_suffix|\u003e'                                        print_info: FIM MID token    = 151660 '\u003c|fim_middle|\u003e'                                        print_info: FIM PAD token    = 151662 '\u003c|fim_pad|\u003e'                                        print_info: FIM REP token    = 151663 '\u003c|repo_name|\u003e'                                        print_info: FIM SEP token    = 151664 '\u003c|file_sep|\u003e'                                        print_info: EOG token        = 151643 '\u003c|endoftext|\u003e'                                        print_info: EOG token        = 151645 '\u003c|im_end|\u003e'                                        print_info: EOG token        = 151662 '\u003c|fim_pad|\u003e'                                        print_info: EOG token        = 151663 '\u003c|repo_name|\u003e'                                        print_info: EOG token        = 151664 '\u003c|file_sep|\u003e'                                        print_info: max token length = 256                                        load_tensors: loading model tensors, this can take a while... (mmap = true)                                        load_tensors: offloading 48 repeating layers to GPU                                        load_tensors: offloading output layer to GPU                                        load_tensors: offloaded 49/49 layers to GPU                                        load_tensors: Metal_Mapped model buffer size = 17763.35 MiB                                        load_tensors:   CPU_Mapped model buffer size =   166.92 MiB                                        ....................................................................................................                                        llama_context: constructing llama_context                                        llama_context: n_seq_max     = 1                                        llama_context: n_ctx         = 20000                                        llama_context: n_ctx_per_seq = 20000                                        llama_context: n_batch       = 2048                                        llama_context: n_ubatch      = 512                                        llama_context: causal_attn   = 1                                        llama_context: flash_attn    = 0                                        llama_context: freq_base     = 1000000.0                                        llama_context: freq_scale    = 1                                        llama_context: n_ctx_per_seq (20000) \u003c n_ctx_train (32768) -- the full capacity of the model will not be utilized                                        ggml_metal_init: allocating                                        ggml_metal_init: found device: Apple M4 Pro                                        ggml_metal_init: picking default device: Apple M4 Pro                                        ggml_metal_load_library: using embedded metal library                                        ggml_metal_init: GPU name:   Apple M4 Pro                                        ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)                                        ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)                                        ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)                                        ggml_metal_init: simdgroup reduction   = true                                        ggml_metal_init: simdgroup matrix mul. = true                                        ggml_metal_init: has residency sets    = true                                        ggml_metal_init: has bfloat            = true                                        ggml_metal_init: use bfloat            = false                                        ggml_metal_init: hasUnifiedMemory      = true                                        ggml_metal_init: recommendedMaxWorkingSetSize  = 38654.71 MB                                        ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)                                        ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)                                        ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)                                        ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)                                        ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)                                        ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)                                        ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)                                        ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)                                        ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)                                        ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)                                        ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)                                        ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)                                        llama_context:        CPU  output buffer size =     0.58 MiB                                        init: kv_size = 20000, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 48, can_shift = 1                                        init:      Metal KV buffer size =  1875.00 MiB                                        llama_context: KV self size  = 1875.00 MiB, K (f16):  937.50 MiB, V (f16):  937.50 MiB                                        llama_context:      Metal compute buffer size =  1313.07 MiB                                        llama_context:        CPU compute buffer size =    43.07 MiB                                        llama_context: graph nodes  = 3126                                        llama_context: graph splits = 2                                        common_init_from_params: setting dry_penalty_last_n to ctx_size = 20000                                        common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)                                        common_chat_templates_init: failed to parse chat template (defaulting to chatml): Expected value expression at row 18, column 30:                                        {%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}                                        {%- for message in messages[::-1] %}                                                                     ^                                            {%- set index = (messages|length - 1) - loop.index0 %}                                                                                 main: llama threadpool init, n_threads = 10                                        main: chat template is available, enabling conversation mode (disable it with -no-cnv)                                        main: chat template example:                                        \u003c|im_start|\u003esystem                                        You are a helpful assistant\u003c|im_end|\u003e                                        \u003c|im_start|\u003euser                                        Hello\u003c|im_end|\u003e                                        \u003c|im_start|\u003eassistant                                        Hi there\u003c|im_end|\u003e                                        \u003c|im_start|\u003euser                                        How are you?\u003c|im_end|\u003e                                        \u003c|im_start|\u003eassistant                                                                                                                          system_info: n_threads = 10 (n_threads_batch = 10) / 14 | Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 |                                                                                  main: interactive mode on.                                        sampler seed: 153282782                                        sampler params:                                          repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000                                         dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 20000                                         top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800                                         mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000                                        sampler chain: logits -\u003e logit-bias -\u003e penalties -\u003e dry -\u003e top-k -\u003e typical -\u003e top-p -\u003e min-p -\u003e xtc -\u003e temp-ext -\u003e dist                                         generate: n_ctx = 20000, n_batch = 2048, n_predict = -1, n_keep = 0                                                                                 == Running in interactive mode. ==                                         - Press Ctrl+C to interject at any time.                                         - Press Return to return control to the AI.                                         - To return control without starting a new line, end your input with '/'.                                         - If you want to submit another line, end your input with '\\'.                                         - Not using system message. To change it, set a different value via -sys PROMPT                                                                                                                          \u003e Encoded text:                                         oyfjdnisdr rtqwainr acxz mynzbhhx                                         Decoded text:                                         Think step by step                                                                                  Encoded text:                                         oyekaijzdf aaptcg suaokybhai ouow aqht mynznvaatzacdfoulxxz                                         Decoded text:                                        \u003cthink\u003e                                        Okay, let's see. I need to figure out how to decode these encoded texts. The first one is \"oyfjdnisdr rtqwainr acxz mynzbhhx\" which decodes to \"Think step by step\". The second encoded text is \"oyekaijzdf aaptcg suaokybhai ouow aqht mynznvaatzacdfoulxxz\" and I need to find its decoded version.                                                                                  First, I should look at the first example to see if there's a pattern. The decoded text is \"Think step by step\". Let me write down the encoded and decoded words:                                                                                 Encoded: oyfjdnisdr rtqwainr acxz mynzbhhx                                        Decoded: Think step by step                                                                                 Let me check each word. Let's take the first encoded word \"oyfjdnisdr\" which becomes \"Think\". Let me count the letters. \"Think\" has 5 letters, but \"oyfjdnisdr\" has 10 letters. Wait, maybe each letter is shifted by a certain number? Or maybe it's a different cipher. Let me check the letters.                                                                                 Alternatively, maybe it's a substitution cipher where each letter is shifted by a certain value. Let me check the first word. Let's take the first letter of the encoded word and the first letter of the decoded word.                                                                                 Encoded first letter: o (ASCII 111)                                        Decoded first letter: T (ASCII 84)                                        Difference: 111 - 84 = 27. But that's more than 26, so maybe modulo 26? 27 mod 26 is 1. So maybe shifting back by 1? Let's check.                                                                                 If I shift 'o' (14th letter if a=0, but maybe a=1? Let's think in terms of A=0, B=1... Z=25. So o is 14, T is 19. Wait, that's different. Wait, T is the 19th letter if A=0. Wait, no, A is 0, B is 1, ..., T is 19. So if the encoded letter is o (14) and the decoded is T (19). So 14 + x = 19 mod 26. So x = 5. So shift forward by 5? Let's check.                                                                                 Wait, but if I shift o (14) by +5, that would be 19, which is T. Yes. Let's check the next letter. Encoded 'y' (24) shifted by +5 would be 24 +5=29 mod 26=3, which is D. The decoded word is 'Think' so the second letter is 'h'. Wait, h is 7. But 3 is D. That doesn't match. Hmm. So maybe that's not the case.                                                                                 Alternatively, maybe shift backwards. Let's try shifting o (14) back by 5: 14 -5=9, which is J. Not T. Hmm. So that doesn't work. Maybe another shift. Let's see. For the first letter, o (14) to T (19). So 19-14=5. So shift forward by 5. Let's check the second letter of the first word: encoded 'y' (24) shifted +5 is 24+5=29 mod26=3, which is D. The decoded word is 'Think' so second letter is 'h' (7). Doesn't match. So maybe not a simple shift.                                                                                 Alternatively, maybe it's a different cipher. Maybe a Vigen√®re cipher with a keyword? Or maybe a different approach. Let me check the length of the words. \"Think\" is 5 letters, but the encoded word \"oyfjdnisdr\" is 10 letters. Wait, that's double. Maybe each letter is split into two? Or maybe it's a different approach. Let me check the second word in the first example: \"rtqwainr\" decodes to \"step\". Let's check that. \"step\" is 4 letters, \"rtqwainr\" is 8 letters. Again, double. So perhaps each letter in the decoded word is represented by two letters in the encoded text? Maybe a cipher where each letter is replaced by two letters, perhaps a bifid cipher or something else.                                                                                 Alternatively, maybe it's a combination of two shifts. Let's think again. Let me check the first decoded word: \"Think\". Let's look at the encoded word \"oyfjdnisdr\". Maybe splitting into pairs? For example, \"oy\", \"fj\", \"dn\", \"is\", \"dr\". If each pair represents a letter. Let's see. For example, \"oy\" could be O (14) and Y (24). Maybe adding them? 14+24=38, mod 26 is 12, which is M. Not T. Hmm. Alternatively, multiplying? 14*24=336 mod26: 336 /26 is 12*26=312, 336-312=24, which is Y. Not T. Not helpful.                                                                                 Alternatively, maybe each pair is a letter in some other way. For example, using a Polybius square. If we have a 5x5 grid, but with 26 letters, maybe. But I'm not sure. Let me think. Alternatively, maybe each letter in the decoded word is shifted by a certain value, but the shift varies. But without more data, it's hard to tell.                                                                                 Alternatively, maybe the encoded text is a cipher where each letter is shifted by its position in the word. For example, first letter shifted by 1, second by 2, etc. Let's try for the first word \"oyfjdnisdr\" which is 10 letters. Let's take each letter and shift by its position. But the decoded word is \"Think\" which is 5 letters. So maybe the encoded word is twice the length. Maybe each decoded letter corresponds to two encoded letters. For example, for \"Think\", each letter is represented by two letters in the encoded text. Let's check.                                                                                 Take the first decoded letter T. Let's see what two encoded letters could represent T. Maybe the first two letters of the first encoded word: o and y. Let's see. If we take o (14) and y (24). Maybe some operation. Maybe adding them: 14+24=38 mod26=12, which is M. Not T. If we subtract: 24-14=10, which is K. Not T. If we do some other operation. Alternatively, maybe using the first letter as the row and the second as the column in a Polybius square. But without knowing the square, it's hard.                                                                                 Alternatively, maybe the encoded text is a cipher where each letter is shifted by a certain number, but the shift changes per letter. For example, a Vigen√®re cipher with a keyword. But since we only have one example, it's hard to find the keyword. Let me try to see if the first example could be a Vigen√®re cipher.                                                                                 The decoded text is \"Think step by step\". Let's take the first word \"Think\" and compare with the first encoded word \"oyfjdnisdr\". Let's see the letters:                                                                                 T (19) ‚Üí o (14)                                        h (7) ‚Üí y (24)                                        i (8) ‚Üí f (5)                                        n (13) ‚Üí j (9)                                        k (10) ‚Üí d (3)                                                                                 Wait, if I look at the differences between the encoded and decoded letters (assuming shift):                                                                                 For T (19) ‚Üí o (14): 14 -19 = -5 mod26=21. So shift of -5 or +21.                                                                                 h (7) ‚Üí y (24): 24-7=17. So shift +17.                                                                                 i (8) ‚Üí f (5): 5-8=-3=23.                                                                                 n (13) ‚Üí j (9): 9-13=-4=22.                                                                                 k (10) ‚Üí d (3): 3-10=-7=19.                                                                                 This seems inconsistent. So not a simple Vigen√®re cipher with a repeating key. Maybe another approach.                                                                                 Alternatively, maybe the letters are reversed. For example, \"Think\" reversed is \"kniht\". But the encoded word is \"oyfjdnisdr\". Doesn't seem to match.                                                                                 Alternatively, maybe the encoded text is a combination of letters from the decoded text. For example, taking every other letter or something. But I don't see it.                                                                                 Wait, maybe it's a Caesar cipher with a shift of +5. Let's check:                                                                                 T (19) shifted by +5: 19+5=24 ‚Üí Y                                        h (7)+5=12 ‚Üí M                                        i (8)+5=13 ‚Üí N                                        n (13)+5=18 ‚Üí S                                        k (10)+5=15 ‚Üí P                                        So \"Think\" would become YMNSP. But the encoded word is \"oyfjdnisdr\", which doesn't match. So that's not it.                                                                                 Alternatively, maybe a shift of -5:                                                                                 T (19)-5=14 ‚Üí O                                        h (7)-5=2 ‚Üí C                                        i (8)-5=3 ‚Üí D                                        n (13)-5=8 ‚Üí I                                        k (10)-5=5 ‚Üí F                                        So \"Think\" would be O C D I F ‚Üí \"OCDIF\", which doesn't match \"oyfjdnisdr\".                                                                                 Not matching. Hmm.                                                                                 Wait, maybe the encoded text is split into two parts. For example, the first encoded word \"oyfjdnisdr\" has 10 letters, which could be two 5-letter words. Let's try splitting it into \"oyfjd\" and \"nisdr\". Then see if those can be decoded to \"Think\" somehow. Not sure.                                                                                 Alternatively, maybe it's a cipher where each letter is shifted by its position in the word. For example, first letter shifted by 1, second by 2, etc. Let's try for the first word \"oyfjdnisdr\" (10 letters):                                                                                 But the decoded word is \"Think\" which is 5 letters. So maybe each decoded letter corresponds to two encoded letters. Let's take the first two letters of the encoded word: o and y. Maybe o (14) + y (24) = 38 mod26=12 ‚Üí M. Not T. Or o (14) - y (24) = -10 mod26=16 ‚Üí Q. Not T. Not helpful.                                                                                 Alternatively, maybe each pair of letters represents a letter through a different method. Like, using the first letter as the row and the second as the column in a 5x5 grid. But without knowing the grid, this is hard. Maybe using a keyword to generate the grid. But again, without more info, it's tricky.                                                                                 Alternatively, maybe the encoded text is a cipher where each letter is shifted by a certain value based on the position in the decoded word. For example, the first letter of the decoded word is shifted by x, the second by y, etc. But again, not enough data.                                                                                 Wait, the decoded text is \"Think step by step\". Let's look at the second encoded word \"rtqwainr\" which decodes to \"step\". Let's check that. \"step\" is 4 letters, and \"rtqwainr\" is 8 letters. Again, maybe each letter is represented by two letters. Let's take the first two letters of \"rtqwainr\": r and t. If I do something with them to get 's' (18). Let's see: r (17) and t (19). 17 + 19 = 36 mod26=10 ‚Üí K. Not s. 17-19=-2=24 ‚Üí Y. No. 17*19=323 mod26: 323 /26 is 12*26=312, 323-312=11 ‚Üí L. Not helpful.                                                                                 Alternatively, maybe the letters are shifted by their position in the word. For example, for the first letter of the word \"step\" (s), which is the first letter, so maybe shift by 1. Let's say the encoded letter is r (17), shifted by +1: 18 ‚Üí s. Yes! That works. Let's check the next letter in \"step\": t. The corresponding encoded letters are t and q (from \"rtqwainr\"). Wait, the second pair would be t and q. If the second letter of the decoded word is t, maybe shifted by 2. So encoded letter q (16) shifted by +2: 18 ‚Üí t. Yes! That works. Then third letter of \"step\" is e. The third pair would be w and a. Wait, the encoded word is \"rtqwainr\", so the pairs are rt, qw, ai, nr. So for the third pair, ai: a (0) and i (8). If the third letter of the decoded word is e (4), and the shift is +3. So maybe take the first letter of the pair (a=0) +3=3 ‚Üí D, not e. Or the second letter of the pair (i=8) +3=11 ‚Üí L. No. Alternatively, maybe take the first letter of the pair and shift by position. For the third pair (ai), position 3, so a (0) +3=3 ‚Üí D. Not e. Not matching.                                                                                  Wait, the first pair (rt): r (17) shifted by +1 (position 1) gives 18 ‚Üí s. Second pair (qw): q (16) +2 (position 2) = 18 ‚Üí t. Third pair (ai): a (0) +3 (position 3) = 3 ‚Üí D. But the third letter of \"step\" is e (4). Not matching. Hmm. Maybe the shift is different. What if the shift is position + something? Or maybe the shift is based on the letter's position in the word.                                                                                 Alternatively, maybe it's the second letter of the pair. For the first pair rt: t (19), shift by 1: 19 +1=20 ‚Üí u. No. Not s. Alternatively, r (17) shifted by +1 (position 1) is s. That worked. So maybe the first letter of each pair is shifted by the position in the word (1,2,3,4). Let's try:                                                                                 For the first pair (r and t), take the first letter r (17) +1 = s (18). Correct for 's'.                                                                                 Second pair (q and w): first letter q (16) +2=18 ‚Üí t. Correct for 't'.                                                                                 Third pair (a and i): first letter a (0) +3=3 ‚Üí D. But we need 'e' (4). Not matching. Hmm. What if it's the second letter of the pair? For third pair: i (8) +3=11 ‚Üí L. No. Not e.                                                                                 Alternatively, maybe it's the average of the two letters. For the third pair: a (0) and i (8). (0+8)/2=4 ‚Üí e. Yes! That works. Let's check:                                                                                 First pair: r (17) and t (19). (17+19)/2 = 36/2=18 ‚Üí s. Correct.                                                                                 Second pair: q (16) and w (22). (16+22)/2 = 19 ‚Üí t. Correct.                                                                                 Third pair: a (0) and i (8). (0+8)/2=4 ‚Üí e. Correct.                                                                                 Fourth pair: n (13) and r (17). (13+17)/2=15 ‚Üí p. Correct! So \"step\" is s, t, e, p. This seems to work.                                                                                 So the pattern here is that each pair of letters in the encoded word is averaged to get the decoded letter. Let's verify with the first encoded word \"oyfjdnisdr\" which decodes to \"Think\".                                                                                 Split the encoded word into pairs: oy, fj, dn, is, dr.                                                                                 Let's compute the average of each pair:                                                                                 oy: o (14) and y (24). (14 +24)/2 = 38/2 = 19 ‚Üí T. Correct.                                                                                 fj: f (5) and j (9). (5+9)/2 = 7 ‚Üí H. Correct.                                                                                 dn: d (3) and n (13). (3+13)/2 = 8 ‚Üí I. Correct.                                                                                 is: i (8) and s (18). (8+18)/2 = 13 ‚Üí N. Correct.                                                                                 dr: d (3) and r (17). (3+17)/2 = 10 ‚Üí K. Correct.                                                                                 Yes! That works. So the decoding method is to split the encoded text into pairs of letters, average their positions (using A=0, B=1, etc.), and then convert back to letters.                                                                                 So for the second encoded text: \"oyekaijzdf aaptcg suaokybhai ouow aqht mynznvaatzacdfoulxxz\"                                                                                 Let's decode it using this method.                                                                                 First, split each word into pairs. Let's take each word one by one.                                                                                 First word: \"oyekaijzdf\"                                                                                 Split into pairs: oy, ek, ai, jz, df.                                                                                 Compute each pair:                                                                                 oy: o(14) + y(24) = 38/2 = 19 ‚Üí T                                                                                 ek: e(4) + k(10) = 14/2 =7 ‚Üí H                                                                                 ai: a(0) + i(8) = 8/2=4 ‚Üí E                                                                                 jz: j(9) + z(25) = 34/2=17 ‚Üí R                                                                                 df: d(3) + f(5) = 8/2=4 ‚Üí E                                                                                 So the first word decodes to T H E R E ‚Üí \"There\"                                                                                 Next word: \"aaptcg\" ‚Üí split into aa, pt, cg.                                                                                 aa: a(0)+a(0)=0 ‚Üí A                                                                                 pt: p(15) + t(19) =34/2=17 ‚Üí R                                                                                 cg: c(2) + g(6)=8/2=4 ‚Üí E                                                                                 So \"aaptcg\" ‚Üí A R E ‚Üí \"Are\"                                                                                 Next word: \"suaokybhai\" ‚Üí split into su, ao, ky, bh, ai.                                                                                 su: s(18)+u(20)=38/2=19 ‚Üí T                                                                                 ao: a(0)+o(14)=14/2=7 ‚Üí H                                                                                 ky: k(10)+y(24)=34/2=17 ‚Üí R                                                                                 bh: b(1)+h(7)=8/2=4 ‚Üí E                                                                                 ai: a(0)+i(8)=8/2=4 ‚Üí E                                                                                 So \"suaokybhai\" ‚Üí T H R E E ‚Üí \"Three\"                                                                                 Next word: \"ouow\" ‚Üí split into ou, ow.                                                                                 ou: o(14)+u(20)=34/2=17 ‚Üí R                                                                                 ow: o(14)+w(22)=36/2=18 ‚Üí S                                                                                 So \"ouow\" ‚Üí R S ‚Üí \"Rs\" but maybe \"rows\"? Wait, but \"ouow\" is four letters. Wait, \"ouow\" split into \"ou\" and \"ow\". So R and S ‚Üí \"RS\". But the decoded text might be \"rows\"? But that would need \"r o w s\", but the decoded letters are R and S. Hmm, maybe it's \"rs\" but that's not a word. Wait, maybe I made a mistake.                                                                                 Wait, \"ouow\" is \"o\", \"u\", \"o\", \"w\". Split into \"ou\" and \"ow\". \"ou\" is o(14) and u(20) ‚Üí (14+20)/2=17 ‚Üí R. \"ow\" is o(14) and w(22) ‚Üí (14+22)/2=18 ‚Üí S. So \"RS\". But maybe the original decoded text is \"rows\"? But \"rows\" is four letters. However, the encoded word is four letters, which is two pairs. So it's two letters. So \"RS\". But in the example, the first decoded text was \"Think step by step\". So maybe the decoded text here is \"RS\" but that seems odd. Wait, let me check the next words to see if there's a pattern.                                                                                 Next word: \"aqht\" ‚Üí split into aq, ht.                                                                                 aq: a(0) + q(16) =16/2=8 ‚Üí I                                                                                 ht: h(7) + t(19)=26/2=13 ‚Üí N                                                                                 So \"aqht\" ‚Üí I N ‚Üí \"In\"                                                                                 Last word: \"mynznvaatzacdfoulxxz\" ‚Üí split into pairs:                                                                                 my, nz, nv, aa, tz, ac, df, ou, lx, xz.                                                                                 Wait, let's split the entire word step by step. The word is \"mynznvaatzacdfoulxxz\". Let's count the letters: m y n z n v a a t z a c d f o u l x x z ‚Üí that's 20 letters. So 10 pairs.                                                                                 Let me list them:                                                                                 1. my                                        2. nz                                        3. nv                                        4. aa                                        5. tz                                        6. ac                                        7. df                                        8. ou                                        9. lx                                        10. xz                                                                                 Now compute each pair:                                                                                 1. my: m(12) + y(24) = 36/2=18 ‚Üí S                                        2. nz: n(13) + z(25) = 38/2=19 ‚Üí T                                        3. nv: n(13) + v(21) = 34/2=17 ‚Üí R                                        4. aa: a(0) + a(0) =0 ‚Üí A                                        5. tz: t(19) + z(25) =44/2=22 ‚Üí W                                        6. ac: a(0) + c(2) =2/2=1 ‚Üí B                                        7. df: d(3) + f(5) =8/2=4 ‚Üí E                                        8. ou: o(14) + u(20) =34/2=17 ‚Üí R                                        9. lx: l(11) + x(23) =34/2=17 ‚Üí R                                        10. xz: x(23) + z(25) =48/2=24 ‚Üí Y                                                                                 So the decoded letters are S, T, R, A, W, B, E, R, R, Y ‚Üí \"STRAWBERRY\"                                                                                 Putting it all together:                                                                                 First word: There                                                                                 Second: Are                                                                                 Third: Three                                                                                 Fourth: RS (but maybe it's \"row\"? Wait, but the decoded letters are R and S. Maybe it's \"RS\", but maybe there's a mistake. Wait, the original example had \"Think step by step\", which is \"Think step by step\". Let me check if the decoded text for the second encoded text is \"There are three rs in strawberry\"? But that seems a bit forced. Alternatively, maybe I made a mistake with the \"ouow\" part.                                                                                 Wait, let me recheck the \"ouow\" word. The encoded word is \"ouow\", which is split into \"ou\" and \"ow\". \"ou\" is o(14) + u(20) = 17 ‚Üí R. \"ow\" is o(14) + w(22) = 18 ‚Üí S. So RS. But maybe it's supposed to be \"rows\" but that would require four letters. However, the encoded word is four letters, which is two pairs. So maybe it's \"RS\". But in the context of the other words, perhaps it's part of a longer phrase. Let me see the full decoded text so far:                                                                                 \"There Are Three RS In Strawberry\" ‚Äì but that doesn't make sense. Wait, the last word is \"STRAWBERRY\". So maybe the full decoded text is \"There are three rs in strawberry\" ‚Äì but \"rs\" might be part of the word. Wait, but \"strawberry\" has an 's' and a 'r', but the decoded part is \"RS\". Maybe it's \"There are three R S in strawberry\". But that seems odd. Alternatively, maybe I made a mistake in the decoding of \"ouow\".                                                                                 Wait, let's think again. Maybe the split is different? For \"ouow\", could it be split as \"o\", \"u\", \"o\", \"w\"? But the method requires pairs. So it has to be two pairs: \"ou\" and \"ow\".                                                                                 Alternatively, maybe the encoded text has a typo, but assuming it's correct, then \"RS\" is the decoded part. Let me check the other parts. The decoded text so far is:                                                                                 There Are Three RS In Strawberry. But \"RS\" could be a typo or maybe it's supposed to be \"rows\" but that would require a different split. Alternatively, maybe I missed something.                                                                                 Wait, the last word is \"mynznvaatzacdfoulxxz\" which decodes to \"STRAWBERRY\". So the entire decoded text would be:                                                                                 \"There Are Three RS In Strawberry\" ‚Äì but that doesn't make sense. Maybe there's a mistake in the decoding of the fourth word. Let me recheck the fourth word: \"ouow\" ‚Üí RS. Let's see the other parts.                                                                                 Wait, the second encoded text is \"oyekaijzdf aaptcg suaokybhai ouow aqht mynznvaatzacdfoulxxz\"                                                                                 Decoded:                                                                                 \"there are three rs in strawberry\"? But maybe the actual decoded text is \"There are three rs in strawberry\". However, \"rs\" might be part of the word. Alternatively, maybe there's a mistake in the decoding of \"ouow\".                                                                                 Wait, maybe the word \"ouow\" is actually \"roww\"? But no, the encoded text is \"ouow\". Alternatively, maybe the split is different. If it's \"o\", \"u\", \"o\", \"w\", but we need pairs. So \"ou\" and \"ow\".                                                                                 Alternatively, maybe the encoded text for the fourth word is \"ouow\" and the decoded is \"row\". But how? Let's think. If \"ouow\" is split into \"ou\" and \"ow\", and we get R and S. But if there's a different method, maybe it's not averaging. But earlier examples showed that the averaging method works. So perhaps the decoded text is \"There are three rs in strawberry\", which is a bit odd, but maybe that's the intended answer. Alternatively, maybe I missed something.                                                                                 Alternatively, maybe the fourth word \"ouow\" is a typo and should be \"ouow\" which decodes to RS, but maybe the actual decoded text is \"rows\". Let's check: \"rows\" is r, o, w, s. To get that, the encoded word would need to be four letters that when paired average to r, o, w, s. But the current encoded word is \"ouow\", which is o, u, o, w. Split into ou and ow, which is r and s. So it's not possible. Unless there's a different method.                                                                                 Alternatively, maybe the split is different, like \"o\", \"uow\" but that's not pairs. So I think the method is correct, and the decoded text is \"There are three rs in strawberry\". But that seems like an odd phrase. However, given that the first example's decoded text was \"Think step by step\", which is a common phrase, maybe this is a similar case.                                                                                 Alternatively, perhaps the fourth word is \"row\" and the fifth word is \"in\", and the last word is \"strawberry\". Let me check the fifth word: \"aqht\" decodes to \"in\". Yes, as I found earlier: aq ‚Üí i, ht ‚Üí n. So \"in\". So putting it all together:                                                                                 \"There are three rs in strawberry\" ‚Äì but maybe it's \"There are three 'rs' in strawberry\", where 'rs' is a typo or something. Alternatively, maybe the fourth word is \"row\" and the fifth is \"in\", but I need to check.                                                                                 Wait, maybe the fourth word is \"ouow\" which is \"ou\" and \"ow\" ‚Üí R and S. If the decoded text is \"There are three RS in strawberry\", then RS could stand for something, but it's unclear. However, given the decoding method, this is the result. So the final decoded text would be \"There are three rs in strawberry\".                                                                                 But maybe I made a mistake in the splitting. Let me check again the entire encoded text:                                                                                 Encoded text: oyekaijzdf aaptcg suaokybhai ouow aqht mynznvaatzacdfoulxxz                                                                                 Decoded:                                                                                 oyekaijzdf ‚Üí There                                                                                 aaptcg ‚Üí Are                                                                                 suaokybhai ‚Üí Three                                                                                 ouow ‚Üí RS                                                                                 aqht ‚Üí In                                                                                 mynznvaatzacdfoulxxz ‚Üí Strawberry                                                                                 So the full decoded text is \"There are three rs in strawberry\". But maybe the actual intended answer is \"There are three 'r's in strawberry\", where \"rs\" is actually \"r's\". But in the decoding, \"rs\" would be two letters, but \"r's\" would require an apostrophe, which isn't in the encoded text. Alternatively, maybe the \"ouow\" is supposed to be \"rows\", and there's an error in the encoded text. But without more info, I have to go with the decoding method.                                                                                 Therefore, the decoded text is \"There are three rs in strawberry\".                                        \u003c/think\u003e                                                                                 To decode the given encoded text, we observe that each word in the encoded text is split into **pairs of letters**. For each pair, we compute the **average of their alphabetical positions** (with 'A' = 0, 'B' = 1, ..., 'Z' = 25) to determine the corresponding decoded letter.                                                                                 ---                                                                                 ### Step-by-step Decoding                                                                                 #### Encoded Word: `oyekaijzdf`                                        - `oy` ‚Üí (14 + 24)/2 = 19 ‚Üí **T**                                        - `ek` ‚Üí (4 + 10)/2 = 7 ‚Üí **H**                                        - `ai` ‚Üí (0 + 8)/2 = 4 ‚Üí **E**                                        - `jz` ‚Üí (9 + 25)/2 = 17 ‚Üí **R**                                        - `df` ‚Üí (3 + 5)/2 = 4 ‚Üí **E**                                                                                 **Decoded:** `There`                                                                                 ---                                                                                 #### Encoded Word: `aaptcg`                                        - `aa` ‚Üí (0 + 0)/2 = 0 ‚Üí **A**                                        - `pt` ‚Üí (15 + 19)/2 = 17 ‚Üí **R**                                        - `cg` ‚Üí (2 + 6)/2 = 4 ‚Üí **E**                                                                                 **Decoded:** `Are`                                                                                 ---                                                                                 #### Encoded Word: `suaokybhai`                                        - `su` ‚Üí (18 + 20)/2 = 19 ‚Üí **T**                                        - `ao` ‚Üí (0 + 14)/2 = 7 ‚Üí **H**                                        - `ky` ‚Üí (10 + 24)/2 = 17 ‚Üí **R**                                        - `bh` ‚Üí (1 + 7)/2 = 4 ‚Üí **E**                                        - `ai` ‚Üí (0 + 8)/2 = 4 ‚Üí **E**                                                                                 **Decoded:** `Three`                                                                                 ---                                                                                 #### Encoded Word: `ouow`                                        - `ou` ‚Üí (14 + 20)/2 = 17 ‚Üí **R**                                        - `ow` ‚Üí (14 + 22)/2 = 18 ‚Üí **S**                                                                                 **Decoded:** `RS`                                                                                 ---                                                                                 #### Encoded Word: `aqht`                                        - `aq` ‚Üí (0 + 16)/2 = 8 ‚Üí **I**                                        - `ht` ‚Üí (7 + 19)/2 = 13 ‚Üí **N**                                                                                 **Decoded:** `In`                                                                                 ---                                                                                 #### Encoded Word: `mynznvaatzacdfoulxxz`                                        Split into 10 pairs:                                                                                 - `my` ‚Üí (12 + 24)/2 = 18 ‚Üí **S**                                        - `nz` ‚Üí (13 + 25)/2 = 19 ‚Üí **T**                                        - `nv` ‚Üí (13 + 21)/2 = 17 ‚Üí **R**                                        - `aa` ‚Üí (0 + 0)/2 = 0 ‚Üí **A**                                        - `tz` ‚Üí (19 + 25)/2 = 22 ‚Üí **W**                                        - `ac` ‚Üí (0 + 2)/2 = 1 ‚Üí **B**                                        - `df` ‚Üí (3 + 5)/2 = 4 ‚Üí **E**                                        - `ou` ‚Üí (14 + 20)/2 = 17 ‚Üí **R**                                        - `lx` ‚Üí (11 + 23)/2 = 17 ‚Üí **R**                                        - `xz` ‚Üí (23 + 25)/2 = 24 ‚Üí **Y**                                                                                 **Decoded:** `Strawberry`                                                                                 ---                                                                                 ### Final Decoded Text                                                                                 Combining all the decoded parts:                                                                                 **\"There are three RS in strawberry\"**                                                                                 Note: \"RS\" appears to be a direct result of decoding the word `ouow`. If interpreted as \"r's\", it would align with the phrase \"There are three r's in strawberry\", which is a natural and meaningful statement. This suggests that the encoded word `ouow` may have been intended to represent \"r's\", possibly due to a typo or formatting choice.                                                                                 ---                                                                                 ### ‚úÖ Final Answer:                                                                                 **There are three r's in strawberry**                                                                                 \u003e                                         llama_perf_sampler_print:    sampling time =     544.25 ms /  7529 runs   (    0.07 ms per token, 13833.79 tokens per second)                                        llama_perf_context_print:        load time =     876.05 ms                                        llama_perf_context_print: prompt eval time =     530.15 ms /    74 tokens (    7.16 ms per token,   139.58 tokens per second)                                        llama_perf_context_print:        eval time =  288610.00 ms /  7454 runs   (   38.72 ms per token,    25.83 tokens per second)                                        llama_perf_context_print:       total time =  299825.18 ms /  7528 tokens",
      "summary": "\n\n\n\nThe article discusses the performance of Qwen3 30B A3B (4_0 quant) in solving a cipher problem first introduced in the OpenAI o1-preview technical paper. The model successfully solved the cipher in 5 minutes, significantly faster than QwQ, which took 32 minutes. The article includes detailed logs of the model's initialization and execution on an Apple M4 Pro GPU, highlighting its technical specifications and efficiency.",
      "title": "Qwen3 30B A3B (4_0 quant) solves the Cipher problem first showcased in the OpenAI o1-preview Technical Paper (https://openai.com/index/learning-to-reason-with-llms/). QwQ solved it in 32 minutes, while Qwen3 solved it in 5 minutes.",
      "entry_id": "t3_1kewkno",
      "processing_time_ms": 31467
    },
    {
      "url": "https://github.com/ikawrakow/ik_llama.cpp",
      "original_content": "ik_llama.cpp: llama.cpp fork with better CPU performance  TL;DR This repository is a fork of llama.cpp with better CPU and hybrid GPU/CPU performance, new SOTA quantization types, first-class Bitnet support, better DeepSeek performance via MLA, FlashMLA, fused MoE operations and tensor overrides for hybrid GPU/CPU inference, row-interleaved quant packing, etc. Latest News  May 12 2025: User can now control if/which operations with tensors held in RAM are offloaded to the GPU. See PR 405 May 12 2025: Compatibility issues with mainline llama.cpp GGUFs for DeepSeek models with MLA enabled were resolved in PR 394. The lower prompt processing performance resulting from using llama.cpp-style MLA GGUFs was recovered in PR 409. May 11 2025: üöÄ Slightly faster flash attention for DeepSeek models on CUDA, along with extending compatibility to Touring or newer GPUs. See PR 408 May 9 2025: Support for LlaMA-3-Nemotron models added, see PR 377 May 7 2025: üöÄ Faster TG for DeepSeek models with GPU or hybrid GPU/CPU inference. See PR 386 for details. Caveat: Ampere or newer Nvidia GPU required May 4 2025: üöÄ Significant token generation performance improvement on CUDA with Flash Attention for GQA models. For details and benchmarks see PR #370 April 29 2025: Qwen3 support added, see PR 355 April 26 2025: GLM-4 support added, see PR 344 April 26 2025: Command-A support added, see PR 341 April 22 2025: Support for the latest Microsoft Bitnet model added, see PR 337 April 21 2025: ik_llama.cpp builds and runs successfully on Android (using termux), see PR 336 April 17 2025: üöÄ Better CPU Flash Attention token generation performance, see PR 332 April 13 2025: IQ1_M quantization improvements, see PR 327 April 10 2025: LLaMA-4 support added, see PR 321. In the PR there are also some custom quantization recipes for L4-Scout provided. April 7 2025: IQ2_XS quantization improvements, see PR 312 April 3 2025: üöÄ Much faster MoE implementation on Metal, see PR 307 April 1 2025: Quantization improvements for Q2_K, Q4_K, Q5_K, Q4_1, Q5_1, see PR 302 March 28 2025: Quantization imrovements for Q4_0, Q5_0, Q6_0, Q3_K, Q6_K, IQ4_XS, IQ4_NL, see PR 295 March 25 2025: üöÄ Better MoE performance on CUDA March 23 2025: üöÄ Better batched processing speed for DeepSeek models March 22 2025: Gemma3 support added March 21 2025: üöÄ FlashMLA-3: fastest CPU-only inference for DeepSeek models March 18 2025: Reduce compute buffer size March 17 2025: üöÄ FlashMLA-2 performance improvements March 12 2025: Allow Q8_0 KV cache with FlashMLA-2 on CUDA March 10 2025: üöÄ Better TG performance for MoE models on CUDA March 9 2025: üöÄ FlashMLA on CUDA March 8 2025: üöÄ Faster FlashMLA CPU implementation March 7 2025: Custom quantization mixes using regular expressions March 5 2025: üöÄ FlashMLA on CUDA March 3 2025: üöÄ Introducing FlashMLA - MLA with Flash Attention March 1 2025: Smart Expert Reduction for faster DeepSeek inference Feb 27 2025: MLA without transposed cache Feb 25 2025: Tensor overrides for better control where model weights are stored (GPU or CPU) Feb 23 2025: üöÄ Fused FFN ops for faster MoE inference Feb 23 2025: sweep-bench - better performance benchmarking Feb 20 2025: üöÄ Fast GEMM/GEMV for IQ1_S Feb 19 2025: Q8_KV - new type for 8-bit KV-cache quantization Feb 13 2025: Allow Q8_0 quantized cache with MLA Feb 11 2025: üöÄ Flash Attention support for DeepSeek models Feb 9 2025: üöÄ MLA for DeepSeek models Jan 23 2025: DeepSeek-V3 support added  Resources There is no single point of reference describing all new ik_llama.cpp features. Pull requests often contain detailed information, so browsing the PRs is often the best way to learn about new features and how to use them. In addition  The Wiki page has performance comparisons to mainline llama.cpp This guide is a good place to start if you came here because of DeepSeek models This discussion is about running DeepSeek-V3/R1 on a 16 x 3090 setup This discussion describes the new quantization types available in ik_llama.cpp  Contributing Contributions in form of pull requests, issue submissions (bug reports, feature requests), or general discussions, are welcome. License MIT",
      "summary": "\n\n\n\n**Summary:**  \n*ik_llama.cpp* is a fork of *llama.cpp* focused on enhancing CPU and hybrid GPU/CPU performance, featuring new state-of-the-art quantization methods, Bitnet support, and optimizations like MLA, FlashMLA, and fused MoE operations. It includes numerous updates from April to May 2025, such as improved DeepSeek performance, faster token generation, and support for models like LLaMA-3-Nemotron, Qwen3, GLM-4, and Gemma3. The project emphasizes flexibility with tensor overrides, quantization customization, and detailed documentation via PRs and a wiki. It is licensed under MIT.",
      "title": "GitHub - ikawrakow/ik_llama.cpp: llama.cpp fork with additional SOTA quants and improved performance",
      "entry_id": "t3_1keoint",
      "processing_time_ms": 5948
    },
    {
      "url": "https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek",
      "original_content": "Kate SouleDirector, Technical Product Management, Granite                 IBM              We‚Äôre excited to present IBM Granite 4.0 Tiny Preview, a preliminary version of the smallest model in the upcoming Granite 4.0 family of language models, to the open source community.      Granite 4.0 Tiny Preview is extremely compact and compute efficient: at FP8 precision, several concurrent sessions performing long context (128K) tasks can be run on consumer grade hardware, including GPUs commonly available for under $350 USD.1  Though the model is only partially trained‚Äîit has only seen 2.5T of a planned 15T or more training tokens‚Äîit already offers performance rivaling that of IBM Granite 3.3 2B Instruct despite fewer active parameters and a roughly 72% reduction in memory requirements.2 We anticipate Granite 4.0 Tiny‚Äôs performance to be on par with that of Granite 3.3 8B Instruct by the time it has completed training and post-training.                              As its name suggests, Granite 4.0 Tiny will be among the smallest offerings in the Granite 4.0 model family. It will be officially released this summer as part of a model lineup that also includes Granite 4.0 Small and Granite 4.0 Medium. Granite 4.0 continues IBM‚Äôs firm commitment to making efficiency and practicality the cornerstone of its enterprise LLM development. This preliminary version of Granite 4.0 Tiny is now available on Hugging Face‚Äîthough we do not yet recommend the preview version for enterprise use‚Äîunder a standard Apache 2.0 license. Our intent is to allow even GPU-poor developers to experiment and tinker with the model on consumer-grade GPUs. The model‚Äôs novel architecture is pending support in Hugging Face transformers and vLLM, which we anticipate will be completed shortly for both projects. Official support to run this model locally through platform partners including Ollama and LMStudio is expected in time for the full model release later this summer.                                       Enterprise performance on consumer hardware                                 LLM memory requirements are often provided, literally and figuratively, without proper context. It‚Äôs not enough to know that a model can be successfully loaded into your GPU(s): you need to know that your hardware can handle the model at the context lengths that your use case requires. Furthermore, many enterprise use cases entail not a lone model deployment, but batch inferencing of multiple concurrent instances. Therefore, IBM endeavors to measure and report memory requirements with long context and concurrent sessions in mind.                         Granite 4.0 Tiny is one of the most memory-efficient language models available today. Even at very long contexts, several concurrent instances of Granite 4.0 Tiny can easily run on a modest consumer GPU.                 An all-new hybrid MoE architecture                                 Whereas prior generations of Granite LLMs utilized a conventional transformer architecture, all models in the Granite 4.0 family utilize a new hybrid Mamba-2/Transformer architecture, marrying the speed and efficiency of Mamba with the precision of transformer-based self-attention. Granite 4.0 Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts (MoE) model, with 7B total parameters and only 1B active parameters at inference time. Many of the innovations informing the Granite 4 architecture arose from IBM Research‚Äôs collaboration with the original Mamba creators on Bamba, an experimental open source hybrid model whose successor (Bamba v2) was released earlier this week.                                A very brief history of Mamba models                   Mamba is a type of state space model (SSM), introduced in 2023‚Äîabout 6 years after the debut of transformers in 2017. SSMs are conceptually similar to the recurrent neural networks (RNNs) that dominated natural language processing (NLP) in the pre-transformer era. They were originally designed to predict the next state of a continuous sequence (like an electrical signal) using only information from the current state, previous state, and range of possibilities (the state space). Though they‚Äôve been used across several domains for decades, SSMs share certain shortcomings with RNNs that, until recently, limited their potential for language modeling. Unlike the self-attention mechanism of transformers, conventional SSMs have no inherent ability to selectively focus on or ignore specific pieces of contextual information. So in 2023, Carnegie Mellon‚Äôs Albert Gu and Princeton‚Äôs Tri Dao introduced a type of structured state space sequence (‚ÄúS4‚Äù) neural network that adds a selection mechanism and a scan method (for computational efficiency)‚Äîabbreviated as an ‚ÄúS6‚Äù model‚Äîand achieved language modeling results competitive with transformers. They nicknamed their model ‚ÄúMamba‚Äù because, among other reasons, all of those S‚Äôs sound like a snake‚Äôs hiss. In 2024, Gu and Dao released Mamba-2, a simplified and optimized implementation of the Mamba architecture. Equally importantly, their technical paper fleshed out the compatibility between SSMs and self-attention.                                Mamba-2 vs. transformers                   Mamba‚Äôs major advantages over transformer-based models center on efficiency and speed. Transformers have a crucial weakness: the compute requirements of self-attention scale quadratically with context. In other words, each time your context length doubles, the attention mechanism doesn‚Äôt just use double the resources‚Äîit uses quadruple the resources. This ‚Äúquadratic bottleneck‚Äù increasingly throttles speed and performance as the context window (and corresponding KV-cache) grows. Conversely, Mamba‚Äôs computational needs scale linearly: if you double the length of an input sequence, Mamba uses only double the resources. Whereas self-attention must repeatedly compute the relevance of every previous token to each new token, Mamba simply maintains a condensed, fixed-size ‚Äúsummary‚Äù of prior context from prior tokens. As the model ‚Äúreads‚Äù each new token, it determines that token‚Äôs relevance, then updates (or doesn't update) the summary accordingly. Essentially, whereas self-attention retains every bit of information and then weights the influence of each based on their relevance, Mamba selectively retains only the relevant information. That being said, transformers‚Äô more memory-intensive and computationally redundant method has its own advantages. For instance, research has shown that transformers still outpace both Mamba and Mamba-2 on tasks requiring in-context learning (such as few-shot prompting), copying, or long-context reasoning.                                The best of both worlds                   Fortunately, the respective strengths of transformers and Mamba are not mutually exclusive. In the original Mamba-2 paper itself, authors Dao and Gu suggest that a hybrid model could exceed the performance of a pure transformer or SSM‚Äîa notion validated by NVIDIA research from last year. To explore this further, IBM Research collaborated with Dao and Gu themselves, along with the University of Illinois at Urbana-Champaign (UIUC)‚Äôs Minjia Zhang, on Bamba and Bamba V2. Bamba, in turn, informed many of the architectural elements of Granite 4.0. The Granite 4.0 MoE architecture employs 9 Mamba blocks for every 1 transformer block. In essence, the selectivity mechanisms of the Mamba blocks efficiently capture global context, which is then passed to transformer blocks that enable a more nuanced parsing of local context. The result is a dramatic reduction in memory usage and latency with no apparent tradeoff in performance. Granite 4.0 Tiny doubles down on these efficiency gains by implementing them within a compact, fine-grained mixture of experts (MoE) framework, comprising 7B total parameters and 64 experts, yielding 1B active parameters at inference time. Further details are available in Granite 4.0 Tiny Preview‚Äôs Hugging Face model card.                                       Unconstrained context length                                 One of the more tantalizing aspects of SSM-based language models is the theoretical ability to handle infinitely long sequences. But due to practical constraints, the word ‚Äútheoretical‚Äù typically does a lot of heavy lifting. One of those constraints, especially for hybrid-SSM models, comes from the positional encoding (PE) used to represent information about the order of words. PE adds computational steps, and research has shown that models using PE techniques such as rotary positional encoding (RoPE) struggle to generalize to sequences longer than what they‚Äôve seen in training.3 The Granite 4.0 architecture uses no positional encoding (NoPE). Our testing demonstrates convincingly that this has had no adverse effect on long-context performance. At present, we have already validated Tiny Preview‚Äôs long-context performance for at least 128K tokens, and expect to validate similar performance on significantly longer context lengths by the time the model has completed training and post-training. It‚Äôs worth noting that a key challenge in definitively validating performance on tasks in the neighborhood of 1M-token context is the scarcity of suitable datasets. The other practical constraint on Mamba context length is compute. Linear scaling is better than quadratic scaling, but it still adds up eventually. Here again, Granite 4.0 Tiny has two key advantages: Unlike PE, NoPE doesn‚Äôt add any additional computational burden to the attention mechanism in the model‚Äôs transformer layers.Granite 4.0 Tiny is extremely compact and efficient, leaving plenty of hardware space for linear scaling.¬† Put simply, the Granite 4.0 MoE architecture itself places no constraints on context length. It can go as far as your hardware will take you.                                       What's happening next                                 We‚Äôre excited to continue pre-training Granite 4.0 Tiny, given such promising results so early in the process. We‚Äôre also excited to apply our learnings from post-training Granite 3.3, particularly with regard to reasoning capabilities and complex instruction following, to the new models. Like its predecessors in Granite 3.2 and Granite 3.3, Granite 4.0 Tiny Preview offers toggleablethinking on andthinking off functionality (though its reasoning-focused post-training is very much incomplete). More information about new developments in the Granite Series will be presented at IBM Think 2025, as well as in the weeks and months to follow. Check out Granite 4.0 Tiny Preview on Hugging Face ‚Üí",
      "summary": "\n\n\n\nIBM has introduced the **Granite 4.0 Tiny Preview**, a compact and efficient language model in its upcoming Granite 4.0 family, designed for consumer-grade hardware. Despite being partially trained, it matches the performance of larger models while using significantly less memory. The model uses a **hybrid Mamba-2/Transformer architecture**, combining the efficiency of Mamba with the precision of transformers, and features a **fine-grained mixture of experts (MoE)** framework. It supports long contexts (up to 128K tokens) and multiple concurrent sessions on modest GPUs. The preview is available on Hugging Face, with full enterprise support expected later this summer. The model's **NoPE (no positional encoding)** approach allows flexible context lengths, and IBM plans to continue training it for improved performance.",
      "title": "IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models",
      "entry_id": "t3_1kedu0d",
      "processing_time_ms": 7897
    }
  ],
  "persona": {
    "Name": "LocalLLaMa",
    "FeedURL": "https://reddit.com/r/localllama.rss",
    "Topic": "AI Technology and Large Language Models",
    "PersonaIdentity": "an AI researcher and enthusiast who loves diving deep into technical details.",
    "BasePromptTask": "Your job is to curate the best AI Technology and Large Language Models news items and create an engaging newsletter.",
    "SummaryPromptTask": "Your task is to analyze multiple AI Technology and Large Language Models news items and create a comprehensive newsletter-style overview.",
    "FocusAreas": [
      "New LLM models, runners or other infrastructure being released or open sourced",
      "Big AI lab news (OpenAI, Anthropic, etc.)",
      "Security news"
    ],
    "RelevanceCriteria": [
      "Contains specific technical details or specifications",
      "Explains the significance and impact of the development",
      "Includes performance metrics, benchmarks, or comparisons",
      "Discusses novel approaches or techniques"
    ],
    "SummaryAnalysis": [
      "Major advances in model architecture or capabilities",
      "Emerging trends in open-source LLM development",
      "Notable performance breakthroughs",
      "Industry impact and adoption patterns"
    ],
    "ExclusionCriteria": [
      "Content entirely unrelated to AI/ML technology",
      "Questions that would be better suited for tech support",
      "Duplicate posts of already covered developments",
      "Random complaints without technical content",
      "Humor posts"
    ]
  },
  "benchmark_date": "2025-05-17T12:48:49.56277657+10:00",
  "total_processing_time_ms": 430073,
  "entry_total_processing_time_ms": 178806,
  "image_total_processing_time_ms": 189464,
  "web_content_total_processing_time_ms": 61801,
  "success_rate": 1
}