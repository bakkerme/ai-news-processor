{
  "post_id": "1lmk2dj",
  "fetched_at": "2025-06-29T12:13:59.321372742+10:00",
  "comments": [
    {
      "id": "n084h9h",
      "body": "Uh, is it not a bit early to call progress stalled when the top 5 models are about 2-3 months old?",
      "parent_id": "t3_1lmk2dj",
      "author": "Brilliant-Weekend-68",
      "score": 189,
      "created": "2025-06-28T11:32:34Z"
    },
    {
      "id": "n083etv",
      "body": "Yes I think so. For my use cases I don't care about reasoning and I noticed that they haven't improved for a while. That being said small models ARE improving, which is pretty good for running them locally.",
      "parent_id": "t3_1lmk2dj",
      "author": "ArcaneThoughts",
      "score": 67,
      "created": "2025-06-28T11:23:42Z"
    },
    {
      "id": "n085731",
      "body": "Does \"Qwen3 /no\\_think\" count as non-reasoning?",
      "parent_id": "t3_1lmk2dj",
      "author": "MokoshHydro",
      "score": 16,
      "created": "2025-06-28T11:38:19Z"
    },
    {
      "id": "n083x4t",
      "body": "More like progress stalled with non-reasoning models in general.",
      "parent_id": "t3_1lmk2dj",
      "author": "pip25hu",
      "score": 12,
      "created": "2025-06-28T11:27:49Z"
    },
    {
      "id": "n0a4305",
      "body": "Not at all, look at the parameter counts of these models. We are getting performance above the 110B Command A from Mistral Small 3.2 24B and Qwen 3 32B. There's definitely stagnation on the high-end, but we're able to accomplish with the high-end models do with increasingly less and less parameters",
      "parent_id": "t3_1lmk2dj",
      "author": "ArsNeph",
      "score": 6,
      "created": "2025-06-28T18:22:15Z"
    },
    {
      "id": "n08qzpg",
      "body": "Progress is stalled in non-reasoning models in general. If you focus in the Artificial Analysis Intelligence Index then DeepSeek V3 is the best non-reasoning model in both closed and open source.\n\nI think it’s just difficult to keep making non-reasoning smarter without going bigger. I think the only non-reasoning models I like more than V3 is GPT 4.1 and Sonnet 4, both are more than 8x more expensive so likely way bigger. Regardless they aren’t exactly smarter than V3 they just are better for some of my use cases.",
      "parent_id": "t3_1lmk2dj",
      "author": "MKU64",
      "score": 13,
      "created": "2025-06-28T14:04:13Z"
    },
    {
      "id": "n08a3sv",
      "body": "definitely not stalled, compare dsv3.1 to even closed sourced non reasoning models,  it is highly competitive and this was only a few months ago, look at mistral small 3.2 and compare it to mistral small 3.1's scores, it is way smarter",
      "parent_id": "t3_1lmk2dj",
      "author": "myvirtualrealitymask",
      "score": 6,
      "created": "2025-06-28T12:15:20Z"
    },
    {
      "id": "n08k3ma",
      "body": "they are focusing on \"reasoning\" language models and image gen ones to produce much more high quality data which will then be fed to classic LM/VLMs.\n\nin the next months we're gonna see new releases for sure.\n\nclassic models is what 99% of people need to build applications. so dont worry",
      "parent_id": "t3_1lmk2dj",
      "author": "masc98",
      "score": 5,
      "created": "2025-06-28T13:22:24Z"
    },
    {
      "id": "n08f2ju",
      "body": "sorry, but in your benchmark, QWEN3 has no think mode, and Mistral small has gained a lot, look at that Mistral large 2 published just half year ago.",
      "parent_id": "t3_1lmk2dj",
      "author": "Asleep-Ratio7535",
      "score": 2,
      "created": "2025-06-28T12:49:53Z"
    },
    {
      "id": "n09ghd3",
      "body": "In my experience, only the most recently released non-reasoning models have been both smart enough and fast enough to be helpful with eg. statitical programming tasks, vs just being so incorrect or taking so long that it wasn't worth it. I felt like only very very recently have there been \"good enough\" local models for my use cases.\n\nBut as they say, YMMV!",
      "parent_id": "t3_1lmk2dj",
      "author": "RobotRobotWhatDoUSee",
      "score": 2,
      "created": "2025-06-28T16:20:31Z"
    },
    {
      "id": "n0ayzau",
      "body": "Synthetic data causes this. We need better and evolving datasets (by evolving I mean daily snapshots of internet where actual and rich discussions are being held, not Reddit LULE)",
      "parent_id": "t3_1lmk2dj",
      "author": "spawncampinitiated",
      "score": 2,
      "created": "2025-06-28T21:08:26Z"
    },
    {
      "id": "n0b3y84",
      "body": "these results and others are showing that we are approaching a fundamental efficacy limit for models that work primarily through a layer of natural language\n\n\nhttps://arxiv.org/abs/2506.10077",
      "parent_id": "t3_1lmk2dj",
      "author": "BidWestern1056",
      "score": 2,
      "created": "2025-06-28T21:36:21Z"
    },
    {
      "id": "n0b797o",
      "body": "y'all. it's 2025. this shit is still brand new, and it's peak vacation season. chill.",
      "parent_id": "t3_1lmk2dj",
      "author": "DigThatData",
      "score": 2,
      "created": "2025-06-28T21:55:18Z"
    },
    {
      "id": "n0b9v4u",
      "body": "Where is my girl Gemma-3?   \nSeriously, I've been dragging her through mud and she is something else.  In my opinion (which as we know is worth nothing) it is the best model that appeared in a long time. 130k context! Vision included! Finetunes like a butter. (Yeah, I know, I'm strong in analogies)",
      "parent_id": "t3_1lmk2dj",
      "author": "FPham",
      "score": 2,
      "created": "2025-06-28T22:10:31Z"
    },
    {
      "id": "n085c7p",
      "body": "I don't really get large non-reasoning models anymore. If I have a large database and a small, very clever reasoning model, why do I need a large model? I mean what for? The small model can use the database and it can mine VERY niche knowledge. It can use that mined knowledge and develop it.",
      "parent_id": "t3_1lmk2dj",
      "author": "custodiam99",
      "score": 4,
      "created": "2025-06-28T11:39:26Z"
    },
    {
      "id": "n09c9bq",
      "body": "Stuff has been incremental for ages. Not just open source.\n\nPeople often say \"nuh-uh\" because it improved on their particular application or they still buy into benchmarks.\n\nThe focus has shifted to getting small models better and math/stem maxxing at the expense of everything else. Probably next thing will be pushing agents, which has already started.",
      "parent_id": "t3_1lmk2dj",
      "author": "a_beautiful_rhind",
      "score": 2,
      "created": "2025-06-28T15:58:03Z"
    },
    {
      "id": "n08z96d",
      "body": "I switched back from deepseek r1 0528 to deepseek v3 because I didn't feel like waiting for all the reasoning tokens and v3 is very close to r1 anyway for most stuff that I need it for. It seriously feels like a cheat code though. It's at the top because it truly feels like having Claude at home.",
      "parent_id": "t3_1lmk2dj",
      "author": "Hoodfu",
      "score": 2,
      "created": "2025-06-28T14:49:36Z"
    },
    {
      "id": "n08jzt7",
      "body": "When did each of those models release?",
      "parent_id": "t3_1lmk2dj",
      "author": "yaosio",
      "score": 1,
      "created": "2025-06-28T13:21:43Z"
    },
    {
      "id": "n092psq",
      "body": "These are just the Open Source models, which is excluding a lot of the top models.",
      "parent_id": "t3_1lmk2dj",
      "author": "kaleNhearty",
      "score": 1,
      "created": "2025-06-28T15:08:02Z"
    },
    {
      "id": "n0apvwc",
      "body": "Closed source seems to be improving.",
      "parent_id": "t3_1lmk2dj",
      "author": "mythicinfinity",
      "score": 1,
      "created": "2025-06-28T20:19:19Z"
    },
    {
      "id": "n0c6t7x",
      "body": "So for all the trashing and the firing of people and researchers quitting that Llama4 got, its there at second place? With a model half the size of Deepseek?",
      "parent_id": "t3_1lmk2dj",
      "author": "ortegaalfredo",
      "score": 1,
      "created": "2025-06-29T01:34:44Z"
    },
    {
      "id": "n08tuqh",
      "body": "Reasoning is a meme and i wish it is abandoned.",
      "parent_id": "t3_1lmk2dj",
      "author": "Alkeryn",
      "score": -1,
      "created": "2025-06-28T14:20:19Z",
      "controversiality": 1
    },
    {
      "id": "n09fslc",
      "body": "Eschew reasoning? Blurting out the first thing that comes to its mind like a middle-schooler can only take an LLM so far.",
      "parent_id": "t3_1lmk2dj",
      "author": "michaelmalak",
      "score": 0,
      "created": "2025-06-28T16:16:51Z"
    },
    {
      "id": "n0875tt",
      "body": "Yeah, maybe if companies weren't chasing fresh trends just to show-off, and finished at least one general-purpose model as a solid product, this wouldn't happen. Instead, we have reasoning models that are wasteful and aren't as useful as they are advertised.\n\nLlama series has no model in sizes from 14b to 35b at all, Mistral and Google failed to train at least one stably-performing model in that size, others don't seem to care about anything of average size - it's either 4b and lower, or 70+b.\n\nConsidering improvements to architectures, even training an old-size (7b, 14b, 22b?) model would give a better result, you just need to focus on finishing at least one model instead of experimenting on every new hot idea. Without it, all these new cool architectures and improvements will never be fully explored and will never become effective.",
      "parent_id": "t3_1lmk2dj",
      "author": "dobomex761604",
      "score": -1,
      "created": "2025-06-28T11:53:28Z"
    },
    {
      "id": "n091yit",
      "body": "[deleted]",
      "parent_id": "t3_1lmk2dj",
      "author": "[deleted]",
      "score": 0,
      "created": "2025-06-28T15:04:00Z"
    },
    {
      "id": "n09ahym",
      "body": "Check new hunyuan model :)\n\nPlus given the power of mistral small and medium, upcoming large should re balance the cards ;)",
      "parent_id": "t3_1lmk2dj",
      "author": "AdventurousSwim1312",
      "score": 0,
      "created": "2025-06-28T15:48:46Z"
    },
    {
      "id": "n08j4p6",
      "body": "Gemma 3n, mistral small 3.2, qwen 3 are all incredible and new. The models are just getting denser. A year ago you would use llama 3.1 70b for the same results you'd get from an 8b model now. Most people are using llms on single gpus, or just paying for an online service, so it makes sense to lower the size of the open source models. Gemma 3n is equivalent to llama 3 70b, but has vision, 4x the context length, and runs on a phone cpu.",
      "parent_id": "t3_1lmk2dj",
      "author": "DataCraftsman",
      "score": -3,
      "created": "2025-06-28T13:16:13Z"
    },
    {
      "id": "n0b5hc4",
      "body": "Just be patient. These models aren’t usually developed by the local trillion dollar corporation. I use DeepSeek regularly without it feeling like it’s a major downgrade vs Gemini or ChatGPT",
      "parent_id": "t3_1lmk2dj",
      "author": "[deleted]",
      "score": -1,
      "created": "2025-06-28T21:45:07Z"
    },
    {
      "id": "n0a1jxo",
      "body": "A non reasoning model is akin to a person who is only allowed to give instantaneous intuitive answers to questions, with no opportunity to take multiple steps to find an answer. So their natural theoretical limit is the limit of intuition.",
      "parent_id": "t3_1lmk2dj",
      "author": "owenwp",
      "score": -2,
      "created": "2025-06-28T18:09:01Z"
    }
  ],
  "raw_api_url": "/comments/1lmk2dj"
}