{
  "post_id": "1lmfiu9",
  "fetched_at": "2025-06-29T11:05:44.940133611+10:00",
  "comments": [
    {
      "id": "n07cde5",
      "body": "Thanks for these details. Looking at these it seems to be that there's a flaw in the time measurement somewhere, and the scoring is potentially also not as good as it looks.\n\n\u0026gt;DeepSeek got faster answering¬†*its own*¬†questions (80 tokens/s vs. avg 40 tokens/s)\n\nYou didn't specify using a speculative decoding model (probably wouldn't have fit the 8GB in addition to the model anyway). The LLM answer generation speed is somewhat constant - at least at your 4k context length. If it suddenly generated tokens at twice the speed then there must be an oversight in execution (swapping to system RAM, background tasks, incorrect timing, measuring prompt processing time together with inference time, wrong model, etc).\n\n\u0026gt;Qwen3 4B took¬†8+ mins to generate a single Math question!\n\nAt 6 tokens per second that's too slow compared to the other models. You might not have run an apples to apples comparison here. Maybe not all models were quantized in the same format?\n\nSpeaking of quantization, this hurts the output quality quite a bit:\n\n\u0026gt;os.environ\\[\"OLLAMA\\_KV\\_CACHE\\_TYPE\"\\] = \"q4\\_0\"\n\nIt's OKish to set the V cache to Q4, but the K cache [should stay at least on Q8](https://www.reddit.com/r/LocalLLaMA/comments/1iuw1kx/comment/me15i32/?context=3).\n\n\u0026gt;Each model ... Evaluated every answer (including their own)\n\nIt would've been helpful for assessing the scoring to let two large, high quality models generate a few sets of questions and also perform a few evaluation runs over the output of the small models. By running multiple sets with the smaller models you could also measure the variance in generation quality - what if a model in your single run randomly performed pretty well, while another randomly generated low quality output?",
      "parent_id": "t3_1lmfiu9",
      "author": "Chromix_",
      "score": 22,
      "created": "2025-06-28T07:06:01Z"
    },
    {
      "id": "n07zml5",
      "body": "Please do humanity another favour and include Gemma 3N results as well ü•π",
      "parent_id": "t3_1lmfiu9",
      "author": "Ultimatepritam",
      "score": 12,
      "created": "2025-06-28T10:51:43Z"
    },
    {
      "id": "n077onj",
      "body": "gemma is good around all the tasks in general",
      "parent_id": "t3_1lmfiu9",
      "author": "horse_tinder",
      "score": 31,
      "created": "2025-06-28T06:23:40Z"
    },
    {
      "id": "n07mjv9",
      "body": "I think the reason the reasoning models like DeepSeek-R1-Distill-Qwen-1.5B is doing poorly is the short context length.\n\nReasoning models will often need 8K to perform reliability without being cut off / entering a loop.",
      "parent_id": "t3_1lmfiu9",
      "author": "Mysterious_Finish543",
      "score": 8,
      "created": "2025-06-28T08:45:00Z"
    },
    {
      "id": "n07eeb2",
      "body": "\u0026gt; Qwen3 4B generates 2‚Äì3x more tokens per answer\n\nIt's such a bloody chatty one.",
      "parent_id": "t3_1lmfiu9",
      "author": "chub79",
      "score": 5,
      "created": "2025-06-28T07:24:52Z"
    },
    {
      "id": "n087zu8",
      "body": "Have you tried Gemma 3n?\nDamn capable for it's size in my personal testing.",
      "parent_id": "t3_1lmfiu9",
      "author": "Randommaggy",
      "score": 3,
      "created": "2025-06-28T11:59:46Z"
    },
    {
      "id": "n078iwq",
      "body": "So if you had to pick just one to run, which would it be?\n\nYada yada ‚Äútask dependence‚Äù etc, we all get it. \n\nStill, which would you pick as your preferred all arounder?",
      "parent_id": "t3_1lmfiu9",
      "author": "Nonomomomo2",
      "score": 5,
      "created": "2025-06-28T06:31:08Z"
    },
    {
      "id": "n07ei35",
      "body": "I somehow really doubt that llama 3.2 1B beats 3.1 8B, except asking knowledge about very current stuff. Now, I don't know your benchmark questions and maybe it's the best for your usecase, but the relax small models being this good doesn't really track with my usage experience. They are great for their size though.   \nMaybe this just answers us the question if llms should numerically evaluate texts",
      "parent_id": "t3_1lmfiu9",
      "author": "LevianMcBirdo",
      "score": 2,
      "created": "2025-06-28T07:25:53Z"
    },
    {
      "id": "n07ld93",
      "body": "nice test thanks for sharing, have you been able to determine how much you used your HD swap memory when loading the 7b/8b models, do note that using swap for LLM is not recommended because you will rapidly deteriorate the HD",
      "parent_id": "t3_1lmfiu9",
      "author": "These-Dog6141",
      "score": 1,
      "created": "2025-06-28T08:33:19Z"
    },
    {
      "id": "n07nfnu",
      "body": "I like your plot, and I don't even own a Mac. Congrats, and thanks for the insight!",
      "parent_id": "t3_1lmfiu9",
      "author": "IrisColt",
      "score": 1,
      "created": "2025-06-28T08:53:46Z"
    },
    {
      "id": "n07ozqu",
      "body": "Yeah qwen3 8b is a beast. But it wouldn't be my choice for complex debugging that a 400b+ model struggles with",
      "parent_id": "t3_1lmfiu9",
      "author": "admajic",
      "score": 1,
      "created": "2025-06-28T09:09:16Z"
    },
    {
      "id": "n07ywct",
      "body": "I see Llama-3.1-8B still holding strong. Did you try Qwen3-8B without thinking?",
      "parent_id": "t3_1lmfiu9",
      "author": "entsnack",
      "score": 1,
      "created": "2025-06-28T10:45:15Z"
    },
    {
      "id": "n08bq4f",
      "body": "i ran Qwen 1.7b for a short time and thought it was too stupid for any use? what was your experience ?",
      "parent_id": "t3_1lmfiu9",
      "author": "FormalAd7367",
      "score": 1,
      "created": "2025-06-28T12:27:00Z"
    },
    {
      "id": "n08c2do",
      "body": "Can you explain how you measured average token/sec and the prompts for evaluation. Thanks",
      "parent_id": "t3_1lmfiu9",
      "author": "Prestigious_Wish_887",
      "score": 1,
      "created": "2025-06-28T12:29:22Z"
    },
    {
      "id": "n08hp1d",
      "body": "Very interesting results!\n\nWhat would you do if you had 16 or 32GB of RAM instead?",
      "parent_id": "t3_1lmfiu9",
      "author": "jinnyjuice",
      "score": 1,
      "created": "2025-06-28T13:07:02Z"
    },
    {
      "id": "n08msmh",
      "body": "Thanks for sharing this. Wish you tested bunch more models on this. Please try if you get a chance. Thanks again.\n\nFor example:\n\n1. Gemma3-4B (since Llama3.2-3B \u0026amp; Qwen3-4B in your list)\n2. Qwen3-0.6B (since Llama3.2-1B \u0026amp; Gemma3-1B in your list)\n3. Phi-4-mini-instruct (since Llama3.1-8B \u0026amp; Mistral-7B in your list)\n4. Jan-nano\n5. Granite3.3-2B\n6. MiniCPM4-0.5B (Optional)\n7. BitCPM4-1B (Optional)",
      "parent_id": "t3_1lmfiu9",
      "author": "pmttyji",
      "score": 1,
      "created": "2025-06-28T13:39:11Z"
    },
    {
      "id": "n08thmw",
      "body": "Thank you for this.",
      "parent_id": "t3_1lmfiu9",
      "author": "LowBall7666",
      "score": 1,
      "created": "2025-06-28T14:18:19Z"
    },
    {
      "id": "n08xbjz",
      "body": "If i understand right you are asking the models to evaluate the answers of the other models, and themselves. I think a really helpful upgrade would be to have a set of questions that have known answers and then have a model evaluate the answers against known correct answers.\n\nThere's a decent chance you'd have to do that yourself because a lot of benchmarks out there are overtrained in the models.",
      "parent_id": "t3_1lmfiu9",
      "author": "Over-Independent4414",
      "score": 1,
      "created": "2025-06-28T14:39:15Z"
    },
    {
      "id": "n08zszq",
      "body": "Do you have the source available? Would like to test on my M1 Macbook air.",
      "parent_id": "t3_1lmfiu9",
      "author": "BinaryHelix",
      "score": 1,
      "created": "2025-06-28T14:52:32Z"
    },
    {
      "id": "n08zz96",
      "body": "Wow, this is seriously impressive, especially on just 8GB RAM! I‚Äôve struggled to run anything over 3B smoothly on my own M1.\n\nThanks for the detailed breakdown. The self-evaluation part is really interesting, especially the bias detection and score inflation. I‚Äôm curious if you tried mixing models, like generating questions with one and evaluating with another.\n\nAlso, if you ever publish the full dataset or want help building a benchmark UI around it, I‚Äôd be happy to contribute!",
      "parent_id": "t3_1lmfiu9",
      "author": "Ok-Bid-1264",
      "score": 1,
      "created": "2025-06-28T14:53:28Z"
    },
    {
      "id": "n097jbl",
      "body": "You can use /nothink to prevent Qwen models from consuming reasoning tokens. Their behavior becomes more comparable to typical small models like that.",
      "parent_id": "t3_1lmfiu9",
      "author": "Glxblt76",
      "score": 1,
      "created": "2025-06-28T15:33:14Z"
    },
    {
      "id": "n098wnt",
      "body": "How about Mistral Nemo 12B at UD Q2KXL or UD Q3KXL? Should fit in ram with a good amount of Context especially with the more quantized model. Or even Gemma 3 12B.¬†",
      "parent_id": "t3_1lmfiu9",
      "author": "My_Unbiased_Opinion",
      "score": 1,
      "created": "2025-06-28T15:40:24Z"
    },
    {
      "id": "n0a1kn4",
      "body": "I'm not sure if this was mentioned, but is it possible to compare with Gemma3n just yet?\n\nI'm interested to see how it benchmarks next to OG Gemma3.",
      "parent_id": "t3_1lmfiu9",
      "author": "clduab11",
      "score": 1,
      "created": "2025-06-28T18:09:07Z"
    },
    {
      "id": "n0bwuh9",
      "body": "Which one is best for coding?",
      "parent_id": "t3_1lmfiu9",
      "author": "nareshdamera",
      "score": 1,
      "created": "2025-06-29T00:30:06Z"
    },
    {
      "id": "n085a5i",
      "body": "you can run qwen3:8b, qwen2.5-vl:7b and deepseek-qwen3:8b via MLX with 10-13 token/s, you'll need 6-7gb of free ssd space or it will freeze\n\nalso try whisper, parakeet, kokoro, chatterbox and facefusion\n\nm1 8gb is still strong and gives you an ability to check out new stuff, build and test workflows, you can scale when you get that m4 512gb mac mini. if someone considers m1 8gb and reads this - yeah it's not ideal, but it will enable you to start your local journey for a very low price",
      "parent_id": "t3_1lmfiu9",
      "author": "madaradess007",
      "score": 1,
      "created": "2025-06-28T11:38:59Z"
    },
    {
      "id": "n08asnf",
      "body": "no phi4 is a heresy, imo the best out of all of these",
      "parent_id": "t3_1lmfiu9",
      "author": "No-Source-9920",
      "score": 1,
      "created": "2025-06-28T12:20:21Z"
    },
    {
      "id": "n0795fi",
      "body": "[deleted]",
      "parent_id": "t3_1lmfiu9",
      "author": "[deleted]",
      "score": -9,
      "created": "2025-06-28T06:36:47Z"
    }
  ],
  "raw_api_url": "/comments/1lmfiu9"
}