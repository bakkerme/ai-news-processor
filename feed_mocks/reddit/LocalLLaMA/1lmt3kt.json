{
  "post_id": "1lmt3kt",
  "fetched_at": "2025-06-29T11:05:53.666575259+10:00",
  "comments": [
    {
      "id": "n0aaaam",
      "body": "In your case, when running an LLM on the CPU, the primary bottleneck is almost always **memory bandwidth**, not the CPU's raw processing power.  \nIf you are not using any GPU, your dual-channel DDR4-2666 RAM gives you a approx. maximum bandwidth of 41-42 GB/s.For dense models (not MOE) if you divide this number to your model's total memory usage(eg. model size, kv cache etc.) you will get your approx tps.\n\nMoreover, the slow prompt processing and 100% CPU usage are also classic symptoms of a memory-bound workload—the CPU is working as fast as it can, but it's constantly waiting for data from the slower RAM. So in terms of tps etc. your best options are   \n1- run on gpu,  \n2- Upgrade your ddr4 ram to 3600+ mhz. (afaik i5-8500 only have two slots, therefore u cannot add more ram)  \nfor 6 gb model (with model size, kv cache etc. included)   \n\\- on first scenario u probbly x10 to x20 tps increase **(this is purely based on the GPU's bandwight)**  \n\\- you probbly get (lets say if you upgraded to 4000 mhz) like 1.5x of your current tps.",
      "parent_id": "t3_1lmt3kt",
      "author": "Mir4can",
      "score": 4,
      "created": "2025-06-28T18:54:55Z"
    },
    {
      "id": "n0aaa0t",
      "body": "That actually sounds like a pretty decent speed for your setup, I would be expecting less. And yes all cores you allocate will be used to 100% for the duration since practically any CPU will be compute bound by performance regardless.\n\nI think a sparse model like Qwen-30B-A3B could be the best option for your setup, it's 3B active so the speed should be similar, and the 30B total should just about fit into 24 GB, total performance of about a 10B dense.",
      "parent_id": "t3_1lmt3kt",
      "author": "MoffKalast",
      "score": 5,
      "created": "2025-06-28T18:54:53Z"
    },
    {
      "id": "n0ajcq1",
      "body": "are you sure you are in dual channel ? or do you have 3 8gb stick ? I am pretty sure you need 2 or 4 stick of the same size for dual channel. maybe getting anoter ram stick and messing with your bios a bit could help what qwant are you running ? q4 km imatrix one ? you should try q4\\_0 or even q8\\_0 since they are more cpu friendly. and try other model especialy if you are running them trough llama.cpp or ollama",
      "parent_id": "t3_1lmt3kt",
      "author": "Wild_Requirement8902",
      "score": 2,
      "created": "2025-06-28T19:44:01Z"
    },
    {
      "id": "n0asbid",
      "body": "You need to watch the film “The Heist” and then plan one on an electronics store for a gpu. ",
      "parent_id": "t3_1lmt3kt",
      "author": "No-Consequence-1779",
      "score": 2,
      "created": "2025-06-28T20:32:16Z"
    },
    {
      "id": "n0ab9ld",
      "body": "why not add 3060?",
      "parent_id": "t3_1lmt3kt",
      "author": "jacek2023",
      "score": 1,
      "created": "2025-06-28T19:00:05Z"
    },
    {
      "id": "n0b2nmv",
      "body": "On an i5-7500 (4 cores) and llama.cpp, Llama-3.2-3B-Instruct-Q8\\_0.gguf gives me 36.67 prompt tok/sec and 7.06 generation tok/sec, for a small request of 1163 prompt tokens and 887 generation tokens. Set --threads to the number of cores you have, not the number of \"threads\" your CPU thinks it has",
      "parent_id": "t3_1lmt3kt",
      "author": "ElectronSpiderwort",
      "score": 1,
      "created": "2025-06-28T21:29:00Z"
    },
    {
      "id": "n0b8hou",
      "body": "This is what Zen 3 + DDR4-3200 inference looks like:\nhttps://www.reddit.com/r/LocalLLaMA/comments/1d9m0z3/comment/l7fged8/\n\n\nIf I had to guess, Skylake + DDR4-2666 should be around 50-60% of this speed. So at the first glance, your results seem fair",
      "parent_id": "t3_1lmt3kt",
      "author": "Normal-Ad-7114",
      "score": 1,
      "created": "2025-06-28T22:02:26Z"
    }
  ],
  "raw_api_url": "/comments/1lmt3kt"
}