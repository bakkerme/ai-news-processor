<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-05-05T05:32:13+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>Run AI Agents with Near-Native Speed on macOS—Introducing C/ua. : LocalLLaMA</title><entry><author><name>/u/Impressive_Half_2819</name><uri>https://www.reddit.com/user/Impressive_Half_2819</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted to share an exciting open-source framework called C/ua, specifically optimized for Apple Silicon Macs. C/ua allows AI agents to seamlessly control entire operating systems running inside high-performance, lightweight virtual containers.&lt;/p&gt; &lt;p&gt;Key Highlights:&lt;/p&gt; &lt;p&gt;Performance: Achieves up to 97% of native CPU speed on Apple Silicon. Compatibility: Works smoothly with any AI language model. Open Source: Fully available on GitHub for customization and community contributions.&lt;/p&gt; &lt;p&gt;Whether you&amp;#39;re into automation, AI experimentation, or just curious about pushing your Mac&amp;#39;s capabilities, check it out here:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://github.com/trycua/cua&quot;&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your thoughts and see what innovative use cases the macOS community can come up with!&lt;/p&gt; &lt;p&gt;Happy hacking!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Impressive_Half_2819&quot;&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1kenw3u</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/" /><updated>2025-05-04T16:28:29+00:00</updated><published>2025-05-04T16:28:29+00:00</published><title>Run AI Agents with Near-Native Speed on macOS—Introducing C/ua.</title></entry><entry><author><name>/u/Aware-Presentation-9</name><uri>https://www.reddit.com/user/Aware-Presentation-9</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This might actually be the best thing I have seen docker used for. I will check it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mql1bc5</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/mql1bc5/"/><updated>2025-05-04T19:14:29+00:00</updated><title>/u/Aware-Presentation-9 on Run AI Agents with Near-Native Speed on macOS—Introducing C/ua.</title></entry><entry><author><name>/u/LocoMod</name><uri>https://www.reddit.com/user/LocoMod</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The ONLY thing that matters is if this project somehow figured out a way to do GPU passthrough inside a container in MacOS. If not, then that entire README is just embellished marketing making the project appear to have accomplished something novel. Deploying a container or VM in MacOS is trivial. There are performance differences between using software emulation and something like Apple&amp;#39;s Virtualization Framework. in regards to AI inference, there is no way to pass the GPU through into the VM or container. So unless something has changed recently, they are likely comparing CPU inference between software emulation or something with better performance like the Virtualization Framework. In other words, unless the sandboxed (container or VM) has direct access to GPU metal like the OS does, there is nothing &amp;quot;high performance&amp;quot; about this.&lt;/p&gt; &lt;p&gt;I would gladly stand corrected here as I have a high interest in MacOS sandboxing with full GPU perf.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqne5i2</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/mqne5i2/"/><updated>2025-05-05T03:23:46+00:00</updated><title>/u/LocoMod on Run AI Agents with Near-Native Speed on macOS—Introducing C/ua.</title></entry></feed>