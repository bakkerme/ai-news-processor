<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-06-28T00:31:13+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/?depth=1" type="text/html" /><subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle><title>Automated GPU kernel optimization for Qwen3 attention - 12.5% average speedup on Apple Silicon using evolutionary programming : LocalLLaMA</title><entry><author><name>/u/asankhs</name><uri>https://www.reddit.com/user/asankhs</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey &lt;a href=&quot;/r/LocalLlama&quot;&gt;r/LocalLlama&lt;/a&gt;! Wanted to share something interesting I&amp;#39;ve been working on that might be relevant for folks running models locally on Apple Silicon.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Used evolutionary programming to automatically optimize Metal GPU kernels for transformer attention. Specifically targeted Qwen3-0.6B&amp;#39;s grouped query attention (40:8 head ratio) running on Apple M-series GPUs through MLX.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tested across 20 different inference scenarios against MLX&amp;#39;s &lt;code&gt;scaled_dot_product_attention&lt;/code&gt; baseline:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Average decode speed improvement: +12.5%&lt;/strong&gt; (Ïƒ = 38.3%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Peak improvement: +106%&lt;/strong&gt; on repetitive pattern generation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best category: +24.8%&lt;/strong&gt; average on general tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory usage: -0.99%&lt;/strong&gt; (slight reduction)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The honest picture:&lt;/strong&gt; It&amp;#39;s workload dependent. Some scenarios saw big gains (+46.6% on dialogue, +73.9% on extreme-length generation), but others regressed (-16.5% on code generation). Success rate was 7/20 benchmarks with &amp;gt;25% improvements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The system automatically evolves the Metal kernel source code using LLMs while preserving the MLX integration. No human GPU programming expertise was provided - it discovered optimizations like:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Perfect SIMD vectorization&lt;/strong&gt;: Found that &lt;code&gt;vec&amp;lt;T, 8&amp;gt;&lt;/code&gt; operations match Apple Silicon&amp;#39;s capabilities for 128-dim attention heads&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Two-pass online softmax&lt;/strong&gt;: Fused softmax normalization with value accumulation, reducing memory bandwidth&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GQA-specific memory patterns&lt;/strong&gt;: Optimized for the 40:8 head structure with coalesced access patterns&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Why this might matter for local inference&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Shows automated optimization can compete with expert-engineered kernels&lt;/li&gt; &lt;li&gt;Demonstrates potential for hardware-specific optimizations without manual tuning&lt;/li&gt; &lt;li&gt;Could be applied to other transformer components or different model architectures&lt;/li&gt; &lt;li&gt;All open source - you can reproduce and extend this work&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Try it yourself&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The code and all benchmarks are available in the &lt;a href=&quot;https://github.com/codelion/openevolve&quot;&gt;OpenEvolve repo&lt;/a&gt;. The MLX kernel optimization example is at &lt;code&gt;examples/mlx_metal_kernel_opt/&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Requirements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apple Silicon Mac&lt;/li&gt; &lt;li&gt;MLX framework&lt;/li&gt; &lt;li&gt;Qwen3-0.6B model&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Currently specific to Apple Silicon and this exact model configuration&lt;/li&gt; &lt;li&gt;Performance improvements are highly workload-dependent&lt;/li&gt; &lt;li&gt;Takes ~25 evolutionary generations to converge (few hours on M3)&lt;/li&gt; &lt;li&gt;No guarantees it&amp;#39;ll work better for your specific use case&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical write-up&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Full details with code diffs and benchmark methodology: &lt;a href=&quot;https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery&quot;&gt;https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to hear thoughts from folks who&amp;#39;ve done MLX optimization work, or if anyone wants to try this on different models/configurations. The evolutionary approach seems promising but definitely has room for improvement.&lt;/p&gt; &lt;p&gt;Has anyone else experimented with automated kernel optimization for local inference?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/asankhs&quot;&gt; /u/asankhs &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1lm98z7</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/" /><updated>2025-06-28T00:10:14+00:00</updated><published>2025-06-28T00:10:14+00:00</published><title>Automated GPU kernel optimization for Qwen3 attention - 12.5% average speedup on Apple Silicon using evolutionary programming</title></entry><entry><author><name>/u/SomeOddCodeGuy</name><uri>https://www.reddit.com/user/SomeOddCodeGuy</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is fantastic. Even if some scenarios regress, having someone out there tinkering with possible ways to further speed up decoding gets me excited; I honestly thought we&amp;#39;d hit the limit of what kind of speed we&amp;#39;d see on the Mac side by way of prompt processing, so just knowing you&amp;#39;re out there doing this makes me really happy.&lt;/p&gt; &lt;p&gt;You mention specifically the requirements being the 0.6b; is that just to repeat your results and it could theoretically work on the larger models, or is it very specific to the 0.6b atm?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n05tauf</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/n05tauf/"/><updated>2025-06-28T00:21:25+00:00</updated><title>/u/SomeOddCodeGuy on Automated GPU kernel optimization for Qwen3 attention - 12.5% average speedup on Apple Silicon using evolutionary programming</title></entry></feed>