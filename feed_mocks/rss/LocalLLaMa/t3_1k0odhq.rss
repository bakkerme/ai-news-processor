<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-04-21T06:26:21+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say... : LocalLLaMA</title><entry><author><name>/u/Eisenstein</name><uri>https://www.reddit.com/user/Eisenstein</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/E0QrtLGdAenlhx0dgrRxQhYXEHxRQVilnk0OkkkKL-M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7a4d66aac5a95d9f7e7eefde94e0ec3332c0946&quot; alt=&quot;KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...&quot; title=&quot;KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Eisenstein&quot;&gt; /u/Eisenstein &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.imgur.com/py5Tvae.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1k0odhq</id><media:thumbnail url="https://external-preview.redd.it/E0QrtLGdAenlhx0dgrRxQhYXEHxRQVilnk0OkkkKL-M.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e7a4d66aac5a95d9f7e7eefde94e0ec3332c0946" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/" /><updated>2025-04-16T16:16:39+00:00</updated><published>2025-04-16T16:16:39+00:00</published><title>KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...</title></entry><entry><author><name>/u/uti24</name><uri>https://www.reddit.com/user/uti24</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have experimented with Gemma 3 27B vision locally (using same KoboldCpp) and I think it&amp;#39;s not very good:&lt;/p&gt; &lt;p&gt;It can say what is on the image (often), but it hallucinates detail.&lt;/p&gt; &lt;p&gt;It often says something different for the image, like it can not say difference between picture of centaur and horse, snake and lizard. It will tell details that is not on the picture if you ask about those details, like &amp;quot;what color of boots of the character on the picture&amp;quot; and it will tell you something, even if it can not see boots part.&lt;/p&gt; &lt;p&gt;Well, to understand one probably should try themselves.&lt;/p&gt; &lt;p&gt;Even in your case, it selects not the best image and then just hallucinated why it is best representing of what you have asked about.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnfrwot</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/mnfrwot/"/><updated>2025-04-16T16:58:26+00:00</updated><title>/u/uti24 on KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...</title></entry><entry><author><name>/u/You_Wen_AzzHu</name><uri>https://www.reddit.com/user/You_Wen_AzzHu</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;My OCR practice shows 12b is better than 27b. Now sure why this is.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mngwn2b</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/mngwn2b/"/><updated>2025-04-16T20:18:58+00:00</updated><title>/u/You_Wen_AzzHu on KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...</title></entry><entry><author><name>/u/tengo_harambe</name><uri>https://www.reddit.com/user/tengo_harambe</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Try Qwen2.5-VL. It is compatible with koboldcpp now. It&amp;#39;s very impressive, also has the best OCR benchmarks for local models. 32B and 72B are ChatGPT 4o level.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnjcjzf</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/mnjcjzf/"/><updated>2025-04-17T05:16:32+00:00</updated><title>/u/tengo_harambe on KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...</title></entry><entry><author><name>/u/-Ellary-</name><uri>https://www.reddit.com/user/-Ellary-</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;From my experience Gemma 3 is smart but hallucinate quite a lot. About 2x more than Gemma 2.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mng3a4g</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/mng3a4g/"/><updated>2025-04-16T17:52:00+00:00</updated><title>/u/-Ellary- on KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...</title></entry><entry><author><name>/u/durden111111</name><uri>https://www.reddit.com/user/durden111111</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;how do you use multimodal in koboldcpp? Is a single 3090 enough? From what Ive read it seems it needs to load a second really large vision model along side gemma 27b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnhofq4</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/mnhofq4/"/><updated>2025-04-16T22:46:41+00:00</updated><title>/u/durden111111 on KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...</title></entry><entry><author><name>/u/Chance_Value_Not</name><uri>https://www.reddit.com/user/Chance_Value_Not</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’ve found koboldcpp (or rather the webui) to downscale the images waaay to much to be any good at image recognition (especially if you try ocr) Compare this with the cli tool from llama.cpp and you’ll get way better results there&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnk7lmy</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/mnk7lmy/"/><updated>2025-04-17T10:26:44+00:00</updated><title>/u/Chance_Value_Not on KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...</title></entry></feed>