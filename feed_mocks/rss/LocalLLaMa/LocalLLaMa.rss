<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <category term="LocalLLaMA" label="r/LocalLLaMA" />
    <updated>2025-05-05T05:31:41+00:00</updated>
    <icon>https://www.redditstatic.com/icon.png/</icon>
    <id>/r/localllama.rss</id>
    <link rel="self" href="https://www.reddit.com/r/localllama.rss" type="application/atom+xml" />
    <link rel="alternate" href="https://www.reddit.com/r/localllama" type="text/html" />
    <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
    <title>LocalLlama</title>
    <entry>
        <author>
            <name>/u/Recurrents</name>
            <uri>https://www.reddit.com/user/Recurrents</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/&quot;&gt;
            &lt;img
            src=&quot;https://external-preview.redd.it/Gj8FzKNPTvVSOxJwgeuufUJzmZ6BR-6YWri04zLtxfs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=99bf755df82e7e9bb2bc2cafc9271bdc27217ed4&quot;
            alt=&quot;What do I test out / run first?&quot; title=&quot;What do I test out / run
            first?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div
            class=&quot;md&quot;&gt;&lt;p&gt;Just got her in the mail. Haven&amp;#39;t had a chance
            to put her in yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by
            &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Recurrents&quot;&gt;
            /u/Recurrents &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/gallery/1kexdgy&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1kexdgy</id>
        <media:thumbnail
            url="https://external-preview.redd.it/Gj8FzKNPTvVSOxJwgeuufUJzmZ6BR-6YWri04zLtxfs.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=99bf755df82e7e9bb2bc2cafc9271bdc27217ed4" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/" />
        <updated>2025-05-04T23:21:02+00:00</updated>
        <published>2025-05-04T23:21:02+00:00</published>
        <title>What do I test out / run first?</title>
    </entry>
    <entry>
        <author>
            <name>/u/AaronFeng47</name>
            <uri>https://www.reddit.com/user/AaronFeng47</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/&quot;&gt;
            &lt;img
            src=&quot;https://external-preview.redd.it/jJ4wm0NIfgUy0MSOkw2YI6r-EjpVW_Y_SPR-xICfNk4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bf4c693cb7ebd3ae7a7b3eb2dc65cfbfc6e1d6d&quot;
            alt=&quot;Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison&quot;
            title=&quot;Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison&quot; /&gt;
            &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div
            class=&quot;md&quot;&gt;&lt;p&gt;Since IQ4_XS is my favorite quant for 32B models, I
            decided to run some benchmarks to compare IQ4_XS GGUFs from different sources.&lt;/p&gt;
            &lt;p&gt;&lt;strong&gt;MMLU-PRO 0.25 subset(3003 questions), 0 temp, No Think, IQ4_XS,
            Q8 KV Cache&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The entire benchmark took
            &lt;strong&gt;&lt;em&gt;11 hours, 37 minutes, and 30
            seconds.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a
            href=&quot;https://preview.redd.it/9ptc0cl2svye1.png?width=2475&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06a3b551fba60a33877f8e67af9932e381a15cc6&quot;&gt;https://preview.redd.it/9ptc0cl2svye1.png?width=2475&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06a3b551fba60a33877f8e67af9932e381a15cc6&lt;/a&gt;&lt;/p&gt;
            &lt;p&gt;The difference is apparently minimum, so just keep using whatever iq4 quant you
            already downloaded. &lt;/p&gt; &lt;p&gt;&lt;em&gt;The official MMLU-PRO leaderboard is
            listing the score of Qwen3 base model instead of instruct, that&amp;#39;s why these iq4
            quants score higher than the one on MMLU-PRO leaderboard.&lt;/em&gt;&lt;/p&gt;
            &lt;p&gt;gguf source:&lt;/p&gt; &lt;p&gt;&lt;a
            href=&quot;https://huggingface.co/unsloth/Qwen3-32B-GGUF/blob/main/Qwen3-32B-IQ4_XS.gguf&quot;&gt;https://huggingface.co/unsloth/Qwen3-32B-GGUF/blob/main/Qwen3-32B-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt;
            &lt;p&gt;&lt;a
            href=&quot;https://huggingface.co/unsloth/Qwen3-32B-128K-GGUF/blob/main/Qwen3-32B-128K-IQ4_XS.gguf&quot;&gt;https://huggingface.co/unsloth/Qwen3-32B-128K-GGUF/blob/main/Qwen3-32B-128K-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt;
            &lt;p&gt;&lt;a
            href=&quot;https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF/blob/main/Qwen_Qwen3-32B-IQ4_XS.gguf&quot;&gt;https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF/blob/main/Qwen_Qwen3-32B-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt;
            &lt;p&gt;&lt;a
            href=&quot;https://huggingface.co/mradermacher/Qwen3-32B-i1-GGUF/blob/main/Qwen3-32B.i1-IQ4_XS.gguf&quot;&gt;https://huggingface.co/mradermacher/Qwen3-32B-i1-GGUF/blob/main/Qwen3-32B.i1-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt;
            &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/AaronFeng47&quot;&gt; /u/AaronFeng47 &lt;/a&gt;
            &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1kf1yg9</id>
        <media:thumbnail
            url="https://external-preview.redd.it/jJ4wm0NIfgUy0MSOkw2YI6r-EjpVW_Y_SPR-xICfNk4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8bf4c693cb7ebd3ae7a7b3eb2dc65cfbfc6e1d6d" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/" />
        <updated>2025-05-05T03:21:49+00:00</updated>
        <published>2025-05-05T03:21:49+00:00</published>
        <title>Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison</title>
    </entry>
    <entry>
        <author>
            <name>/u/eastwindtoday</name>
            <uri>https://www.reddit.com/user/eastwindtoday</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/&quot;&gt;
            &lt;img
            src=&quot;https://preview.redd.it/gefvhv84qsye1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=235b3e1de1b7df4bd1bc1f7519f84b5259303d05&quot;
            alt=&quot;Visa is looking for vibe coders - thoughts?&quot; title=&quot;Visa is looking
            for vibe coders - thoughts?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32;
            submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/eastwindtoday&quot;&gt; /u/eastwindtoday
            &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://i.redd.it/gefvhv84qsye1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1keolh9</id>
        <media:thumbnail
            url="https://preview.redd.it/gefvhv84qsye1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=235b3e1de1b7df4bd1bc1f7519f84b5259303d05" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/" />
        <updated>2025-05-04T16:58:36+00:00</updated>
        <published>2025-05-04T16:58:36+00:00</published>
        <title>Visa is looking for vibe coders - thoughts?</title>
    </entry>
    <entry>
        <author>
            <name>/u/Impressive_Half_2819</name>
            <uri>https://www.reddit.com/user/Impressive_Half_2819</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/&quot;&gt;
            &lt;img
            src=&quot;https://preview.redd.it/627wnr5emsye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b896f5165e878160c1e104137518ab1d80b3addc&quot;
            alt=&quot;UI-Tars-1.5 reasoning never fails to entertain me.&quot;
            title=&quot;UI-Tars-1.5 reasoning never fails to entertain me.&quot; /&gt; &lt;/a&gt;
            &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;7B
            parameter computer use agent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32;
            submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/Impressive_Half_2819&quot;&gt;
            /u/Impressive_Half_2819 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://i.redd.it/627wnr5emsye1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1keo3te</id>
        <media:thumbnail
            url="https://preview.redd.it/627wnr5emsye1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=b896f5165e878160c1e104137518ab1d80b3addc" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/" />
        <updated>2025-05-04T16:37:31+00:00</updated>
        <published>2025-05-04T16:37:31+00:00</published>
        <title>UI-Tars-1.5 reasoning never fails to entertain me.</title>
    </entry>
    <entry>
        <author>
            <name>/u/panchovix</name>
            <uri>https://www.reddit.com/user/panchovix</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">
            &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there
            guys, hope is all going good.&lt;/p&gt; &lt;p&gt;I have been testing some bigger models
            on this setup and wanted to share some metrics if it helps someone!&lt;/p&gt;
            &lt;p&gt;Setup is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt;
            &lt;li&gt;192GB DDR5 6000Mhz at CL30 (overclocked and adjusted resistances to make it
            stable)&lt;/li&gt; &lt;li&gt;RTX 5090 MSI Vanguard LE SOC, flashed to Gigabyte Aorus
            Master VBIOS.&lt;/li&gt; &lt;li&gt;RTX 4090 ASUS TUF, flashed to Galax HoF
            VBIOS.&lt;/li&gt; &lt;li&gt;RTX 4090 Gigabyte Gaming OC, flashed to Galax HoF
            VBIOS.&lt;/li&gt; &lt;li&gt;RTX A6000 (Ampere)&lt;/li&gt; &lt;li&gt;AM5 MSI Carbon
            X670E&lt;/li&gt; &lt;li&gt;Running at X8 5.0 (5090) / X8 4.0 (4090) / X4 4.0 (4090) / X4
            4.0 (A6000), all from CPU lanes (using M2 to PCI-E adapters)&lt;/li&gt; &lt;li&gt;Fedora
            41-42 (believe me, I tried these on Windows and multiGPU is just borked
            there)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The models I have tested are:&lt;/p&gt;
            &lt;ul&gt; &lt;li&gt;DeepSeek V3 0324 at Q2_K_XL (233GB), from &lt;a
            href=&quot;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD&quot;&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;Qwen3 235B at Q3_K_XL, Q4_K_L, Q6_K from &lt;a
            href=&quot;https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF&quot;&gt;https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;Llama-3.1-Nemotron-Ultra-253B at Q3_K_XL from &lt;a
            href=&quot;https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF&quot;&gt;https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;c4ai-command-a-03-2025 111B at Q6_K_XL from &lt;a
            href=&quot;https://huggingface.co/bartowski/CohereForAI_c4ai-command-a-03-2025-GGUF&quot;&gt;https://huggingface.co/bartowski/CohereForAI_c4ai-command-a-03-2025-GGUF&lt;/a&gt;&lt;/li&gt;
            &lt;li&gt;Mistral-Large-Instruct-2411 123B at Q4_K_M from &lt;a
            href=&quot;https://huggingface.co/bartowski/Mistral-Large-Instruct-2411-GGUF&quot;&gt;https://huggingface.co/bartowski/Mistral-Large-Instruct-2411-GGUF&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt; &lt;p&gt;All on llamacpp, for offloading mostly on the case of bigger
            models. command a and Mistral Large run faster on EXL2.&lt;/p&gt; &lt;p&gt;I have also
            used llamacpp (&lt;a
            href=&quot;https://github.com/ggml-org/llama.cpp&quot;&gt;https://github.com/ggml-org/llama.cpp&lt;/a&gt;)
            and ikllamacpp (&lt;a
            href=&quot;https://github.com/ikawrakow/ik%5C_llama.cpp&quot;&gt;https://github.com/ikawrakow/ik\_llama.cpp&lt;/a&gt;),
            so I will note where I use which.&lt;/p&gt; &lt;p&gt;All of these models were loaded
            with 32K, without flash attention or cache quantization, except in the case of Nemotron,
            mostly to give some VRAM usages. FA when avaialble reduces VRAM usage with cache/buffer
            size heavily.&lt;/p&gt; &lt;p&gt;Also, when running -ot, I did use each layer instead of
            regex. This is because when using the regex I got issues with VRAM usage.&lt;/p&gt;
            &lt;p&gt;They were compiled from source with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;CC=gcc-14
            CXX=g++-14 CUDAHOSTCXX=g++-14 cmake -B build_linux \&lt;/code&gt;&lt;/p&gt;
            &lt;p&gt;&lt;code&gt;-DGGML_CUDA=ON \&lt;/code&gt;&lt;/p&gt;
            &lt;p&gt;&lt;code&gt;-DGGML_CUDA_FA_ALL_QUANTS=ON \&lt;/code&gt;&lt;/p&gt;
            &lt;p&gt;&lt;code&gt;-DGGML_BLAS=OFF \&lt;/code&gt;&lt;/p&gt;
            &lt;p&gt;&lt;code&gt;-DCMAKE_CUDA_ARCHITECTURES=&amp;quot;86;89;120&amp;quot;
            \&lt;/code&gt;&lt;/p&gt;
            &lt;p&gt;&lt;code&gt;-DCMAKE_CUDA_FLAGS=&amp;quot;-allow-unsupported-compiler
            -ccbin=g++-14&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;(Had to force CC and CXX 14, as
            CUDA doesn&amp;#39;t support GCC15 yet, which is what Fedora ships)&lt;/p&gt;
            &lt;h1&gt;DeepSeek V3 0324 (Q2_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;For this model, MLA
            was added recently, which let me to use more tensors on GPU.&lt;/p&gt; &lt;p&gt;Command
            to run it was&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m
            &amp;#39;/GGUFs/DeepSeek-V3-0324-UD-Q2_K_XL-merged.gguf&amp;#39; -c 32768 --no-mmap
            --no-warmup -ngl 999 -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; -ot
            &amp;quot;blk.(7|8|9|10).ffn.=CUDA1&amp;quot; -ot
            &amp;quot;blk.(11|12|13|14|15).ffn.=CUDA2&amp;quot; -ot
            &amp;quot;blk.(16|17|18|19|20|21|22|23|24|25).ffn.=CUDA3&amp;quot; -ot
            &amp;quot;ffn.*=CPU&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt;
            &lt;p&gt;&lt;code&gt;prompt eval time = 38919.92 ms / 1528 tokens ( 25.47 ms per token,
            39.26 tokens per second)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;eval time = 57175.47 ms /
            471 tokens ( 121.39 ms per token, 8.24 tokens per second)&lt;/code&gt;&lt;/p&gt;
            &lt;p&gt;This makes it pretty usable. The important part is setting the experts to be
            only on CPU, and active params + other experts on GPU. With MLA, it uses ~4GB for 32K
            and ~8GB for 64K. Without MLA, 16K uses 80GB of VRAM.&lt;/p&gt; &lt;h1&gt;Qwen3 235B
            (Q3_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;For this model and size, we&amp;#39;re able to
            load the model entirely on VRAM. Note: When using only GPU, on my case, llamacpp is
            faster than ik llamacpp.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt;
            &lt;p&gt;&lt;code&gt;./llama-server -m
            &amp;#39;/GGUFs/Qwen3-235B-A22B-128K-UD-Q3_K_XL-00001-of-00003.gguf&amp;#39; -c 32768
            --no-mmap --no-warmup -ngl 999 -ts 0.8,0.8,1.2,2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And
            speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 6532.37 ms / 3358 tokens (
            1.95 ms per token, 514.06 tokens per second)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;eval
            time = 53259.78 ms / 1359 tokens ( 39.19 ms per token, 25.52 tokens per
            second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Pretty good model but I would try to use at
            least Q4_K_S/M. Cache size at 32K is 6GB, and 12GB at 64K. This cache size is the same
            for all Qwen3 235B quants&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q4_K_XL, llamacpp)&lt;/h1&gt;
            &lt;p&gt;For this model, we&amp;#39;re using ~20GB of RAM and the rest on GPU.&lt;/p&gt;
            &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m
            &amp;#39;/GGUFs/Qwen3-235B-A22B-128K-UD-Q4_K_XL-00001-of-00003.gguf&amp;#39; -c 32768
            --no-mmap --no-warmup -ngl 999 -ot
            &amp;quot;blk\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|13)\.ffn.*=CUDA0&amp;quot; -ot
            &amp;quot;blk\.(14|15|16|17|18|19|20|21|22|23|24|25|26|27)\.ffn.*=CUDA1&amp;quot; -ot
            &amp;quot;blk\.(28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|)\.ffn.*=CUDA2&amp;quot;
            -ot
            &amp;quot;blk\.(47|48|49|50|51|52|53|54|55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73|74|75|76|77|78)\.ffn.*=CUDA3&amp;quot;
            -ot &amp;quot;ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds
            are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 17405.76 ms / 3358 tokens ( 5.18
            ms per token, 192.92 tokens per second)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;eval time =
            92420.55 ms / 1549 tokens ( 59.66 ms per token, 16.76 tokens per
            second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Model is pretty good at this point, and speeds
            are still acceptable. But on this case is where ik llamacpp shines.&lt;/p&gt;
            &lt;h1&gt;Qwen3 235B (Q4_K_XL, ik llamacpp)&lt;/h1&gt; &lt;p&gt;ik llamacpp with some
            extra parameters makes the models run faster when offloading. If you&amp;#39;re
            wondering why this isn&amp;#39;t the case or I didn&amp;#39;t post with DeepSeek V3
            0324, it is because quants of main llamacpp have MLA which are incompatible with MLA
            from ikllamacpp, which was implemented before via another method.&lt;/p&gt;
            &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m
            &amp;#39;/GGUFs/Qwen3-235B-A22B-128K-UD-Q4_K_XL-00001-of-00003.gguf&amp;#39; -c 32768
            --no-mmap --no-warmup -ngl 999 -ot
            &amp;quot;blk\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|13)\.ffn.*=CUDA0&amp;quot; -ot
            &amp;quot;blk\.(14|15|16|17|18|19|20|21|22|23|24|25|26|27)\.ffn.*=CUDA1&amp;quot; -ot
            &amp;quot;blk\.(28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|)\.ffn.*=CUDA2&amp;quot;
            -ot
            &amp;quot;blk\.(47|48|49|50|51|52|53|54|55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73|74|75|76|77|78)\.ffn.*=CUDA3&amp;quot;
            -ot &amp;quot;ffn.*=CPU&amp;quot; -fmoe -amb 1024 -rtr&lt;/code&gt;&lt;/p&gt;
            &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;INFO [ print_timings] prompt
            eval time = 15739.89 ms / 3358 tokens ( 4.69 ms per token, 213.34 tokens per second) |
            tid=&amp;quot;140438394236928&amp;quot; ti&lt;/code&gt;&lt;br/&gt;
            &lt;code&gt;mestamp=1746406901 id_slot=0 id_task=0 t_prompt_processing=15739.888
            n_prompt_tokens_processed=3358 t_token=4.687280524121501
            n_tokens_second=213.34332239212884&lt;/code&gt;&lt;br/&gt; &lt;code&gt;INFO [
            print_timings] generation eval time = 66275.69 ms / 1067 runs ( 62.11 ms per token,
            16.10 tokens per second) | tid=&amp;quot;140438394236928&amp;quot;
            ti&lt;/code&gt;&lt;br/&gt; &lt;code&gt;mestamp=1746406901 id_slot=0 id_task=0
            t_token_generation=66275.693 n_decoded=1067 t_token=62.11405154639175
            n_tokens_second=16.099416719791975&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So basically 10% more
            speed in PP and similar generation t/s.&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q6_K,
            llamacpp)&lt;/h1&gt; &lt;p&gt;This is the point where models are really close to Q8 and
            then to F16. This was more for test porpouses, but still is very usable.&lt;/p&gt;
            &lt;p&gt;This uses about 70GB RAM and rest on VRAM.&lt;/p&gt;
            &lt;p&gt;&lt;code&gt;Command to run was:&lt;/code&gt;&lt;br/&gt;
            &lt;code&gt;./llama-server -m
            &amp;#39;/models_llm/Qwen3-235B-A22B-128K-Q6_K-00001-of-00004.gguf&amp;#39; -c 32768
            --no-mmap --no-warmup -ngl 999 -ot
            &amp;quot;blk\.(0|1|2|3|4|5|6|7|8)\.ffn.*=CUDA0&amp;quot; -ot
            &amp;quot;blk\.(9|10|11|12|13|14|15|16|17)\.ffn.*=CUDA1&amp;quot; -ot
            &amp;quot;blk\.(18|19|20|21|22|23|24|25|26|27|28|29|30)\.ffn.*=CUDA2&amp;quot; -ot
            &amp;quot;blk\.(31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51|52)\.ffn.*=CUDA3&amp;quot;
            -ot &amp;quot;ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speed
            are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 57152.69 ms / 3877 tokens ( 14.74
            ms per token, 67.84 tokens per second) eval time = 38705.90 ms / 318 tokens ( 121.72 ms
            per token, 8.22 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q6_K, ik
            llamacpp)&lt;/h1&gt; &lt;p&gt;ik llamacpp makes a huge increase in PP
            performance.&lt;/p&gt; &lt;p&gt;Command to run was:&lt;/p&gt; &lt;p&gt;./llama-server -m
            &amp;#39;/models_llm/Qwen3-235B-A22B-128K-Q6_K-00001-of-00004.gguf&amp;#39; -c 32768
            --no-mmap --no-warmup -ngl 999 -ot
            &amp;quot;blk\.(0|1|2|3|4|5|6|7|8)\.ffn.*=CUDA0&amp;quot; -ot
            &amp;quot;blk\.(9|10|11|12|13|14|15|16|17)\.ffn.*=CUDA1&amp;quot; -ot
            &amp;quot;blk\.(18|19|20|21|22|23|24|25|26|27|28|29|30)\.ffn.*=CUDA2&amp;quot; -ot
            &amp;quot;blk\.(31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51|52)\.ffn.*=CUDA3&amp;quot;
            -ot &amp;quot;ffn.*=CPU&amp;quot; -fmoe -amb 512 -rtr&lt;/p&gt; &lt;p&gt;And speeds
            are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;INFO [ print_timings] prompt eval time = 36897.66 ms
            / 3877 tokens ( 9.52 ms per token, 105.07 tokens per second) |
            tid=&amp;quot;140095757803520&amp;quot; timestamp=1746307138 id_slot=0 id_task=0
            t_prompt_processing=36897.659 n_prompt_tokens_processed=3877 t_token=9.517064482847562
            n_tokens_second=105.07441678075024&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;INFO [
            print_timings] generation eval time = 143560.31 ms / 1197 runs ( 119.93 ms per token,
            8.34 tokens per second) | tid=&amp;quot;140095757803520&amp;quot; timestamp=1746307138
            id_slot=0 id_task=0 t_token_generation=143560.31 n_decoded=1197
            t_token=119.93342522974102 n_tokens_second=8.337959147622348&lt;/code&gt;&lt;/p&gt;
            &lt;p&gt;Basically 40-50% more PP performance and similar generation speed.&lt;/p&gt;
            &lt;h1&gt;Llama 3.1 Nemotron 253B (Q3_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;This model was
            PAINFUL to make it work fully on GPU, as layers are uneven. Some layers near the end are
            8B each.&lt;/p&gt; &lt;p&gt;This is also the only model I had to use CTK8/CTV4, else it
            doesn&amp;#39;t fit. &lt;/p&gt; &lt;p&gt;The commands to run it were:&lt;/p&gt;
            &lt;p&gt;&lt;code&gt;export CUDA_VISIBLE_DEVICES=0,1,3,2&lt;/code&gt;&lt;/p&gt;
            &lt;p&gt;&lt;code&gt;./llama-server -m
            /run/media/pancho/08329F4A329F3B9E/models_llm/Llama-3_1-Nemotron-Ultra-253B-v1-UD-Q3_K_XL-00001-of-00003.gguf
            -c 32768 -ngl 163 -ts 6.5,6,10,4 --no-warmup -fa -ctk q8_0 -ctv q4_0 -mg 2 --prio
            3&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I don&amp;#39;t have the specific speeds at the moment
            (as to run this model I have to close any application of my desktop), but they are, from
            a picture I got some days ago:&lt;/p&gt; &lt;p&gt;&lt;code&gt;PP: 130
            t/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Generation speed: 7.5
            t/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Cache size is 5GB for 32K and 10GB for
            64K.&lt;/p&gt; &lt;h1&gt;c4ai-command-a-03-2025 111B (Q6_K, llamacpp)&lt;/h1&gt;
            &lt;p&gt;I particullay have liked command a models, and I also feel this model is great.
            Ran on GPU only.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt;
            &lt;p&gt;&lt;code&gt;./llama-server -m
            &amp;#39;/GGUFs/CohereForAI_c4ai-command-a-03-2025-Q6_K-merged.gguf&amp;#39; -c 32768
            -ngl 99 -ts 10,11,17,20 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds
            are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 4101.94 ms / 3403 tokens ( 1.21
            ms per token, 829.61 tokens per second)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;eval time =
            46452.40 ms / 472 tokens ( 98.42 ms per token, 10.16 tokens per
            second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For reference: EXL2 with the same
            quant size gets ~12 t/s.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Cache size is 8GB for 32K and
            16GB for 64K.&lt;/p&gt; &lt;h1&gt;Mistral Large 2411 123B (Q4_K_M, llamacpp)&lt;/h1&gt;
            &lt;p&gt;Also have been a fan of Mistral Large models, as they work pretty
            good!&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt;
            &lt;p&gt;&lt;code&gt;./llama-server -m
            &amp;#39;/run/media/pancho/DE1652041651DDD9/HuggingFaceModelDownload&lt;/code&gt;&lt;br/&gt;
            &lt;code&gt;er/Storage/GGUFs/Mistral-Large-Instruct-2411-Q4_K_M-merged.gguf&amp;#39; -c
            32768 -ngl 99 -ts 7,7,10,5 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds
            are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 4427.90 ms / 3956 tokens ( 1.12
            ms per token, 893.43 tokens per second)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;eval time =
            30739.23 ms / 387 tokens ( 79.43 ms per token, 12.59 tokens per
            second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Cache size is quite big, 12GB for 32K and 24GB
            for 64K. In fact it is so big that if I want to load it on 3 GPUs (since size is 68GB) I
            need to use flash attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For reference: EXL2 with
            this same size gets 25 t/s with Tensor Parallel enabled. And 16-20 t/s on 6.5bpw EXL2
            (EXL2 lets you to use TP with uneven VRAM)&lt;/strong&gt;&lt;/p&gt;
            &lt;p&gt;That&amp;#39;s all the tests I have been running lately! I have been testing
            for both coding (python, C, C++) and RP. Not sure if you guys are interested in which
            one I prefer for each task or rank them.&lt;/p&gt; &lt;p&gt;Any question is
            welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32;
            &lt;a href=&quot;https://www.reddit.com/user/panchovix&quot;&gt; /u/panchovix &lt;/a&gt;
            &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
        <id>t3_1kezq68</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/" />
        <updated>2025-05-05T01:19:58+00:00</updated>
        <published>2025-05-05T01:19:58+00:00</published>
        <title>Speed metrics running DeepSeekV3 0324/Qwen3 235B and other models, on 128GB VRAM
            (5090+4090x2+A6000) + 192GB RAM on Consumer motherboard/CPU (llamacpp/ikllamacpp)</title>
    </entry>
    <entry>
        <author>
            <name>/u/intofuture</name>
            <uri>https://www.reddit.com/user/intofuture</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/&quot;&gt;
            &lt;img
            src=&quot;https://external-preview.redd.it/JTz5zGA4pu0kVgU5PcVKCkISLorm6EJbAz9pvaqpZXQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b29e8ff81b9a05957e855ea181b9201334e650a3&quot;
            alt=&quot;Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices
            (iOS, Android, Mac, Windows)&quot; title=&quot;Qwen3 performance benchmarks (toks/s, RAM
            utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)&quot; /&gt; &lt;/a&gt;
            &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey
            LocalLlama!&lt;/p&gt; &lt;p&gt;We&amp;#39;ve started publishing open-source model
            performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS,
            Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+
            soon.&lt;/p&gt; &lt;p&gt;We’re doing this because perf metrics determine the viability
            of shipping models in apps to users (no end-user wants crashing/slow AI features that
            hog up their specific device).&lt;/p&gt; &lt;p&gt;Although benchmarks get posted in
            threads here and there, we feel like a more consolidated and standardized hub should
            probably exist.&lt;/p&gt; &lt;p&gt;We figured we&amp;#39;d kickstart this since we
            already maintain this benchmarking infra/tooling at RunLocal for our enterprise
            customers. Note: We’ve mostly focused on supporting model formats like Core ML, ONNX and
            TFLite to date, so a few things are still WIP for GGUF support. &lt;/p&gt;
            &lt;p&gt;Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill
            Tokens=512, Num Generation Tokens=128). &lt;a
            href=&quot;https://huggingface.co/collections/unsloth/qwen3-680edabfb790c8c34a242f95&quot;&gt;GGUFs
            are from Unsloth&lt;/a&gt; 🐐&lt;/p&gt; &lt;p&gt;&lt;a
            href=&quot;https://preview.redd.it/l59qu1gxysye1.png?width=961&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=381abad7b25e1d719265826441b51aa50177d143&quot;&gt;Qwen3
            GGUF benchmarks on laptops&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a
            href=&quot;https://preview.redd.it/z5qxhpc1zsye1.png?width=913&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c48aa6c5c753c7dc74c4397aac34f92383d17afe&quot;&gt;Qwen3
            GGUF benchmarks on phones&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can see more of the benchmark
            data for Qwen3 &lt;a
            href=&quot;https://edgemeter.runlocal.ai/public/pipelines/a240f768-2847-4e06-8df9-156ea3c2c321&quot;&gt;here&lt;/a&gt;.
            We realize there are so many variables (devices, backends, etc.) that interpreting the
            data is currently harder than it should be. We&amp;#39;ll work on that!&lt;/p&gt;
            &lt;p&gt;You can also see benchmarks for a few other models &lt;a
            href=&quot;https://edgemeter.runlocal.ai/public/pipelines&quot;&gt;here&lt;/a&gt;. If
            you want to see benchmarks for any others, feel free to request them and we’ll try to
            publish ASAP!&lt;/p&gt; &lt;p&gt;Lastly, you can run your own benchmarks on our devices
            for free (limited to some degree to avoid our devices melting!).&lt;/p&gt; &lt;p&gt;This
            free/public version is a bit of a frankenstein fork of our enterprise product, so any
            benchmarks you run would be private to your account. But if there&amp;#39;s interest, we
            can add a way for you to also publish them so that the public benchmarks aren’t
            bottlenecked by us. &lt;/p&gt; &lt;p&gt;It’s still very early days for us with this, so
            please let us know what would make it better/cooler for the community: &lt;a
            href=&quot;https://edgemeter.runlocal.ai/public/pipelines&quot;&gt;https://edgemeter.runlocal.ai/public/pipelines&lt;/a&gt;&lt;/p&gt;
            &lt;p&gt;To more on-device AI in production! 💪&lt;/p&gt; &lt;p&gt;&lt;a
            href=&quot;https://i.redd.it/aev5rgjazsye1.gif&quot;&gt;https://i.redd.it/aev5rgjazsye1.gif&lt;/a&gt;&lt;/p&gt;
            &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/intofuture&quot;&gt; /u/intofuture &lt;/a&gt;
            &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1kepuli</id>
        <media:thumbnail
            url="https://external-preview.redd.it/JTz5zGA4pu0kVgU5PcVKCkISLorm6EJbAz9pvaqpZXQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b29e8ff81b9a05957e855ea181b9201334e650a3" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/" />
        <updated>2025-05-04T17:51:10+00:00</updated>
        <published>2025-05-04T17:51:10+00:00</published>
        <title>Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS,
            Android, Mac, Windows)</title>
    </entry>
    <entry>
        <author>
            <name>/u/fakezeta</name>
            <uri>https://www.reddit.com/user/fakezeta</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">
            &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I came
            across this gist &lt;a
            href=&quot;https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4&quot;&gt;https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4&lt;/a&gt;
            that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M
            quantization.&lt;/p&gt; &lt;p&gt;I tried to replicate locally but could I was not able,
            model sometimes entered in a repetition loop even with dry sampling or came to wrong
            conclusion after generating lots of thinking tokens.&lt;/p&gt; &lt;p&gt;I was using
            Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested
            Bartowski Q5_K_S but it had no improvement. The model didn&amp;#39;t entered in any
            repetition loop but generated lots of thinking tokens without finding any
            solution.&lt;/p&gt; &lt;p&gt;Then I saw that sunpazed didn&amp;#39;t used KV
            quantization and tried the same: boom! First time right.&lt;/p&gt; &lt;p&gt;It worked
            with Q5_K_S and also with Q4_K_XL&lt;/p&gt; &lt;p&gt;For who wants more details I leave
            here a gist &lt;a
            href=&quot;https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef&quot;&gt;https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef&lt;/a&gt;&lt;/p&gt;
            &lt;p&gt;Do you have any report of performance degradation with long generations on
            Qwen3 30B A3B and KV quantization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32;
            submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fakezeta&quot;&gt;
            /u/fakezeta &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
        <id>t3_1kewkno</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/" />
        <updated>2025-05-04T22:43:11+00:00</updated>
        <published>2025-05-04T22:43:11+00:00</published>
        <title>Qwen 30B A3B performance degradation with KV quantization</title>
    </entry>
    <entry>
        <author>
            <name>/u/remyxai</name>
            <uri>https://www.reddit.com/user/remyxai</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keyy4k/well_thats_just_like_your_benchmark_man/&quot;&gt;
            &lt;img
            src=&quot;https://preview.redd.it/mdy01ntgwuye1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d618c2e8de6cb32e8937776f2dfc048d10d61fc&quot;
            alt=&quot;Well, that's just, like… your benchmark, man.&quot; title=&quot;Well, that's
            just, like… your benchmark, man.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!--
            SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Especially as teams put AI into
            production, we need to start treating evaluation like a first-class discipline:
            versioned, interpretable, reproducible, and aligned to outcomes and improved
            UX.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Without some kind of ExperimentOps, you’re one
            false positive away from months of shipping the wrong thing.&lt;/strong&gt;&lt;/p&gt;
            &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/remyxai&quot;&gt; /u/remyxai &lt;/a&gt;
            &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://i.redd.it/mdy01ntgwuye1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keyy4k/well_thats_just_like_your_benchmark_man/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1keyy4k</id>
        <media:thumbnail
            url="https://preview.redd.it/mdy01ntgwuye1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4d618c2e8de6cb32e8937776f2dfc048d10d61fc" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1keyy4k/well_thats_just_like_your_benchmark_man/" />
        <updated>2025-05-05T00:39:27+00:00</updated>
        <published>2025-05-05T00:39:27+00:00</published>
        <title>Well, that's just, like… your benchmark, man.</title>
    </entry>
    <entry>
        <author>
            <name>/u/Healthy-Nebula-3603</name>
            <uri>https://www.reddit.com/user/Healthy-Nebula-3603</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/&quot;&gt;
            &lt;img
            src=&quot;https://external-preview.redd.it/rvBe2sSMWUb2BiTsxft299oO0IRh9G0lMoWcfjP8v_w.png?width=140&amp;amp;height=70&amp;amp;crop=140:70,smart&amp;amp;auto=webp&amp;amp;s=725aced25c191e436bfd44f4619541673facf020&quot;
            alt=&quot;QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.&quot;
            title=&quot;QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.&quot;
            /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div
            class=&quot;md&quot;&gt;&lt;p&gt;All models are from Bartowski - q4km version&lt;/p&gt;
            &lt;p&gt;Test only HTML frontend.&lt;/p&gt; &lt;p&gt;My assessment lauout quality from 0
            to 10&lt;/p&gt; &lt;p&gt;Prompt&lt;/p&gt; &lt;p&gt;&amp;quot;Generate a beautiful
            website for Steve&amp;#39;s pc repair using a single html script.&amp;quot;&lt;/p&gt;
            &lt;p&gt;QwQ 32b - 3/10&lt;/p&gt; &lt;p&gt;- poor layout but ..works , very
            basic&lt;/p&gt; &lt;p&gt;- 250 line of code&lt;/p&gt; &lt;p&gt;&lt;a
            href=&quot;https://preview.redd.it/6rol9pc6hsye1.png?width=2461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b&quot;&gt;https://preview.redd.it/6rol9pc6hsye1.png?width=2461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b&lt;/a&gt;&lt;/p&gt;
            &lt;p&gt;Qwen 3 32b - 6/10&lt;/p&gt; &lt;p&gt;- much better looks but still not too
            complex layout&lt;/p&gt; &lt;p&gt;- 310 lines of the code&lt;/p&gt; &lt;p&gt;&lt;a
            href=&quot;https://preview.redd.it/z9qixbh8hsye1.png?width=2461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29e6bb4b272399ba8140785feb429a196ecc5173&quot;&gt;https://preview.redd.it/z9qixbh8hsye1.png?width=2461&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29e6bb4b272399ba8140785feb429a196ecc5173&lt;/a&gt;&lt;/p&gt;
            &lt;p&gt;GLM-4-32b 9/10&lt;/p&gt; &lt;p&gt;- looks insanely good , quality layout like
            sonnet 3.7 easily&lt;/p&gt; &lt;p&gt;- 1500+ code lines&lt;/p&gt; &lt;p&gt;&lt;a
            href=&quot;https://preview.redd.it/3zj2lr2ahsye1.png?width=2469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964&quot;&gt;https://preview.redd.it/3zj2lr2ahsye1.png?width=2469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964&lt;/a&gt;&lt;/p&gt;
            &lt;p&gt;GLM-4-32b is insanely good for html code frontend.&lt;/p&gt; &lt;p&gt;I say
            that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most.&lt;/p&gt;
            &lt;p&gt;&lt;strong&gt;Other coding language like python , c , c++ or any other quality
            of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on
            the seme level but for html and JavaScript ... is GREAT.&lt;/strong&gt;&lt;/p&gt;
            &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/Healthy-Nebula-3603&quot;&gt;
            /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1kenk4f</id>
        <media:thumbnail
            url="https://external-preview.redd.it/rvBe2sSMWUb2BiTsxft299oO0IRh9G0lMoWcfjP8v_w.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=725aced25c191e436bfd44f4619541673facf020" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/" />
        <updated>2025-05-04T16:14:12+00:00</updated>
        <published>2025-05-04T16:14:12+00:00</published>
        <title>QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.</title>
    </entry>
    <entry>
        <author>
            <name>/u/VoidAlchemy</name>
            <uri>https://www.reddit.com/user/VoidAlchemy</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/&quot;&gt;
            &lt;img
            src=&quot;https://a.thumbs.redditmedia.com/bM9LC8PSLdBmtmFQOjBwlZthNsiYL5J4IXaOzEPqwY4.jpg&quot;
            alt=&quot;LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!&quot;
            title=&quot;LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!&quot;
            /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div
            class=&quot;md&quot;&gt;&lt;p&gt;&lt;a
            href=&quot;https://preview.redd.it/3bwwfd4epsye1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adbb0bce2c13bc560499b0d3459329d16d0a3291&quot;&gt;You
            can&amp;#39;t go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B
            and 30B)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a
            href=&quot;https://preview.redd.it/m4x5z2sposye1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26b2ff50d960dd957e86feb04a8c21030ef0195c&quot;&gt;mainline
            llama.cpp just got a boost for fully offloaded Qwen3 MoE (single
            expert)&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;tl;dr;&lt;/h1&gt; &lt;p&gt;I highly recommend
            doing a &lt;code&gt;git pull&lt;/code&gt; and re-building your
            &lt;code&gt;ik_llama.cpp&lt;/code&gt; or &lt;code&gt;llama.cpp&lt;/code&gt; repo to take
            advantage of recent major performance improvements just released.&lt;/p&gt; &lt;p&gt;The
            friendly competition between these amazing projects is producing delicious fruit for the
            whole GGUF loving &lt;code&gt;r/LocalLLaMA&lt;/code&gt; community!&lt;/p&gt; &lt;p&gt;If
            you have enough VRAM to fully offload and already have an existing
            &amp;quot;normal&amp;quot; quant of Qwen3 MoE then you&amp;#39;ll get a little more
            speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take
            advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork!&lt;/p&gt;
            &lt;h1&gt;Details&lt;/h1&gt; &lt;p&gt;I spent yesterday compiling and running benhmarks
            on the newest versions of both &lt;a
            href=&quot;https://github.com/ikawrakow/ik_llama.cpp&quot;&gt;ik_llama.cpp&lt;/a&gt; and
            mainline &lt;a
            href=&quot;https://github.com/ggml-org/llama.cpp&quot;&gt;llama.cpp&lt;/a&gt;.&lt;/p&gt;
            &lt;p&gt;For those that don&amp;#39;t know, ikawrakow was an early contributor to
            mainline llama.cpp working on important features that have since trickled down into
            ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my
            understanding) the &lt;code&gt;ik_llama.cpp&lt;/code&gt; fork was built and has a number
            of interesting features including SotA &lt;code&gt;iqN_k&lt;/code&gt; quantizations that
            pack in a lot of quality for the size while retaining good speed performance. (These new
            quants are &lt;em&gt;not&lt;/em&gt; available in ollma, lmstudio, koboldcpp,
            etc.)&lt;/p&gt; &lt;p&gt;A few recent PRs made by ikawrakow to
            &lt;code&gt;ik_llama.cpp&lt;/code&gt; and by JohannesGaessler to mainline have
            &lt;em&gt;boosted performance across the board&lt;/em&gt; and especially on CUDA with
            Flash Attention implementations for Grouped Query Attention (GQA) models and also
            Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B
            releases!&lt;/p&gt; &lt;h1&gt;References&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a
            href=&quot;https://github.com/ikawrakow/ik_llama.cpp/pull/370&quot;&gt;ikawrakow/ik_llama.cpp/pull/370&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/VoidAlchemy&quot;&gt; /u/VoidAlchemy &lt;/a&gt;
            &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1keoint</id>
        <media:thumbnail
            url="https://a.thumbs.redditmedia.com/bM9LC8PSLdBmtmFQOjBwlZthNsiYL5J4IXaOzEPqwY4.jpg" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/" />
        <updated>2025-05-04T16:55:10+00:00</updated>
        <published>2025-05-04T16:55:10+00:00</published>
        <title>LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title>
    </entry>
    <entry>
        <author>
            <name>/u/thebadslime</name>
            <uri>https://www.reddit.com/user/thebadslime</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">
            &lt;!-- SC_OFF --&gt;&lt;div
            class=&quot;md&quot;&gt;&lt;p&gt;It&amp;#39;s useless and stupid, but also kinda fun.
            You create and add characters to a pretend phone, and then message them.&lt;/p&gt;
            &lt;p&gt;Does not work with &amp;quot;thinking&amp;quot; models as it isn&amp;#39;t set
            to parse out the thinking tags.&lt;/p&gt; &lt;p&gt;&lt;a
            href=&quot;https://github.com/openconstruct/llamaphone&quot;&gt;LLamaPhone&lt;/a&gt;&lt;/p&gt;
            &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/thebadslime&quot;&gt; /u/thebadslime &lt;/a&gt;
            &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1ken4uk/i_made_a_fake_phone_to_text_fake_people_with/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1ken4uk/i_made_a_fake_phone_to_text_fake_people_with/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
        <id>t3_1ken4uk</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1ken4uk/i_made_a_fake_phone_to_text_fake_people_with/" />
        <updated>2025-05-04T15:56:28+00:00</updated>
        <published>2025-05-04T15:56:28+00:00</published>
        <title>I made a fake phone to text fake people with llamacpp</title>
    </entry>
    <entry>
        <author>
            <name>/u/SpeedyBrowser45</name>
            <uri>https://www.reddit.com/user/SpeedyBrowser45</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">
            &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Jetbrains
            just released a coding model. has anyone tried it?&lt;/p&gt; &lt;p&gt;&lt;a
            href=&quot;https://huggingface.co/collections/JetBrains/mellum-68120b4ae1423c86a2da007a&quot;&gt;https://huggingface.co/collections/JetBrains/mellum-68120b4ae1423c86a2da007a&lt;/a&gt;&lt;/p&gt;
            &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/SpeedyBrowser45&quot;&gt; /u/SpeedyBrowser45
            &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kewr1q/jetbrains_coding_model/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kewr1q/jetbrains_coding_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
        <id>t3_1kewr1q</id>
        <link href="https://www.reddit.com/r/LocalLLaMA/comments/1kewr1q/jetbrains_coding_model/" />
        <updated>2025-05-04T22:51:43+00:00</updated>
        <published>2025-05-04T22:51:43+00:00</published>
        <title>Jetbrains Coding model</title>
    </entry>
    <entry>
        <author>
            <name>/u/Impressive_Half_2819</name>
            <uri>https://www.reddit.com/user/Impressive_Half_2819</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kf2ezy/computeruse_model_capabilities/&quot;&gt;
            &lt;img
            src=&quot;https://preview.redd.it/kuxk8trzxvye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=93a6922dbf1de371175b17445c82c9c4f62314bf&quot;
            alt=&quot;Computer-Use Model Capabilities&quot; title=&quot;Computer-Use Model
            Capabilities&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div
            class=&quot;md&quot;&gt;&lt;p&gt;&lt;a
            href=&quot;https://www.trycua.com/blog/build-your-own-operator-on-macos-2#computer-use-model-capabilities&quot;&gt;https://www.trycua.com/blog/build-your-own-operator-on-macos-2#computer-use-model-capabilities&lt;/a&gt;&lt;/p&gt;
            &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/Impressive_Half_2819&quot;&gt;
            /u/Impressive_Half_2819 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://i.redd.it/kuxk8trzxvye1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kf2ezy/computeruse_model_capabilities/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1kf2ezy</id>
        <media:thumbnail
            url="https://preview.redd.it/kuxk8trzxvye1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=93a6922dbf1de371175b17445c82c9c4f62314bf" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kf2ezy/computeruse_model_capabilities/" />
        <updated>2025-05-05T03:47:52+00:00</updated>
        <published>2025-05-05T03:47:52+00:00</published>
        <title>Computer-Use Model Capabilities</title>
    </entry>
    <entry>
        <author>
            <name>/u/Su1tz</name>
            <uri>https://www.reddit.com/user/Su1tz</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">
            &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;It is for
            data science, mostly excel data manipulation in python. &lt;/p&gt; &lt;/div&gt;&lt;!--
            SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/Su1tz&quot;&gt; /u/Su1tz &lt;/a&gt; &lt;br/&gt;
            &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
        <id>t3_1kemt2m</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/" />
        <updated>2025-05-04T15:42:29+00:00</updated>
        <published>2025-05-04T15:42:29+00:00</published>
        <title>Which coding model is best for 48GB VRAM</title>
    </entry>
    <entry>
        <author>
            <name>/u/Capable-Ad-7494</name>
            <uri>https://www.reddit.com/user/Capable-Ad-7494</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">
            &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’ve
            looked into UIGEN and while it does have a good look to some examples, and it seems
            worst than qwen 8b oddly enough?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32;
            submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/Capable-Ad-7494&quot;&gt; /u/Capable-Ad-7494
            &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kf0x6n/what_local_models_are_actually_good_at_generating/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kf0x6n/what_local_models_are_actually_good_at_generating/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
        <id>t3_1kf0x6n</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kf0x6n/what_local_models_are_actually_good_at_generating/" />
        <updated>2025-05-05T02:23:55+00:00</updated>
        <published>2025-05-05T02:23:55+00:00</published>
        <title>What local models are actually good at generating UI’s?</title>
    </entry>
    <entry>
        <author>
            <name>/u/MushroomGecko</name>
            <uri>https://www.reddit.com/user/MushroomGecko</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kebauw/apparently_shipping_ai_platforms_is_a_thing_now/&quot;&gt;
            &lt;img
            src=&quot;https://preview.redd.it/fjze9by1yoye1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e0eb0e11cb1eca743ebbe8cd75e69b96fffd960&quot;
            alt=&quot;Apparently shipping AI platforms is a thing now as per this post from the Qwen
            X account&quot; title=&quot;Apparently shipping AI platforms is a thing now as per this
            post from the Qwen X account&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32;
            submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/MushroomGecko&quot;&gt; /u/MushroomGecko
            &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://i.redd.it/fjze9by1yoye1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kebauw/apparently_shipping_ai_platforms_is_a_thing_now/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1kebauw</id>
        <media:thumbnail
            url="https://preview.redd.it/fjze9by1yoye1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7e0eb0e11cb1eca743ebbe8cd75e69b96fffd960" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kebauw/apparently_shipping_ai_platforms_is_a_thing_now/" />
        <updated>2025-05-04T04:16:37+00:00</updated>
        <published>2025-05-04T04:16:37+00:00</published>
        <title>Apparently shipping AI platforms is a thing now as per this post from the Qwen X
            account</title>
    </entry>
    <entry>
        <author>
            <name>/u/wunnsen</name>
            <uri>https://www.reddit.com/user/wunnsen</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">
            &lt;!-- SC_OFF --&gt;&lt;div
            class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m wondering if I can prompt Qwen 3 models to
            output shorter / longer / more concise think tags.&lt;br/&gt; Has anyone attempted this
            yet for Qwen or a similar model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32;
            submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/wunnsen&quot;&gt;
            /u/wunnsen &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keyvqs/is_it_possible_to_system_prompt_qwen_3_models_to/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keyvqs/is_it_possible_to_system_prompt_qwen_3_models_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
        <id>t3_1keyvqs</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1keyvqs/is_it_possible_to_system_prompt_qwen_3_models_to/" />
        <updated>2025-05-05T00:36:01+00:00</updated>
        <published>2025-05-05T00:36:01+00:00</published>
        <title>Is it possible to system prompt Qwen 3 models to have &quot;reasoning effort&quot;?</title>
    </entry>
    <entry>
        <author>
            <name>/u/ComplexIt</name>
            <uri>https://www.reddit.com/user/ComplexIt</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">
            &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey guys,
            we are trying to improve LDR. &lt;/p&gt; &lt;p&gt;What areas do need attention in your
            opinion? - What features do you need? - What types of research you need? - How to
            improve the UI?&lt;/p&gt; &lt;p&gt;Repo: &lt;a
            href=&quot;https://github.com/LearningCircuit/local-deep-research&quot;&gt;https://github.com/LearningCircuit/local-deep-research&lt;/a&gt;&lt;/p&gt;
            &lt;h3&gt;Quick install:&lt;/h3&gt; &lt;p&gt;```bash pip install local-deep-research
            python -m local_deep_research.web.app&lt;/p&gt; &lt;h1&gt;For SearXNG (highly
            recommended):&lt;/h1&gt; &lt;p&gt;docker pull searxng/searxng docker run -d -p 8080:8080
            --name searxng searxng/searxng&lt;/p&gt; &lt;h1&gt;Start SearXNG (Required after system
            restart)&lt;/h1&gt; &lt;p&gt;docker start searxng ```&lt;/p&gt; &lt;p&gt;(Use Direct
            SearXNG for maximum speed instead of &amp;quot;auto&amp;quot; - this bypasses the LLM
            calls needed for engine selection in auto mode)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON
            --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/ComplexIt&quot;&gt; /u/ComplexIt &lt;/a&gt;
            &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keh382/local_deep_research_v031_we_need_your_help_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keh382/local_deep_research_v031_we_need_your_help_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
        <id>t3_1keh382</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1keh382/local_deep_research_v031_we_need_your_help_for/" />
        <updated>2025-05-04T10:53:51+00:00</updated>
        <published>2025-05-04T10:53:51+00:00</published>
        <title>Local Deep Research v0.3.1: We need your help for improving the tool</title>
    </entry>
    <entry>
        <author>
            <name>/u/ab2377</name>
            <uri>https://www.reddit.com/user/ab2377</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/&quot;&gt;
            &lt;img
            src=&quot;https://external-preview.redd.it/yxzFCzIK4WeSo6gF_lu0-lKTBRUJ2trx5h9oRthAsG8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11a2564e23d1a12b0fe7b5c447c7819e46dd629d&quot;
            alt=&quot;IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite
            models&quot; title=&quot;IBM Granite 4.0 Tiny Preview: A sneak peek at the next
            generation of Granite models&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32;
            submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ab2377&quot;&gt;
            /u/ab2377 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1kedu0d</id>
        <media:thumbnail
            url="https://external-preview.redd.it/yxzFCzIK4WeSo6gF_lu0-lKTBRUJ2trx5h9oRthAsG8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=11a2564e23d1a12b0fe7b5c447c7819e46dd629d" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/" />
        <updated>2025-05-04T07:05:53+00:00</updated>
        <published>2025-05-04T07:05:53+00:00</published>
        <title>IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/Impressive_Half_2819</name>
            <uri>https://www.reddit.com/user/Impressive_Half_2819</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1ket5xm/cua_framework_introduces_agent_trajectory_replay/&quot;&gt;
            &lt;img
            src=&quot;https://external-preview.redd.it/N2ZyMXM2OGxvdHllMdT2xQPEPt3ADINOvjs7FnnvWrRt2DesW7_96NRGC_fD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2352e5b3e2311cc98c3eeb9fb3abfa2db19703c9&quot;
            alt=&quot;C/ua Framework Introduces Agent Trajectory Replay for macOS.&quot;
            title=&quot;C/ua Framework Introduces Agent Trajectory Replay for macOS.&quot; /&gt;
            &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div
            class=&quot;md&quot;&gt;&lt;p&gt;C/ua, the open-source framework for running
            computer-use AI agents optimized for Apple Silicon Macs, has introduced Agent Trajectory
            Replay. &lt;/p&gt; &lt;p&gt;You can now visually replay and analyze each action your AI
            agents perform.&lt;/p&gt; &lt;p&gt;Explore it on GitHub, and feel free to share your
            feedback or use cases.&lt;/p&gt; &lt;p&gt;GitHub : &lt;a
            href=&quot;https://github.com/trycua/cua&quot;&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt;
            &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/Impressive_Half_2819&quot;&gt;
            /u/Impressive_Half_2819 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://v.redd.it/p18ercilotye1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1ket5xm/cua_framework_introduces_agent_trajectory_replay/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1ket5xm</id>
        <media:thumbnail
            url="https://external-preview.redd.it/N2ZyMXM2OGxvdHllMdT2xQPEPt3ADINOvjs7FnnvWrRt2DesW7_96NRGC_fD.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2352e5b3e2311cc98c3eeb9fb3abfa2db19703c9" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1ket5xm/cua_framework_introduces_agent_trajectory_replay/" />
        <updated>2025-05-04T20:11:39+00:00</updated>
        <published>2025-05-04T20:11:39+00:00</published>
        <title>C/ua Framework Introduces Agent Trajectory Replay for macOS.</title>
    </entry>
    <entry>
        <author>
            <name>/u/No-Bicycle-132</name>
            <uri>https://www.reddit.com/user/No-Bicycle-132</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">
            &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;It seems
            evident that Qwen3 with reasoning beats Qwen2.5. But I wonder if the Qwen3 dense models
            with reasoning turned off also outperforms Qwen2.5. Essentially what I am wondering is
            if the improvements mostly come from the reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON
            --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/No-Bicycle-132&quot;&gt; /u/No-Bicycle-132
            &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
        <id>t3_1kegrce</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/" />
        <updated>2025-05-04T10:31:46+00:00</updated>
        <published>2025-05-04T10:31:46+00:00</published>
        <title>Qwen3 no reasoning vs Qwen2.5</title>
    </entry>
    <entry>
        <author>
            <name>/u/Impressive_Half_2819</name>
            <uri>https://www.reddit.com/user/Impressive_Half_2819</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">
            &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanted
            to share an exciting open-source framework called C/ua, specifically optimized for Apple
            Silicon Macs. C/ua allows AI agents to seamlessly control entire operating systems
            running inside high-performance, lightweight virtual containers.&lt;/p&gt; &lt;p&gt;Key
            Highlights:&lt;/p&gt; &lt;p&gt;Performance: Achieves up to 97% of native CPU speed on
            Apple Silicon. Compatibility: Works smoothly with any AI language model. Open Source:
            Fully available on GitHub for customization and community contributions.&lt;/p&gt;
            &lt;p&gt;Whether you&amp;#39;re into automation, AI experimentation, or just curious
            about pushing your Mac&amp;#39;s capabilities, check it out here:&lt;/p&gt;
            &lt;p&gt;&lt;a
            href=&quot;https://github.com/trycua/cua&quot;&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt;
            &lt;p&gt;Would love to hear your thoughts and see what innovative use cases the macOS
            community can come up with!&lt;/p&gt; &lt;p&gt;Happy hacking!&lt;/p&gt;
            &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/Impressive_Half_2819&quot;&gt;
            /u/Impressive_Half_2819 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
        <id>t3_1kenw3u</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kenw3u/run_ai_agents_with_nearnative_speed_on/" />
        <updated>2025-05-04T16:28:29+00:00</updated>
        <published>2025-05-04T16:28:29+00:00</published>
        <title>Run AI Agents with Near-Native Speed on macOS—Introducing C/ua.</title>
    </entry>
    <entry>
        <author>
            <name>/u/RoyalCities</name>
            <uri>https://www.reddit.com/user/RoyalCities</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">
            &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;First
            time running / attempting distributed training via Windows using WSL2 and I&amp;#39;m
            getting constant issues regarding NCCL.&lt;/p&gt; &lt;p&gt;Is Linux essentially the only
            game in town for training if you plan on training with multiple GPUs via NVLink (and the
            pipeline specifically uses NCCL)?&lt;/p&gt; &lt;p&gt;Jensen was out here hyping up WSL2
            in January like it was the best thing since sliced bread but I have hit a wall trying to
            get it to work.&lt;/p&gt; &lt;p&gt;&amp;quot;Windows WSL2...basically it&amp;#39;s two
            operating systems within one - it works perfectly...&amp;quot;&lt;br/&gt; &lt;a
            href=&quot;https://www.youtube.com/live/k82RwXqZHY8?si=xbF7ZLrkBDI6Irzr&amp;amp;t=2940&quot;&gt;https://www.youtube.com/live/k82RwXqZHY8?si=xbF7ZLrkBDI6Irzr&amp;amp;t=2940&lt;/a&gt;&lt;/p&gt;
            &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/RoyalCities&quot;&gt; /u/RoyalCities &lt;/a&gt;
            &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kf3od0/i_have_spent_7_hours_trying_to_get_wsl2_to_work/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kf3od0/i_have_spent_7_hours_trying_to_get_wsl2_to_work/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
        <id>t3_1kf3od0</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kf3od0/i_have_spent_7_hours_trying_to_get_wsl2_to_work/" />
        <updated>2025-05-05T05:05:09+00:00</updated>
        <published>2025-05-05T05:05:09+00:00</published>
        <title>I have spent 7+ hours trying to get WSL2 to work with Multi-GPU training - is it
            basically impossible on windows? lol</title>
    </entry>
    <entry>
        <author>
            <name>/u/9acca9</name>
            <uri>https://www.reddit.com/user/9acca9</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">
            &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I use
            LM-Studio, and I wanted to know if it&amp;#39;s useful to use an install-and-use RAG to
            ask questions about a set of books (text). Or is it the same as adding the book(s) to
            the LM-Studio chat (which, from what I noticed, also creates a RAG when you query (I saw
            it says something about &amp;quot;retrieval&amp;quot; and sending parts of the
            book)).&lt;/p&gt; &lt;p&gt;In that case, it might be useful. Which one do you recommend?
            (Or should I stick with what LM-Studio does?)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;
            &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/9acca9&quot;&gt; /u/9acca9 &lt;/a&gt; &lt;br/&gt;
            &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keqrzb/super_simple_rag/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keqrzb/super_simple_rag/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
        <id>t3_1keqrzb</id>
        <link href="https://www.reddit.com/r/LocalLLaMA/comments/1keqrzb/super_simple_rag/" />
        <updated>2025-05-04T18:29:47+00:00</updated>
        <published>2025-05-04T18:29:47+00:00</published>
        <title>Super simple RAG?</title>
    </entry>
    <entry>
        <author>
            <name>/u/AaronFeng47</name>
            <uri>https://www.reddit.com/user/AaronFeng47</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/&quot;&gt;
            &lt;img
            src=&quot;https://b.thumbs.redditmedia.com/SK05go83T5ZWwsJO6IMatx5Z4uUx4gveCuwSgrYHpAg.jpg&quot;
            alt=&quot;Qwen3 on Dubesor Benchmark&quot; title=&quot;Qwen3 on Dubesor Benchmark&quot;
            /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div
            class=&quot;md&quot;&gt;&lt;p&gt;&lt;a
            href=&quot;https://dubesor.de/benchtable.html&quot;&gt;https://dubesor.de/benchtable.html&lt;/a&gt;&lt;/p&gt;
            &lt;p&gt;One of the few benchmarks that tested both thinking on/off of qwen3 &lt;/p&gt;
            &lt;p&gt;&lt;a
            href=&quot;https://preview.redd.it/eim5m35nxqye1.png?width=1265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd814d571735444429331c73b4cd17a066497907&quot;&gt;https://preview.redd.it/eim5m35nxqye1.png?width=1265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd814d571735444429331c73b4cd17a066497907&lt;/a&gt;&lt;/p&gt;
            &lt;blockquote&gt; &lt;p&gt;Small-scale manual performance comparison benchmark I made
            for myself. This table showcases the results I recorded of various AI models across
            different personal tasks I encountered over time (currently 83). I use a
            &lt;strong&gt;weighted rating system&lt;/strong&gt; and calculate the difficulty for
            each tasks by incorporating the results of all models. This is particularly relevant in
            scoring when failing easy questions or passing hard ones.&lt;/p&gt;
            &lt;p&gt;&lt;strong&gt;NOTE, THAT THIS JUST ME SHARING THE RESULTS FROM MY OWN
            SMALL-SCALE PERSONAL TESTING. YMMV! OBVIOUSLY THE SCORES ARE JUST THAT AND MIGHT NOT
            REFLECT YOUR OWN PERSONAL EXPERIENCES OR OTHER WELL-KNOWN
            BENCHMARKS.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON
            --&gt; &amp;#32; submitted by &amp;#32; &lt;a
            href=&quot;https://www.reddit.com/user/AaronFeng47&quot;&gt; /u/AaronFeng47 &lt;/a&gt;
            &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1keh542</id>
        <media:thumbnail
            url="https://b.thumbs.redditmedia.com/SK05go83T5ZWwsJO6IMatx5Z4uUx4gveCuwSgrYHpAg.jpg" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/" />
        <updated>2025-05-04T10:57:18+00:00</updated>
        <published>2025-05-04T10:57:18+00:00</published>
        <title>Qwen3 on Dubesor Benchmark</title>
    </entry>
</feed>