<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-06-28T00:31:00+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/localllama.rss</id><link rel="self" href="https://www.reddit.com/r/localllama.rss" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/localllama" type="text/html" /><subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle><title>LocalLlama</title><entry><author><name>/u/HOLUPREDICTIONS</name><uri>https://www.reddit.com/user/HOLUPREDICTIONS</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm1v2c/open_source_model_that_does_photoshopgrade_edits/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/ypm4lnr4ni9f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f23d8c2da2fff2f8a6b194ee42f06b2d3e90dca&quot; alt=&quot;Open source model that does photoshop-grade edits without affecting the rest of the pic: OmniGen 2&quot; title=&quot;Open source model that does photoshop-grade edits without affecting the rest of the pic: OmniGen 2&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Code: &lt;a href=&quot;https://github.com/VectorSpaceLab/OmniGen2&quot;&gt;https://github.com/VectorSpaceLab/OmniGen2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href=&quot;https://vectorspacelab.github.io/OmniGen2/&quot;&gt;https://vectorspacelab.github.io/OmniGen2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/HOLUPREDICTIONS&quot;&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/ypm4lnr4ni9f1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm1v2c/open_source_model_that_does_photoshopgrade_edits/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1lm1v2c</id><media:thumbnail url="https://preview.redd.it/ypm4lnr4ni9f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3f23d8c2da2fff2f8a6b194ee42f06b2d3e90dca" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm1v2c/open_source_model_that_does_photoshopgrade_edits/" /><updated>2025-06-27T18:51:13+00:00</updated><published>2025-06-27T18:51:13+00:00</published><title>Open source model that does photoshop-grade edits without affecting the rest of the pic: OmniGen 2</title></entry><entry><author><name>/u/corysama</name><uri>https://www.reddit.com/user/corysama</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm0m6i/copilot_chat_for_vs_code_is_now_open_source/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/tyJeCqipzT78spT8qdYr9nFThGnon2rt0efU2xelzLQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c7ae49e1d763b069953250103aad9e1f240a4f3&quot; alt=&quot;Copilot Chat for VS Code is now Open Source&quot; title=&quot;Copilot Chat for VS Code is now Open Source&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/corysama&quot;&gt; /u/corysama &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://github.com/microsoft/vscode-copilot-chat&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm0m6i/copilot_chat_for_vs_code_is_now_open_source/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1lm0m6i</id><media:thumbnail url="https://external-preview.redd.it/tyJeCqipzT78spT8qdYr9nFThGnon2rt0efU2xelzLQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1c7ae49e1d763b069953250103aad9e1f240a4f3" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm0m6i/copilot_chat_for_vs_code_is_now_open_source/" /><updated>2025-06-27T18:00:56+00:00</updated><published>2025-06-27T18:00:56+00:00</published><title>Copilot Chat for VS Code is now Open Source</title></entry><entry><author><name>/u/Marha01</name><uri>https://www.reddit.com/user/Marha01</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/FouZOpBR8n9C_WGYTOTMN6i2egUkQFWjKrxslBsNmKU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebd1a569f5715c190c324ddbb7cf8ca8b9e4815d&quot; alt=&quot;Prime Intellect: We did it — SYNTHETIC‑2 is complete.&quot; title=&quot;Prime Intellect: We did it — SYNTHETIC‑2 is complete.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Marha01&quot;&gt; /u/Marha01 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://x.com/PrimeIntellect/status/1938490370054361422&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1llx4ky</id><media:thumbnail url="https://external-preview.redd.it/FouZOpBR8n9C_WGYTOTMN6i2egUkQFWjKrxslBsNmKU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ebd1a569f5715c190c324ddbb7cf8ca8b9e4815d" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/" /><updated>2025-06-27T15:42:21+00:00</updated><published>2025-06-27T15:42:21+00:00</published><title>Prime Intellect: We did it — SYNTHETIC‑2 is complete.</title></entry><entry><author><name>/u/kristaller486</name><uri>https://www.reddit.com/user/kristaller486</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/B1uwVS2BmhDOjFW0XJ6pW7-r7n5zECGun4YlOmky9YY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=975cbb18dc0dd9f2342d47d40a0f9fb8fe177327&quot; alt=&quot;Hunyuan-A13B released&quot; title=&quot;Hunyuan-A13B released&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;From HF repo:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Model Introduction&lt;/p&gt; &lt;p&gt;With the rapid advancement of artificial intelligence technology, large language models (LLMs) have achieved remarkable progress in natural language processing, computer vision, and scientific tasks. However, as model scales continue to expand, optimizing resource consumption while maintaining high performance has become a critical challenge. To address this, we have explored Mixture of Experts (MoE) architectures. The newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters. It not only delivers high-performance results but also achieves optimal resource efficiency, successfully balancing computational power and resource utilization.&lt;/p&gt; &lt;p&gt;Key Features and Advantages&lt;/p&gt; &lt;p&gt;Compact yet Powerful: With only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models.&lt;/p&gt; &lt;p&gt;Hybrid Inference Support: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.&lt;/p&gt; &lt;p&gt;Ultra-Long Context Understanding: Natively supports a 256K context window, maintaining stable performance on long-text tasks.&lt;/p&gt; &lt;p&gt;Enhanced Agent Capabilities: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3 and τ-Bench.&lt;/p&gt; &lt;p&gt;Efficient Inference: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/kristaller486&quot;&gt; /u/kristaller486 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://huggingface.co/tencent/Hunyuan-A13B-Instruct&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1llndut</id><media:thumbnail url="https://external-preview.redd.it/B1uwVS2BmhDOjFW0XJ6pW7-r7n5zECGun4YlOmky9YY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=975cbb18dc0dd9f2342d47d40a0f9fb8fe177327" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/" /><updated>2025-06-27T06:59:21+00:00</updated><published>2025-06-27T06:59:21+00:00</published><title>Hunyuan-A13B released</title></entry><entry><author><name>/u/Other_Housing8453</name><uri>https://www.reddit.com/user/Other_Housing8453</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm76gk/hugging_face_releases_a_50_page_report_on_how/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/ixin9dvyqj9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4185435146679d75323286fe47669bc3ecf82fc&quot; alt=&quot;Hugging Face releases a 50+ page report on how they built FineWeb2&quot; title=&quot;Hugging Face releases a 50+ page report on how they built FineWeb2&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Other_Housing8453&quot;&gt; /u/Other_Housing8453 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/ixin9dvyqj9f1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm76gk/hugging_face_releases_a_50_page_report_on_how/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1lm76gk</id><media:thumbnail url="https://preview.redd.it/ixin9dvyqj9f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4185435146679d75323286fe47669bc3ecf82fc" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm76gk/hugging_face_releases_a_50_page_report_on_how/" /><updated>2025-06-27T22:34:23+00:00</updated><published>2025-06-27T22:34:23+00:00</published><title>Hugging Face releases a 50+ page report on how they built FineWeb2</title></entry><entry><author><name>/u/Additional_Top1210</name><uri>https://www.reddit.com/user/Additional_Top1210</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llwfwv/qwen_vlo_from_understanding_the_world_to/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/p-RdsB-v9L-CFrA5EkxqdVn1O17bnDolUwqTorCzqTE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf118a7b3e066763df86407692a4a20da4c744d0&quot; alt=&quot;Qwen VLo: From &amp;quot;Understanding&amp;quot; the World to &amp;quot;Depicting&amp;quot; It&quot; title=&quot;Qwen VLo: From &amp;quot;Understanding&amp;quot; the World to &amp;quot;Depicting&amp;quot; It&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://qwenlm.github.io/blog/qwen-vlo/&quot;&gt;https://qwenlm.github.io/blog/qwen-vlo/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Additional_Top1210&quot;&gt; /u/Additional_Top1210 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1llwfwv&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llwfwv/qwen_vlo_from_understanding_the_world_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1llwfwv</id><media:thumbnail url="https://external-preview.redd.it/p-RdsB-v9L-CFrA5EkxqdVn1O17bnDolUwqTorCzqTE.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=cf118a7b3e066763df86407692a4a20da4c744d0" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llwfwv/qwen_vlo_from_understanding_the_world_to/" /><updated>2025-06-27T15:15:25+00:00</updated><published>2025-06-27T15:15:25+00:00</published><title>Qwen VLo: From &quot;Understanding&quot; the World to &quot;Depicting&quot; It</title></entry><entry><author><name>/u/ApprehensiveAd3629</name><uri>https://www.reddit.com/user/ApprehensiveAd3629</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/KDJV-rJVdBsUxEikcR6mcx63y02QfY38vq7JUDazoWM.jpg&quot; alt=&quot;Qwen3 Coder Soon?&quot; title=&quot;Qwen3 Coder Soon?&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/415iw73n6k9f1.png?width=1093&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4e66852a8d0b6a8981e1e0f23da6ddfd4d0744c&quot;&gt;https://x.com/huybery/status/1938655788849098805&lt;/a&gt;&lt;/p&gt; &lt;p&gt;source: &lt;a href=&quot;https://x.com/huybery/status/1938655788849098805&quot;&gt;https://x.com/huybery/status/1938655788849098805&lt;/a&gt;&lt;/p&gt; &lt;p&gt;i hope they release these models soon! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ApprehensiveAd3629&quot;&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1lm92se</id><media:thumbnail url="https://b.thumbs.redditmedia.com/KDJV-rJVdBsUxEikcR6mcx63y02QfY38vq7JUDazoWM.jpg" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/" /><updated>2025-06-28T00:01:43+00:00</updated><published>2025-06-28T00:01:43+00:00</published><title>Qwen3 Coder Soon?</title></entry><entry><author><name>/u/AdditionalWeb107</name><uri>https://www.reddit.com/user/AdditionalWeb107</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/6zqw0rkhzi9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b84cb94dea055e799bbb2285e64e2b597538da36&quot; alt=&quot;Arch-Router: The first (and fastest) LLM router that can align to your usage preferences.&quot; title=&quot;Arch-Router: The first (and fastest) LLM router that can align to your usage preferences.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Excited to share Arch-Router, our research and model for LLM routing. Routing to the right LLM is still an elusive problem, riddled with nuance and gotchas. For example:&lt;/p&gt; &lt;p&gt;“Embedding-based” (or simple intent-classifier) routers sound good on paper—label each prompt via embeddings as “support,” “SQL,” “math,” then hand it to the matching model—but real chats don’t stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that can’t keep up with multi-turn conversations or fast-moving product requirements.&lt;/p&gt; &lt;p&gt;&amp;quot;Performance-based&amp;quot; routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: “Will Legal accept this clause?” “Does our support tone still feel right?” Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arch-Router skips both pitfalls by routing on&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;preferences you write in plain language.&lt;/em&gt;&lt;/strong&gt; Drop rules like “contract clauses → GPT-4o” or “quick travel tips → Gemini-Flash,” and our 1.5B auto-regressive router model maps prompt along with the context to your routing policies—no retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tiny footprint&lt;/strong&gt; – 1.5 B params → runs on one modern GPU (or CPU while you play).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plug-n-play&lt;/strong&gt; – points at any mix of LLM endpoints; adding models needs &lt;em&gt;zero&lt;/em&gt; retraining.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SOTA query-to-policy matching&lt;/strong&gt; – beats bigger closed models on conversational datasets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost / latency smart&lt;/strong&gt; – push heavy stuff to premium models, everyday queries to the fast ones.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Exclusively available in Arch (the AI-native proxy for agents): &lt;a href=&quot;https://github.com/katanemo/archgw&quot;&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;br/&gt; 🔗 Model + code: &lt;a href=&quot;https://huggingface.co/katanemo/Arch-Router-1.5B&quot;&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;br/&gt; 📄 Paper / longer read: &lt;a href=&quot;https://arxiv.org/abs/2506.16655&quot;&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AdditionalWeb107&quot;&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/6zqw0rkhzi9f1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1lm3jvm</id><media:thumbnail url="https://preview.redd.it/6zqw0rkhzi9f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b84cb94dea055e799bbb2285e64e2b597538da36" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/" /><updated>2025-06-27T20:00:37+00:00</updated><published>2025-06-27T20:00:37+00:00</published><title>Arch-Router: The first (and fastest) LLM router that can align to your usage preferences.</title></entry><entry><author><name>/u/LandoRingel</name><uri>https://www.reddit.com/user/LandoRingel</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/c2JvZG9ndjVnZDlmMe7CY4SqtJeZEukasJn79Adjh2cJgmt44HDkzVTcUucN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24a31f419b54bcf613f907d27abae7c2526e8092&quot; alt=&quot;I'm using a local Llama model for my game's dialogue system!&quot; title=&quot;I'm using a local Llama model for my game's dialogue system!&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m blown away by how fast and intelligent Llama 3.2 is!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/LandoRingel&quot;&gt; /u/LandoRingel &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/cgoobkv5gd9f1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1llhdoq</id><media:thumbnail url="https://external-preview.redd.it/c2JvZG9ndjVnZDlmMe7CY4SqtJeZEukasJn79Adjh2cJgmt44HDkzVTcUucN.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=24a31f419b54bcf613f907d27abae7c2526e8092" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/" /><updated>2025-06-27T01:23:40+00:00</updated><published>2025-06-27T01:23:40+00:00</published><title>I'm using a local Llama model for my game's dialogue system!</title></entry><entry><author><name>/u/Nuenki</name><uri>https://www.reddit.com/user/Nuenki</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c116c7e6295d776b6382e425434256d0d8559943&quot; alt=&quot;The more LLMs think, the worse they translate&quot; title=&quot;The more LLMs think, the worse they translate&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Nuenki&quot;&gt; /u/Nuenki &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://nuenki.app/blog/the_more_llms_think_the_worse_they_translate&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1llqp0a</id><media:thumbnail url="https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c116c7e6295d776b6382e425434256d0d8559943" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/" /><updated>2025-06-27T10:41:40+00:00</updated><published>2025-06-27T10:41:40+00:00</published><title>The more LLMs think, the worse they translate</title></entry><entry><author><name>/u/Beneficial-Sir-6261</name><uri>https://www.reddit.com/user/Beneficial-Sir-6261</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;🏦 For the past 3 months, we&amp;#39;ve been developing AI agents together with banks, fintechs, and software companies. The most critical point I&amp;#39;ve observed during this process is: Agentic transformation will be a painful process, just like digital transformation. What I learned in the field:👇&lt;/p&gt; &lt;p&gt;1- Definitions related to artificial intelligence are not yet standardized. Even the definition of &amp;quot;AI agent&amp;quot; differs between parties in meetings.&lt;/p&gt; &lt;p&gt;2- Organizations typically develop simple agents. They are far from achieving real-world transformation. To transform a job that generates ROI, an average of 20 agents need to work together or separately.&lt;/p&gt; &lt;p&gt;3- Companies initially want to produce a basic working prototype. Everyone is ready to allocate resources after seeing real ROI. But there&amp;#39;s an important point. High performance is expected from small models running on a small amount of GPU, and the success of these models is naturally low. Therefore, they can&amp;#39;t get out of the test environment and the business turns into a chicken-and-egg problem.🐥&lt;/p&gt; &lt;p&gt;4- Another important point in agentic transformation is that significant changes need to be made in the use of existing tools according to the agent to be built. Actions such as UI changes in used applications and providing new APIs need to be taken. This brings many arrangements with it.🌪️&lt;/p&gt; &lt;p&gt;🤷‍♂️ An important problem we encounter with agents is the excitement about agents. This situation causes us to raise our expectations from agents. There are two critical points to pay attention to:&lt;/p&gt; &lt;p&gt;1- Avoid using agents unnecessarily. Don&amp;#39;t try to use agents for tasks that can be solved with software. Agents should be used as little as possible. Because software is deterministic - we can predict the next step with certainty. However, we cannot guarantee 100% output quality from agents. Therefore, we should use agents only at points where reasoning is needed.&lt;/p&gt; &lt;p&gt;2- Due to MCP and Agent excitement, we see technologies being used in the wrong places. There&amp;#39;s justified excitement about MCP in the sector. We brought MCP support to our framework in the first month it was released, and we even prepared a special page on our website explaining the importance of MCP when it wasn&amp;#39;t popular yet. MCP is a very important technology. However, this should not be forgotten: if you can solve a problem with classical software methods, you shouldn&amp;#39;t try to solve it using tool calls (MCP or agent) or LLM. It&amp;#39;s necessary to properly orchestrate the technologies and concepts emerging with agents.🎻&lt;/p&gt; &lt;p&gt;If you can properly orchestrate agents and choose the right agentic transformation points, productivity increases significantly with agents. At one of our clients, a job that took 1 hour was reduced to 5 minutes. The 5 minutes also require someone to perform checks related to the work done by the Agent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Beneficial-Sir-6261&quot;&gt; /u/Beneficial-Sir-6261 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llsztp/what_i_learned_building_agents_for_enterprises/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llsztp/what_i_learned_building_agents_for_enterprises/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1llsztp</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llsztp/what_i_learned_building_agents_for_enterprises/" /><updated>2025-06-27T12:46:41+00:00</updated><published>2025-06-27T12:46:41+00:00</published><title>What I Learned Building Agents for Enterprises</title></entry><entry><author><name>/u/asankhs</name><uri>https://www.reddit.com/user/asankhs</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey &lt;a href=&quot;/r/LocalLlama&quot;&gt;r/LocalLlama&lt;/a&gt;! Wanted to share something interesting I&amp;#39;ve been working on that might be relevant for folks running models locally on Apple Silicon.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Used evolutionary programming to automatically optimize Metal GPU kernels for transformer attention. Specifically targeted Qwen3-0.6B&amp;#39;s grouped query attention (40:8 head ratio) running on Apple M-series GPUs through MLX.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tested across 20 different inference scenarios against MLX&amp;#39;s &lt;code&gt;scaled_dot_product_attention&lt;/code&gt; baseline:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Average decode speed improvement: +12.5%&lt;/strong&gt; (σ = 38.3%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Peak improvement: +106%&lt;/strong&gt; on repetitive pattern generation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best category: +24.8%&lt;/strong&gt; average on general tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory usage: -0.99%&lt;/strong&gt; (slight reduction)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The honest picture:&lt;/strong&gt; It&amp;#39;s workload dependent. Some scenarios saw big gains (+46.6% on dialogue, +73.9% on extreme-length generation), but others regressed (-16.5% on code generation). Success rate was 7/20 benchmarks with &amp;gt;25% improvements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The system automatically evolves the Metal kernel source code using LLMs while preserving the MLX integration. No human GPU programming expertise was provided - it discovered optimizations like:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Perfect SIMD vectorization&lt;/strong&gt;: Found that &lt;code&gt;vec&amp;lt;T, 8&amp;gt;&lt;/code&gt; operations match Apple Silicon&amp;#39;s capabilities for 128-dim attention heads&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Two-pass online softmax&lt;/strong&gt;: Fused softmax normalization with value accumulation, reducing memory bandwidth&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GQA-specific memory patterns&lt;/strong&gt;: Optimized for the 40:8 head structure with coalesced access patterns&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Why this might matter for local inference&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Shows automated optimization can compete with expert-engineered kernels&lt;/li&gt; &lt;li&gt;Demonstrates potential for hardware-specific optimizations without manual tuning&lt;/li&gt; &lt;li&gt;Could be applied to other transformer components or different model architectures&lt;/li&gt; &lt;li&gt;All open source - you can reproduce and extend this work&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Try it yourself&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The code and all benchmarks are available in the &lt;a href=&quot;https://github.com/codelion/openevolve&quot;&gt;OpenEvolve repo&lt;/a&gt;. The MLX kernel optimization example is at &lt;code&gt;examples/mlx_metal_kernel_opt/&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Requirements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apple Silicon Mac&lt;/li&gt; &lt;li&gt;MLX framework&lt;/li&gt; &lt;li&gt;Qwen3-0.6B model&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Currently specific to Apple Silicon and this exact model configuration&lt;/li&gt; &lt;li&gt;Performance improvements are highly workload-dependent&lt;/li&gt; &lt;li&gt;Takes ~25 evolutionary generations to converge (few hours on M3)&lt;/li&gt; &lt;li&gt;No guarantees it&amp;#39;ll work better for your specific use case&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical write-up&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Full details with code diffs and benchmark methodology: &lt;a href=&quot;https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery&quot;&gt;https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to hear thoughts from folks who&amp;#39;ve done MLX optimization work, or if anyone wants to try this on different models/configurations. The evolutionary approach seems promising but definitely has room for improvement.&lt;/p&gt; &lt;p&gt;Has anyone else experimented with automated kernel optimization for local inference?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/asankhs&quot;&gt; /u/asankhs &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1lm98z7</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/" /><updated>2025-06-28T00:10:14+00:00</updated><published>2025-06-28T00:10:14+00:00</published><title>Automated GPU kernel optimization for Qwen3 attention - 12.5% average speedup on Apple Silicon using evolutionary programming</title></entry><entry><author><name>/u/1BlueSpork</name><uri>https://www.reddit.com/user/1BlueSpork</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Just curious, is it just me, or Gemma 3n really sucks in recognizing images?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/1BlueSpork&quot;&gt; /u/1BlueSpork &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm17p6/is_it_just_me_or_gemma_3n_really_sucks_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm17p6/is_it_just_me_or_gemma_3n_really_sucks_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1lm17p6</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm17p6/is_it_just_me_or_gemma_3n_really_sucks_in/" /><updated>2025-06-27T18:25:16+00:00</updated><published>2025-06-27T18:25:16+00:00</published><title>Is it just me, or Gemma 3n really sucks in recognizing images?</title></entry><entry><author><name>/u/DepthHour1669</name><uri>https://www.reddit.com/user/DepthHour1669</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;If you&amp;#39;ve been priced out by the spike to $1000+ recently for the past ~3 months, the prices finally dropped to baseline recently. &lt;/p&gt; &lt;p&gt;You can get a $650-750 Nvidia 3090 fairly easily now, instead of being nearly impossible. &lt;/p&gt; &lt;p&gt;Future pricing is unpredictable- if we follow expected deprecation trends, the 3090 should be around $550-600, but then again Trump&amp;#39;s tariff extensions expire in a few weeks and pricing is wild and likely to spike up. &lt;/p&gt; &lt;p&gt;If you&amp;#39;re interested in GPUs, &lt;strong&gt;now&lt;/strong&gt; is probably the best time to buy for 3090s/4090s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/DepthHour1669&quot;&gt; /u/DepthHour1669 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llms46/fyi_to_everyone_rtx_3090_prices_crashed_and_are/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llms46/fyi_to_everyone_rtx_3090_prices_crashed_and_are/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1llms46</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llms46/fyi_to_everyone_rtx_3090_prices_crashed_and_are/" /><updated>2025-06-27T06:20:23+00:00</updated><published>2025-06-27T06:20:23+00:00</published><title>FYI to everyone: RTX 3090 prices crashed and are back to baseline. You can finally get $600something 3090s again in the USA.</title></entry><entry><author><name>/u/Balance-</name><uri>https://www.reddit.com/user/Balance-</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/H_9g87w3EitABPy3ZAOo2ZH9LlcpQ5L4KMiJgV1zrjo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=baf972823fcd97a8af0b34ddd0ede97ce0d9de05&quot; alt=&quot;AI performance of smartphone SoCs&quot; title=&quot;AI performance of smartphone SoCs&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://ai-benchmark.com/ranking_processors.html&quot;&gt;https://ai-benchmark.com/ranking_processors.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few things notable to me: - The difference between tiers is &lt;em&gt;huge&lt;/em&gt;. A 2022 Snapdragon 8 Gen 2 beats the 8s Gen 4. There are huge gaps between the Dimensity 9000, 8000 and 7000 series. - You can better get a high-end SoC that’s a few years old than the latest mid-range one.&lt;/p&gt; &lt;h2&gt;- In this benchmark, it’s mainly a Qualcomm and Mediatek competition. It seems optimized software libraries are immensely important in using hardware effectively.&lt;/h2&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Balance-&quot;&gt; /u/Balance- &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/gallery/1llnwy5&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1llnwy5</id><media:thumbnail url="https://external-preview.redd.it/H_9g87w3EitABPy3ZAOo2ZH9LlcpQ5L4KMiJgV1zrjo.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=baf972823fcd97a8af0b34ddd0ede97ce0d9de05" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/" /><updated>2025-06-27T07:34:42+00:00</updated><published>2025-06-27T07:34:42+00:00</published><title>AI performance of smartphone SoCs</title></entry><entry><author><name>/u/ImprovementBusy5947</name><uri>https://www.reddit.com/user/ImprovementBusy5947</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Just testing on some old math problems, noticed that Magistral output looks a lot like deepseek chat, but pretty far from Qwen3. I’m guessing Magistral distilled from deepseek directly without acknowledging it?&lt;/p&gt; &lt;p&gt;Suppose that there exist nonzero complex numbers $a$ , $b$ , $c$ , and $d$ such that $k$ is a root of both the equations $ax^3+bx^2+cx+d=0$ and $bx^3+cx^2+dx+a=0$ . Find all possible values of $k$ (including complex values).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Deepseek chat:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Alright, I have this problem: **Problem Statement:** Suppose that there exist nonzero complex numbers a,b,c,, and d such that k is a root of both the equations ax3+bx2+cx+d=0 and bx3+cx2+dx+a=0. Find all possible values of k(including complex values). # Understanding the Problem First, I need to understand what&amp;#39;s being asked. We have two cubic equations: 1. ax3+bx2+cx+d=0 — let&amp;#39;s call this Equation (1). 2. bx3+cx2+dx+a=0 — Equation (2). A complex number k is a root of both equations. The coefficients a,b,c,d are non-zero complex numbers. I need to find all possible values of k that satisfy both equations. # Step 1: Substitute k into Both Equations ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Magistral Small:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Alright, I have this problem: **Problem Statement:** Find all possible values of ( k ) (including complex values) such that there exist nonzero complex numbers ( a, b, c, d ) for which ( k ) is a root of both the equations: 1. ( ax\^3 + bx\^2 + cx + d = 0 ) 2. ( bx\^3 + cx\^2 + dx + a = 0 ) # Understanding the Problem First, I need to understand what&amp;#39;s being asked. We have two cubic equations: 1. The first equation is ( ax\^3 + bx\^2 + cx + d = 0 ). 2. The second equation is ( bx\^3 + cx\^2 + dx + a = 0 ). Both equations have a common root ( k ). The coefficients ( a, b, c, d ) are all nonzero complex numbers. The goal is to find all possible values of ( k ) that satisfy this condition. # Step 1: Substitute the Common Root ( k ) into Both Equations ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Qwen3 32B:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Okay, so I need to find all possible values of k that are roots of both equations: ax³ + bx² + cx + d = 0 and bx³ + cx² + dx + a = 0. The coefficients a, b, c, d are nonzero complex numbers. Hmm, let me think about how to approach this. First, since k is a root of both equations, substituting k into both should satisfy them. Let me write that down: For the first equation: a*k³ + b*k² + c\*k + d = 0. (1) For the second equation: b*k³ + c*k² + d\*k + a = 0. (2) So, I have two equations here with the same root k. Maybe I can manipulate these equations to find a relationship between them? Let me try to express equation (1) and equation (2) in terms of each other. If I can find a way to relate them, perhaps by dividing or subtracting? ... &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ImprovementBusy5947&quot;&gt; /u/ImprovementBusy5947 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm93yi/magistral_small_similarity_to_deepseek_chat/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm93yi/magistral_small_similarity_to_deepseek_chat/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1lm93yi</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm93yi/magistral_small_similarity_to_deepseek_chat/" /><updated>2025-06-28T00:03:21+00:00</updated><published>2025-06-28T00:03:21+00:00</published><title>Magistral small similarity to Deepseek chat?</title></entry><entry><author><name>/u/entsnack</name><uri>https://www.reddit.com/user/entsnack</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on porting a GPT-4.1 project over to an open-source model to deal with a GDPR-compliant client. The task is basically fine-tuning the model to classify text in a western European language.&lt;/p&gt; &lt;p&gt;I tried Qwen3 (0.6B, 1.7B, 8B) without making much progress (the fine-tuned model is far behind GPT-4.1) and finally went back to Llama-3.1-8B, which was what worked for me over a year ago. This is super surprising to me, because Qwen3&amp;#39;s zero-shot performance in English is almost 2x that of Llama&amp;#39;s for similar model sizes.&lt;/p&gt; &lt;p&gt;Does anyone else run fine-tuning heavy workloads in European languages? What&amp;#39;s the best model for this workload that I can fine-tune on an H100 96GB (note: I don&amp;#39;t do PEFT)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/entsnack&quot;&gt; /u/entsnack &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1lm9012</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/" /><updated>2025-06-27T23:58:14+00:00</updated><published>2025-06-27T23:58:14+00:00</published><title>I keep returning to Llama-3.1-8B</title></entry><entry><author><name>/u/ParsaKhaz</name><uri>https://www.reddit.com/user/ParsaKhaz</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llzdi8/i_built_an_automated_ai_stylist_in_24_hours_open/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/aWJoanhkd2I1aTlmMeEsEqhEcpnAGeAOI3lYg_mXc9hWrD9oAMlWiqt_A_Sq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4aa3b18ac6df7c246ae5daf5fad4a83ff312eb26&quot; alt=&quot;I built an Automated AI Stylist in 24 hours (open source, local)&quot; title=&quot;I built an Automated AI Stylist in 24 hours (open source, local)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ParsaKhaz&quot;&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/2v76newb5i9f1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llzdi8/i_built_an_automated_ai_stylist_in_24_hours_open/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1llzdi8</id><media:thumbnail url="https://external-preview.redd.it/aWJoanhkd2I1aTlmMeEsEqhEcpnAGeAOI3lYg_mXc9hWrD9oAMlWiqt_A_Sq.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4aa3b18ac6df7c246ae5daf5fad4a83ff312eb26" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llzdi8/i_built_an_automated_ai_stylist_in_24_hours_open/" /><updated>2025-06-27T17:11:21+00:00</updated><published>2025-06-27T17:11:21+00:00</published><title>I built an Automated AI Stylist in 24 hours (open source, local)</title></entry><entry><author><name>/u/Worth_Contract7903</name><uri>https://www.reddit.com/user/Worth_Contract7903</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Current Situation: * TC: 110k * YoE: 2 years as a Software Engineer (career switcher, mid-30s). * Role: SWE building AI applications using RAG. I&amp;#39;ve developed a strong passion for building LLMs, not just using them. I do not have a PhD.&lt;/p&gt; &lt;p&gt;I&amp;#39;ve been offered a role at a national lab to do exactly that—build LLMs from scratch and publish research, which could be a stepping stone to a top-tier team.&lt;/p&gt; &lt;p&gt;The problem is the offer has major red flags. It’s a significant pay cut, and my contact there admits the rest of the team is unmotivated and out of touch. More critically, the project&amp;#39;s funding is only guaranteed until June of next year, and my contact, the only person I&amp;#39;d want to work with, will likely leave in two years. I&amp;#39;m worried about taking a huge risk that could blow up and leave me with nothing. My decision comes down to the future of AI roles. Is core LLM development a viable path without a PhD, or is the safer money in AI app development and fine-tuning? &lt;/p&gt; &lt;p&gt;Given the unstable funding and weak team, would you take this risky, low-paying job for a shot at a dream role, or is it a career-killing move?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Worth_Contract7903&quot;&gt; /u/Worth_Contract7903 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm0btg/mid30s_swe_take_huge_pay_cut_for_risky_llm/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm0btg/mid30s_swe_take_huge_pay_cut_for_risky_llm/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1lm0btg</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm0btg/mid30s_swe_take_huge_pay_cut_for_risky_llm/" /><updated>2025-06-27T17:49:28+00:00</updated><published>2025-06-27T17:49:28+00:00</published><title>Mid-30s SWE: Take Huge Pay Cut for Risky LLM Research Role?</title></entry><entry><author><name>/u/Frosty-Cap-4282</name><uri>https://www.reddit.com/user/Frosty-Cap-4282</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This was born out of a personal need — I journal daily , and I didn’t want to upload my thoughts to some cloud server and also wanted to use AI. So I built Vinaya to be:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Private&lt;/strong&gt;: Everything stays on your device. No servers, no cloud, no trackers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt;: Clean UI built with Electron + React. No bloat, just journaling.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Insightful&lt;/strong&gt;: Semantic search, mood tracking, and AI-assisted reflections (all offline).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Link to the app: &lt;a href=&quot;https://vinaya-journal.vercel.app/&quot;&gt;https://vinaya-journal.vercel.app/&lt;/a&gt;&lt;br/&gt; Github: &lt;a href=&quot;https://github.com/BarsatKhadka/Vinaya-Journal&quot;&gt;https://github.com/BarsatKhadka/Vinaya-Journal&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m not trying to build a SaaS or chase growth metrics. I just wanted something I could trust and use daily. If this resonates with anyone else, I’d love feedback or thoughts.&lt;/p&gt; &lt;p&gt;If you like the idea or find it useful and want to encourage me to consistently refine it but don’t know me personally and feel shy to say it — just drop a ⭐ on GitHub. That’ll mean a lot :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Frosty-Cap-4282&quot;&gt; /u/Frosty-Cap-4282 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm91sr/local_llama_journaling_app/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm91sr/local_llama_journaling_app/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1lm91sr</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm91sr/local_llama_journaling_app/" /><updated>2025-06-28T00:00:33+00:00</updated><published>2025-06-28T00:00:33+00:00</published><title>Local Llama Journaling app.</title></entry><entry><author><name>/u/----Val----</name><uri>https://www.reddit.com/user/----Val----</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llty3n/gemma_3n_on_chatterui/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/bDhmNGU5b2EyaDlmMaqG3pvP9RZCPXP8pBQTkpjntjyFw5myStLfVsGSm3Uj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=490d26a2034391a147c28b471411c08007a15090&quot; alt=&quot;Gemma 3N on ChatterUI&quot; title=&quot;Gemma 3N on ChatterUI&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/----Val----&quot;&gt; /u/----Val---- &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/qe2y2po62h9f1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llty3n/gemma_3n_on_chatterui/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1llty3n</id><media:thumbnail url="https://external-preview.redd.it/bDhmNGU5b2EyaDlmMaqG3pvP9RZCPXP8pBQTkpjntjyFw5myStLfVsGSm3Uj.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=490d26a2034391a147c28b471411c08007a15090" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llty3n/gemma_3n_on_chatterui/" /><updated>2025-06-27T13:30:45+00:00</updated><published>2025-06-27T13:30:45+00:00</published><title>Gemma 3N on ChatterUI</title></entry><entry><author><name>/u/futureygoodness</name><uri>https://www.reddit.com/user/futureygoodness</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm23z8/finetuning_apples_new_foundation_model/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/0roYtHcb4seFDjNH4QeAKWioknh4Zipx8FBcaIldLTA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=120bcaa987ed2ac8e1384f2a5ec56e0a93723fc2&quot; alt=&quot;Fine-Tuning Apple's New Foundation Model&quot; title=&quot;Fine-Tuning Apple's New Foundation Model&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/futureygoodness&quot;&gt; /u/futureygoodness &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://collisions.substack.com/p/fine-tuning-apples-new-foundation&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm23z8/finetuning_apples_new_foundation_model/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1lm23z8</id><media:thumbnail url="https://external-preview.redd.it/0roYtHcb4seFDjNH4QeAKWioknh4Zipx8FBcaIldLTA.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=120bcaa987ed2ac8e1384f2a5ec56e0a93723fc2" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm23z8/finetuning_apples_new_foundation_model/" /><updated>2025-06-27T19:01:13+00:00</updated><published>2025-06-27T19:01:13+00:00</published><title>Fine-Tuning Apple's New Foundation Model</title></entry><entry><author><name>/u/wwwillchen</name><uri>https://www.reddit.com/user/wwwillchen</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llnwna/dyad_v010_opensource_local_alternative_to/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/eGthenQ5ZHQ5ZjlmMQQDM_dLcTyHBC8BScL5E00e_jl5aRRWjMUA-Nu_qDSf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4758657d96c820f19f51a4aed82016d536d0826b&quot; alt=&quot;dyad v0.10 - open-source local alternative to lovable/v0/bolt.new with ollama/LM Studio support - now supports building mobile apps!&quot; title=&quot;dyad v0.10 - open-source local alternative to lovable/v0/bolt.new with ollama/LM Studio support - now supports building mobile apps!&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m excited to share an update to &lt;a href=&quot;http://dyad.sh/&quot;&gt;&lt;strong&gt;Dyad&lt;/strong&gt;&lt;/a&gt; which is a free, local, open-source AI app builder I&amp;#39;ve been working on for 3 months after leaving Google. It&amp;#39;s designed as an alternative to v0, Lovable, and Bolt, but it runs on your computer (it&amp;#39;s an Electron app)!&lt;/p&gt; &lt;p&gt;Here’s what makes Dyad different:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Run ANY model (including local LLMs!)&lt;/strong&gt; - Based on popular demand from &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1k76ztc/i_built_a_free_local_opensource_alternative_to/&quot;&gt;this sub-reddit&lt;/a&gt;, Dyad supports &lt;a href=&quot;https://www.dyad.sh/docs/guides/ai-models/local-models&quot;&gt;local models&lt;/a&gt; via LM Studio and ollama (I don&amp;#39;t play favorites!), and you can also connect it to any OpenAI API-compatible model!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runs locally&lt;/strong&gt; - Dyad runs entirely on your computer, making it fast and frictionless. Because your code lives locally, you can easily switch back and forth between Dyad and your IDE like Cursor, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Free&lt;/strong&gt; - Dyad is free and bring-your-own API key. This means you can use your free Gemini/OpenRouter API key and build apps in Dyad for free.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Download Dyad for free: &lt;a href=&quot;https://dyad.sh/&quot;&gt;https://dyad.sh/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dyad works on Mac &amp;amp; Windows and Linux (you can download Linux directly from &lt;a href=&quot;https://github.com/dyad-sh/dyad/releases&quot;&gt;GitHub&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Please share any feedback - would you be interested in MCP support?&lt;/p&gt; &lt;p&gt;P.S. I&amp;#39;m also launching on Product Hunt today and would appreciate any support 🙏 &lt;a href=&quot;https://www.producthunt.com/products/dyad-free-local-vibe-coding-tool&quot;&gt;https://www.producthunt.com/products/dyad-free-local-vibe-coding-tool&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/wwwillchen&quot;&gt; /u/wwwillchen &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://v.redd.it/t461p9dt9f9f1&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llnwna/dyad_v010_opensource_local_alternative_to/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1llnwna</id><media:thumbnail url="https://external-preview.redd.it/eGthenQ5ZHQ5ZjlmMQQDM_dLcTyHBC8BScL5E00e_jl5aRRWjMUA-Nu_qDSf.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4758657d96c820f19f51a4aed82016d536d0826b" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llnwna/dyad_v010_opensource_local_alternative_to/" /><updated>2025-06-27T07:34:10+00:00</updated><published>2025-06-27T07:34:10+00:00</published><title>dyad v0.10 - open-source local alternative to lovable/v0/bolt.new with ollama/LM Studio support - now supports building mobile apps!</title></entry><entry><author><name>/u/FeathersOfTheArrow</name><uri>https://www.reddit.com/user/FeathersOfTheArrow</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/718m48of6b9f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b5423692617bfdf316daec6232ca857bc69416c&quot; alt=&quot;DeepSeek R2 delayed&quot; title=&quot;DeepSeek R2 delayed&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;blockquote&gt; &lt;p&gt;Over the past several months, DeepSeek&amp;#39;s engineers have been working to refine R2 until Liang gives the green light for release, according to The Information. However, a fast adoption of R2 could be difficult due to a shortage of Nvidia server chips in China as a result of U.S. export regulations, the report said, citing employees of top Chinese cloud firms that offer DeepSeek&amp;#39;s models to enterprise customers.&lt;/p&gt; &lt;p&gt;A potential surge in demand for R2 would overwhelm Chinese cloud providers, who need advanced Nvidia chips to run AI models, the report said.&lt;/p&gt; &lt;p&gt;DeepSeek did not immediately respond to a Reuters request for comment.&lt;/p&gt; &lt;p&gt;DeepSeek has been in touch with some Chinese cloud companies, providing them with technical specifications to guide their plans for hosting and distributing the model from their servers, the report said.&lt;/p&gt; &lt;p&gt;Among its cloud customers currently using R1, the majority are running the model with Nvidia&amp;#39;s H20 chips, The Information said.&lt;/p&gt; &lt;p&gt;Fresh export curbs imposed by the Trump administration in April have prevented Nvidia from selling in the Chinese market its H20 chips - the only AI processors it could legally export to the country at the time.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Sources : &lt;a href=&quot;https://www.theinformation.com/articles/deepseeks-progress-stalled-u-s-export-controls&quot;&gt;[1]&lt;/a&gt; &lt;a href=&quot;https://x.com/kimmonismus/status/1938221881175183740&quot;&gt;[2]&lt;/a&gt; &lt;a href=&quot;https://www.reuters.com/world/china/deepseek-r2-launch-stalled-ceo-balks-progress-information-reports-2025-06-26/&quot;&gt;[3]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/FeathersOfTheArrow&quot;&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/718m48of6b9f1.jpeg&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1ll6jo5</id><media:thumbnail url="https://preview.redd.it/718m48of6b9f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b5423692617bfdf316daec6232ca857bc69416c" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/" /><updated>2025-06-26T17:43:13+00:00</updated><published>2025-06-26T17:43:13+00:00</published><title>DeepSeek R2 delayed</title></entry><entry><author><name>/u/quakquakquak</name><uri>https://www.reddit.com/user/quakquakquak</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m looking for one I could run locally that isn&amp;#39;t trained yet into doing questions &amp;amp; responses. Unfortunately a bunch of &amp;quot;base&amp;quot; models now are actually already trained to do that, so I had trouble finding a newer one. This is mostly for writing and seeing what sorts of things it comes up with 8)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/quakquakquak&quot;&gt; /u/quakquakquak &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llzuit/whats_a_good_completion_only_model_these_days/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llzuit/whats_a_good_completion_only_model_these_days/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1llzuit</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llzuit/whats_a_good_completion_only_model_these_days/" /><updated>2025-06-27T17:30:16+00:00</updated><published>2025-06-27T17:30:16+00:00</published><title>What's a good completion only model these days?</title></entry></feed>
