<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-06-28T00:31:02+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/?depth=1" type="text/html" /><subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle><title>Prime Intellect: We did it — SYNTHETIC‑2 is complete. : LocalLLaMA</title><entry><author><name>/u/Marha01</name><uri>https://www.reddit.com/user/Marha01</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/FouZOpBR8n9C_WGYTOTMN6i2egUkQFWjKrxslBsNmKU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebd1a569f5715c190c324ddbb7cf8ca8b9e4815d&quot; alt=&quot;Prime Intellect: We did it — SYNTHETIC‑2 is complete.&quot; title=&quot;Prime Intellect: We did it — SYNTHETIC‑2 is complete.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Marha01&quot;&gt; /u/Marha01 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://x.com/PrimeIntellect/status/1938490370054361422&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1llx4ky</id><media:thumbnail url="https://external-preview.redd.it/FouZOpBR8n9C_WGYTOTMN6i2egUkQFWjKrxslBsNmKU.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ebd1a569f5715c190c324ddbb7cf8ca8b9e4815d" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/" /><updated>2025-06-27T15:42:21+00:00</updated><published>2025-06-27T15:42:21+00:00</published><title>Prime Intellect: We did it — SYNTHETIC‑2 is complete.</title></entry><entry><author><name>/u/Chromix_</name><uri>https://www.reddit.com/user/Chromix_</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;50% of the collected reasoning samples are from Qwen3 4B (potentially even a quantized version of it). Shouldn&amp;#39;t synthetic datasets contain highest-quality data? I&amp;#39;ve read about automated verifications - so maybe the Qwen3 4B reasoning was good enough to solve a bunch of problems. Yet for training AI, maybe there are better, more suitable, straight to the point reasoning samples from larger models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n031x0p</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/n031x0p/"/><updated>2025-06-27T15:53:07+00:00</updated><title>/u/Chromix_ on Prime Intellect: We did it — SYNTHETIC‑2 is complete.</title></entry><entry><author><name>/u/Away_Expression_3713</name><uri>https://www.reddit.com/user/Away_Expression_3713</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;what does it do&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n0339kp</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/n0339kp/"/><updated>2025-06-27T15:59:22+00:00</updated><title>/u/Away_Expression_3713 on Prime Intellect: We did it — SYNTHETIC‑2 is complete.</title></entry><entry><author><name>/u/Crafty-Marsupial2156</name><uri>https://www.reddit.com/user/Crafty-Marsupial2156</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Godspeed!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n0310zz</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/n0310zz/"/><updated>2025-06-27T15:49:02+00:00</updated><title>/u/Crafty-Marsupial2156 on Prime Intellect: We did it — SYNTHETIC‑2 is complete.</title></entry><entry><author><name>/u/RickyRickC137</name><uri>https://www.reddit.com/user/RickyRickC137</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;One of the top chess engine (neural network) called Leela was once created by just a few passionate community members!&lt;/p&gt; &lt;p&gt;I truly believe project like this has the potential to do just the same!&lt;/p&gt; &lt;p&gt;Godspeed!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n042oka</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/n042oka/"/><updated>2025-06-27T18:45:50+00:00</updated><title>/u/RickyRickC137 on Prime Intellect: We did it — SYNTHETIC‑2 is complete.</title></entry><entry><author><name>/u/phovos</name><uri>https://www.reddit.com/user/phovos</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Perfect. There is a very fruitful union between inference and &amp;#39;mining&amp;#39; as it were, in the future, and as someone who was excited about bitcoin in its first week I&amp;#39;m finally excited about something related to money, finance, or society, again! It&amp;#39;s all been downhill since bitcoin turned into pedo money.&lt;/p&gt; &lt;p&gt;Think cognitive &amp;#39;folding at home&amp;#39;; putting a network of distributed general purpose asics to a measurable task, on a global scale.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n03d1fs</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/n03d1fs/"/><updated>2025-06-27T16:45:26+00:00</updated><title>/u/phovos on Prime Intellect: We did it — SYNTHETIC‑2 is complete.</title></entry><entry><author><name>/u/Unable_Journalist543</name><uri>https://www.reddit.com/user/Unable_Journalist543</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;A lot of what this company has done feels... pointless? Intellect 1 was the first distributed training from scratch, not a good one but it was one and thats a big deal. But intellect 2 is just a qwen finetune which are in very large supply, and synthetic 2 is 50% qwen 3 4b, why would the main used model be a tiny mobile model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n050wm7</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/n050wm7/"/><updated>2025-06-27T21:38:42+00:00</updated><title>/u/Unable_Journalist543 on Prime Intellect: We did it — SYNTHETIC‑2 is complete.</title></entry><entry><author><name>/u/nntb</name><uri>https://www.reddit.com/user/nntb</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So will this lead to a local llm model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n054swq</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/n054swq/"/><updated>2025-06-27T21:59:59+00:00</updated><title>/u/nntb on Prime Intellect: We did it — SYNTHETIC‑2 is complete.</title></entry></feed>
