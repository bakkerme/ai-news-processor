<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <category term="LocalLLaMA" label="r/LocalLLaMA" />
    <updated>2025-05-05T05:32:12+00:00</updated>
    <icon>https://www.redditstatic.com/icon.png/</icon>
    <id>/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/.rss?depth=1</id>
    <link rel="self"
        href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/.rss?depth=1"
        type="application/atom+xml" />
    <link rel="alternate"
        href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/?depth=1"
        type="text/html" />
    <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
    <title>IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models :
        LocalLLaMA</title>
    <entry>
        <author>
            <name>/u/ab2377</name>
            <uri>https://www.reddit.com/user/ab2377</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/&quot;&gt;
            &lt;img
            src=&quot;https://external-preview.redd.it/yxzFCzIK4WeSo6gF_lu0-lKTBRUJ2trx5h9oRthAsG8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11a2564e23d1a12b0fe7b5c447c7819e46dd629d&quot;
            alt=&quot;IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite
            models&quot; title=&quot;IBM Granite 4.0 Tiny Preview: A sneak peek at the next
            generation of Granite models&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32;
            submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/ab2377&quot;&gt;
            /u/ab2377 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a
            href=&quot;https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt;
            &amp;#32; &lt;span&gt;&lt;a
            href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;
            &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
        <id>t3_1kedu0d</id>
        <media:thumbnail
            url="https://external-preview.redd.it/yxzFCzIK4WeSo6gF_lu0-lKTBRUJ2trx5h9oRthAsG8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=11a2564e23d1a12b0fe7b5c447c7>819e46dd629d" />
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/" />
        <updated>2025-05-04T07:05:53+00:00</updated>
        <published>2025-05-04T07:05:53+00:00</published>
        <title>IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/AaronFeng47</name>
            <uri>https://www.reddit.com/user/AaronFeng47</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hope they
            can release a larger one like 30b-a3b &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqi26wz</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqi26wz/" />
        <updated>2025-05-04T07:27:30+00:00</updated>
        <title>/u/AaronFeng47 on IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation
            of Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/ab2377</name>
            <uri>https://www.reddit.com/user/ab2377</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;so a new
            architecture, more moe goodness &lt;/p&gt; &lt;p&gt;&amp;quot;Whereas prior generations
            of Granite LLMs utilized a conventional transformer architecture, all models in the
            Granite 4.0 family utilize a new hybrid Mamba-2/Transformer architecture, marrying the
            speed and efficiency of Mamba with the precision of transformer-based self-attention.
            Granite 4.0 Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts
            (MoE) model, with 7B total parameters and only 1B active parameters at inference
            time.&lt;/p&gt; &lt;p&gt;Many of the innovations informing the Granite 4 architecture
            arose from IBM Research’s collaboration with the original Mamba creators on Bamba, an
            experimental open source hybrid model whose successor (Bamba v2) was released earlier
            this week.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqi0ajf</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqi0ajf/" />
        <updated>2025-05-04T07:08:16+00:00</updated>
        <title>/u/ab2377 on IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of
            Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/jacek2023</name>
            <uri>https://www.reddit.com/user/jacek2023</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Please
            look here:&lt;/p&gt; &lt;p&gt;&lt;a
            href=&quot;https://huggingface.co/ibm-granite/granite-4.0-tiny-preview/discussions/2&quot;&gt;https://huggingface.co/ibm-granite/granite-4.0-tiny-preview/discussions/2&lt;/a&gt;&lt;/p&gt;
            &lt;p&gt;&lt;a
            href=&quot;https://huggingface.co/gabegoodhart&quot;&gt;gabegoodhart&lt;/a&gt; IBM
            Granite org &lt;a
            href=&quot;https://huggingface.co/ibm-granite/granite-4.0-tiny-preview/discussions/2#681501d00747cee6d56790e3&quot;&gt;1
            day ago&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Since this model is hot-off-the-press, we
            don&amp;#39;t have inference support in &lt;code&gt;llama.cpp&lt;/code&gt; yet.
            I&amp;#39;m actively working on it, but since this is one of the first major models
            using a hybrid-recurrent architecture, there are a number of in-flight architectural
            changes in the codebase that need to all meet up to get this supported. We&amp;#39;ll
            keep you posted!&lt;/p&gt; &lt;p&gt;&lt;a
            href=&quot;https://huggingface.co/gabegoodhart&quot;&gt;gabegoodhart&lt;/a&gt; IBM
            Granite org &lt;a
            href=&quot;https://huggingface.co/ibm-granite/granite-4.0-tiny-preview/discussions/2#681504468c1176e69b50f991&quot;&gt;1
            day ago&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We definitely expect the model quality to improve
            beyond this preview. So far, this preview checkpoint has been trained on ~2.5T tokens,
            but it will continue to train up to ~15T tokens before final release.&lt;/p&gt;
            &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqia5nf</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqia5nf/" />
        <updated>2025-05-04T08:49:44+00:00</updated>
        <title>/u/jacek2023 on IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of
            Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/LagOps91</name>
            <uri>https://www.reddit.com/user/LagOps91</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;i hope we
            can see some larger models too! I really want them to scale those more experimental
            architectures and see where it leads. I think there is huge potential in combining
            attention with hidden state models. attention to understand context, hidden state to
            think ahead, remember key information etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqi6i1e</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqi6i1e/" />
        <updated>2025-05-04T08:11:29+00:00</updated>
        <title>/u/LagOps91 on IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of
            Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/Balance-</name>
            <uri>https://www.reddit.com/user/Balance-</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Read the
            full thing. It’s worth it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqi6tq1</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqi6tq1/" />
        <updated>2025-05-04T08:14:50+00:00</updated>
        <title>/u/Balance- on IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of
            Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/lets_theorize</name>
            <uri>https://www.reddit.com/user/lets_theorize</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Holy,
            this actually looks really good. IBM might actually be able to catch up with Alibaba
            with this one.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqi0scx</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqi0scx/" />
        <updated>2025-05-04T07:13:20+00:00</updated>
        <title>/u/lets_theorize on IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation
            of Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/sammcj</name>
            <uri>https://www.reddit.com/user/sammcj</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Neat but
            unless folks really start working to help add support for mamba architectures to
            llama.cpp it&amp;#39;ll be dead on arrival.&lt;/p&gt; &lt;p&gt;It would be great to see
            the folks at &lt;a href=&quot;/u/IBM&quot;&gt;/u/IBM&lt;/a&gt; step up and help out
            llama.cpp to support things like this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqibp8e</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqibp8e/" />
        <updated>2025-05-04T09:05:53+00:00</updated>
        <title>/u/sammcj on IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of
            Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/cpldcpu</name>
            <uri>https://www.reddit.com/user/cpldcpu</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;blockquote&gt;
            &lt;p&gt;The Granite 4.0 architecture uses no positional encoding (NoPE). Our testing
            demonstrates convincingly that this has had no adverse effect on long-context
            performance. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is interesting. Are there any
            papers that explain why this still works?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqi7ybf</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqi7ybf/" />
        <updated>2025-05-04T08:26:38+00:00</updated>
        <title>/u/cpldcpu on IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of
            Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/Healthy-Nebula-3603</name>
            <uri>https://www.reddit.com/user/Healthy-Nebula-3603</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Looking
            very promising...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqiarbh</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqiarbh/" />
        <updated>2025-05-04T08:55:58+00:00</updated>
        <title>/u/Healthy-Nebula-3603 on IBM Granite 4.0 Tiny Preview: A sneak peek at the next
            generation of Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/silenceimpaired</name>
            <uri>https://www.reddit.com/user/silenceimpaired</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is IBM
            going to be the silent winner? It’s impressive that their tiny model is 8b MOE and
            likely to perform at the same level as their previous dense 8b: Granite 4.0
            Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts (MoE) model,
            with 7B total parameters and only 1B active parameters at inference time.&lt;/p&gt;
            &lt;p&gt;I hope their efforts attempt to improve in &lt;a
            href=&quot;https://fiction.live/stories/Fiction-liveBench-April-6-2025/oQdzQvKHw8JyXbN87&quot;&gt;https://fiction.live/stories/Fiction-liveBench-April-6-2025/oQdzQvKHw8JyXbN87&lt;/a&gt;
            and not just passkey testing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqiwu8r</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqiwu8r/" />
        <updated>2025-05-04T12:20:44+00:00</updated>
        <title>/u/silenceimpaired on IBM Granite 4.0 Tiny Preview: A sneak peek at the next
            generation of Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/EmberElement</name>
            <uri>https://www.reddit.com/user/EmberElement</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div
            class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m just a dreamer without much background in
            ML stuff. Can anyone with sense comment on how likely it is we&amp;#39;ll ever see
            something that might be so efficient it&amp;#39;ll run well on CPU? I mean, this model
            already sounds pretty exciting from an efficiency perspective. Wondering if
            we&amp;#39;ve exhausted architectural changes that would e.g. reduce memory bandwidth
            requirements&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqiwvnt</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqiwvnt/" />
        <updated>2025-05-04T12:21:01+00:00</updated>
        <title>/u/EmberElement on IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation
            of Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/silenceimpaired</name>
            <uri>https://www.reddit.com/user/silenceimpaired</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;“We’re
            excited to continue pre-training Granite 4.0 Tiny, given such promising results so early
            in the process. We’re also excited to apply our learnings from post-training Granite
            3.3, particularly with regard to reasoning capabilities and complex instruction
            following, to the new models. Like its predecessors in Granite 3.2 and Granite 3.3,
            Granite 4.0 Tiny Preview offers toggleablethinking on andthinking off functionality
            (though its reasoning-focused post-training is very much incomplete).”&lt;/p&gt;
            &lt;p&gt;I hope some of this involves interacting with fictional text in a creative
            fashion: scene summaries, character profiles, plot outlining, hypothetical change
            impacts - books are great datasets just like large code bases and just need a good set
            of training data — use Guttenberg public domain books that are modernized with AI and
            then create training around the above elements.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON
            --&gt;</content>
        <id>t1_mqiyo3f</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqiyo3f/" />
        <updated>2025-05-04T12:34:01+00:00</updated>
        <title>/u/silenceimpaired on IBM Granite 4.0 Tiny Preview: A sneak peek at the next
            generation of Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/Slasher1738</name>
            <uri>https://www.reddit.com/user/Slasher1738</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Now if
            only we could get IBM to sell a version of their AI card to the public&lt;/p&gt;
            &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqj365w</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqj365w/" />
        <updated>2025-05-04T13:04:22+00:00</updated>
        <title>/u/Slasher1738 on IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation
            of Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/pigeon57434</name>
            <uri>https://www.reddit.com/user/pigeon57434</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;ibm doing
            better work than meta theyre surprisingly becoming a big player in open source (for
            small models)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqndkl8</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqndkl8/" />
        <updated>2025-05-05T03:19:50+00:00</updated>
        <title>/u/pigeon57434 on IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation
            of Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/AppearanceHeavy6724</name>
            <uri>https://www.reddit.com/user/AppearanceHeavy6724</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wonder
            what is prompt processing speed for semi-recurrent stuff compared to transformers.
            Transformers have fantastic prompt processing speed like 1000t/s easy even on crap like
            3060, but slow down during token generation as context grows. This seems to be the other
            way around, slow PP but fast TG.&lt;/p&gt; &lt;p&gt;I might be completely
            wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqj4tcf</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqj4tcf/" />
        <updated>2025-05-04T13:14:54+00:00</updated>
        <title>/u/AppearanceHeavy6724 on IBM Granite 4.0 Tiny Preview: A sneak peek at the next
            generation of Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/silenceimpaired</name>
            <uri>https://www.reddit.com/user/silenceimpaired</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Large
            datasets: all of Harry Potter series asking questions like, what would have to change in
            the series for Harry to end up with Hermione or for Voldemort to win. It’s a series
            everyone knows fairly well and requires details in the story and the story
            whole.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqiy1p4</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqiy1p4/" />
        <updated>2025-05-04T12:29:34+00:00</updated>
        <title>/u/silenceimpaired on IBM Granite 4.0 Tiny Preview: A sneak peek at the next
            generation of Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/wonderfulnonsense</name>
            <uri>https://www.reddit.com/user/wonderfulnonsense</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I
            remember seeing this model a few days ago. There&amp;#39;s no gguf so i cant try it out.
            I guess there&amp;#39;s not a lot of interest in this moe or it&amp;#39;s not currently
            possibly to make ggufs for it at the moment. &lt;/p&gt; &lt;p&gt;Webui stopped working
            for me last year after i updated it and I&amp;#39;ve never been able to get it working
            right since then, so been using lm studio appimages. That program runs everything good
            for me but only runs ggufs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content>
        <id>t1_mqjuhh0</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqjuhh0/" />
        <updated>2025-05-04T15:36:32+00:00</updated>
        <title>/u/wonderfulnonsense on IBM Granite 4.0 Tiny Preview: A sneak peek at the next
            generation of Granite models</title>
    </entry>
    <entry>
        <author>
            <name>/u/Echo9Zulu-</name>
            <uri>https://www.reddit.com/user/Echo9Zulu-</uri>
        </author>
        <category term="LocalLLaMA" label="r/LocalLLaMA" />
        <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Yall need
            to learn Transformers and stop hating on llama.cpp&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON
            --&gt;</content>
        <id>t1_mqjhh5d</id>
        <link
            href="https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/mqjhh5d/" />
        <updated>2025-05-04T14:28:12+00:00</updated>
        <title>/u/Echo9Zulu- on IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of
            Granite models</title>
    </entry>
</feed>