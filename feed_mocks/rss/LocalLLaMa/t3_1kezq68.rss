<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-05-05T05:31:51+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>Speed metrics running DeepSeekV3 0324/Qwen3 235B and other models, on 128GB VRAM (5090+4090x2+A6000) + 192GB RAM on Consumer motherboard/CPU (llamacpp/ikllamacpp) : LocalLLaMA</title><entry><author><name>/u/panchovix</name><uri>https://www.reddit.com/user/panchovix</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hi there guys, hope is all going good.&lt;/p&gt; &lt;p&gt;I have been testing some bigger models on this setup and wanted to share some metrics if it helps someone!&lt;/p&gt; &lt;p&gt;Setup is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen 7 7800X3D&lt;/li&gt; &lt;li&gt;192GB DDR5 6000Mhz at CL30 (overclocked and adjusted resistances to make it stable)&lt;/li&gt; &lt;li&gt;RTX 5090 MSI Vanguard LE SOC, flashed to Gigabyte Aorus Master VBIOS.&lt;/li&gt; &lt;li&gt;RTX 4090 ASUS TUF, flashed to Galax HoF VBIOS.&lt;/li&gt; &lt;li&gt;RTX 4090 Gigabyte Gaming OC, flashed to Galax HoF VBIOS.&lt;/li&gt; &lt;li&gt;RTX A6000 (Ampere)&lt;/li&gt; &lt;li&gt;AM5 MSI Carbon X670E&lt;/li&gt; &lt;li&gt;Running at X8 5.0 (5090) / X8 4.0 (4090) / X4 4.0 (4090) / X4 4.0 (A6000), all from CPU lanes (using M2 to PCI-E adapters)&lt;/li&gt; &lt;li&gt;Fedora 41-42 (believe me, I tried these on Windows and multiGPU is just borked there)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The models I have tested are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek V3 0324 at Q2_K_XL (233GB), from &lt;a href=&quot;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD&quot;&gt;https://huggingface.co/unsloth/DeepSeek-V3-0324-GGUF-UD&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qwen3 235B at Q3_K_XL, Q4_K_L, Q6_K from &lt;a href=&quot;https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF&quot;&gt;https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Llama-3.1-Nemotron-Ultra-253B at Q3_K_XL from &lt;a href=&quot;https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF&quot;&gt;https://huggingface.co/unsloth/Llama-3_1-Nemotron-Ultra-253B-v1-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;c4ai-command-a-03-2025 111B at Q6_K_XL from &lt;a href=&quot;https://huggingface.co/bartowski/CohereForAI_c4ai-command-a-03-2025-GGUF&quot;&gt;https://huggingface.co/bartowski/CohereForAI_c4ai-command-a-03-2025-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mistral-Large-Instruct-2411 123B at Q4_K_M from &lt;a href=&quot;https://huggingface.co/bartowski/Mistral-Large-Instruct-2411-GGUF&quot;&gt;https://huggingface.co/bartowski/Mistral-Large-Instruct-2411-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All on llamacpp, for offloading mostly on the case of bigger models. command a and Mistral Large run faster on EXL2.&lt;/p&gt; &lt;p&gt;I have also used llamacpp (&lt;a href=&quot;https://github.com/ggml-org/llama.cpp&quot;&gt;https://github.com/ggml-org/llama.cpp&lt;/a&gt;) and ikllamacpp (&lt;a href=&quot;https://github.com/ikawrakow/ik%5C_llama.cpp&quot;&gt;https://github.com/ikawrakow/ik\_llama.cpp&lt;/a&gt;), so I will note where I use which.&lt;/p&gt; &lt;p&gt;All of these models were loaded with 32K, without flash attention or cache quantization, except in the case of Nemotron, mostly to give some VRAM usages. FA when avaialble reduces VRAM usage with cache/buffer size heavily.&lt;/p&gt; &lt;p&gt;Also, when running -ot, I did use each layer instead of regex. This is because when using the regex I got issues with VRAM usage.&lt;/p&gt; &lt;p&gt;They were compiled from source with:&lt;/p&gt; &lt;p&gt;&lt;code&gt;CC=gcc-14 CXX=g++-14 CUDAHOSTCXX=g++-14 cmake -B build_linux \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DGGML_CUDA=ON \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DGGML_CUDA_FA_ALL_QUANTS=ON \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DGGML_BLAS=OFF \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DCMAKE_CUDA_ARCHITECTURES=&amp;quot;86;89;120&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-DCMAKE_CUDA_FLAGS=&amp;quot;-allow-unsupported-compiler -ccbin=g++-14&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;(Had to force CC and CXX 14, as CUDA doesn&amp;#39;t support GCC15 yet, which is what Fedora ships)&lt;/p&gt; &lt;h1&gt;DeepSeek V3 0324 (Q2_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;For this model, MLA was added recently, which let me to use more tensors on GPU.&lt;/p&gt; &lt;p&gt;Command to run it was&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m &amp;#39;/GGUFs/DeepSeek-V3-0324-UD-Q2_K_XL-merged.gguf&amp;#39; -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; -ot &amp;quot;blk.(7|8|9|10).ffn.=CUDA1&amp;quot; -ot &amp;quot;blk.(11|12|13|14|15).ffn.=CUDA2&amp;quot; -ot &amp;quot;blk.(16|17|18|19|20|21|22|23|24|25).ffn.=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 38919.92 ms / 1528 tokens ( 25.47 ms per token, 39.26 tokens per second)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;eval time = 57175.47 ms / 471 tokens ( 121.39 ms per token, 8.24 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This makes it pretty usable. The important part is setting the experts to be only on CPU, and active params + other experts on GPU. With MLA, it uses ~4GB for 32K and ~8GB for 64K. Without MLA, 16K uses 80GB of VRAM.&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q3_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;For this model and size, we&amp;#39;re able to load the model entirely on VRAM. Note: When using only GPU, on my case, llamacpp is faster than ik llamacpp.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m &amp;#39;/GGUFs/Qwen3-235B-A22B-128K-UD-Q3_K_XL-00001-of-00003.gguf&amp;#39; -c 32768 --no-mmap --no-warmup -ngl 999 -ts 0.8,0.8,1.2,2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 6532.37 ms / 3358 tokens ( 1.95 ms per token, 514.06 tokens per second)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;eval time = 53259.78 ms / 1359 tokens ( 39.19 ms per token, 25.52 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Pretty good model but I would try to use at least Q4_K_S/M. Cache size at 32K is 6GB, and 12GB at 64K. This cache size is the same for all Qwen3 235B quants&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q4_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;For this model, we&amp;#39;re using ~20GB of RAM and the rest on GPU.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m &amp;#39;/GGUFs/Qwen3-235B-A22B-128K-UD-Q4_K_XL-00001-of-00003.gguf&amp;#39; -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|13)\.ffn.*=CUDA0&amp;quot; -ot &amp;quot;blk\.(14|15|16|17|18|19|20|21|22|23|24|25|26|27)\.ffn.*=CUDA1&amp;quot; -ot &amp;quot;blk\.(28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|)\.ffn.*=CUDA2&amp;quot; -ot &amp;quot;blk\.(47|48|49|50|51|52|53|54|55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73|74|75|76|77|78)\.ffn.*=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 17405.76 ms / 3358 tokens ( 5.18 ms per token, 192.92 tokens per second)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;eval time = 92420.55 ms / 1549 tokens ( 59.66 ms per token, 16.76 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Model is pretty good at this point, and speeds are still acceptable. But on this case is where ik llamacpp shines.&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q4_K_XL, ik llamacpp)&lt;/h1&gt; &lt;p&gt;ik llamacpp with some extra parameters makes the models run faster when offloading. If you&amp;#39;re wondering why this isn&amp;#39;t the case or I didn&amp;#39;t post with DeepSeek V3 0324, it is because quants of main llamacpp have MLA which are incompatible with MLA from ikllamacpp, which was implemented before via another method.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m &amp;#39;/GGUFs/Qwen3-235B-A22B-128K-UD-Q4_K_XL-00001-of-00003.gguf&amp;#39; -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk\.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|13)\.ffn.*=CUDA0&amp;quot; -ot &amp;quot;blk\.(14|15|16|17|18|19|20|21|22|23|24|25|26|27)\.ffn.*=CUDA1&amp;quot; -ot &amp;quot;blk\.(28|29|30|31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|)\.ffn.*=CUDA2&amp;quot; -ot &amp;quot;blk\.(47|48|49|50|51|52|53|54|55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73|74|75|76|77|78)\.ffn.*=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&amp;quot; -fmoe -amb 1024 -rtr&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;INFO [ print_timings] prompt eval time = 15739.89 ms / 3358 tokens ( 4.69 ms per token, 213.34 tokens per second) | tid=&amp;quot;140438394236928&amp;quot; ti&lt;/code&gt;&lt;br/&gt; &lt;code&gt;mestamp=1746406901 id_slot=0 id_task=0 t_prompt_processing=15739.888 n_prompt_tokens_processed=3358 t_token=4.687280524121501 n_tokens_second=213.34332239212884&lt;/code&gt;&lt;br/&gt; &lt;code&gt;INFO [ print_timings] generation eval time = 66275.69 ms / 1067 runs ( 62.11 ms per token, 16.10 tokens per second) | tid=&amp;quot;140438394236928&amp;quot; ti&lt;/code&gt;&lt;br/&gt; &lt;code&gt;mestamp=1746406901 id_slot=0 id_task=0 t_token_generation=66275.693 n_decoded=1067 t_token=62.11405154639175 n_tokens_second=16.099416719791975&lt;/code&gt;&lt;/p&gt; &lt;p&gt;So basically 10% more speed in PP and similar generation t/s.&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q6_K, llamacpp)&lt;/h1&gt; &lt;p&gt;This is the point where models are really close to Q8 and then to F16. This was more for test porpouses, but still is very usable.&lt;/p&gt; &lt;p&gt;This uses about 70GB RAM and rest on VRAM.&lt;/p&gt; &lt;p&gt;&lt;code&gt;Command to run was:&lt;/code&gt;&lt;br/&gt; &lt;code&gt;./llama-server -m &amp;#39;/models_llm/Qwen3-235B-A22B-128K-Q6_K-00001-of-00004.gguf&amp;#39; -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk\.(0|1|2|3|4|5|6|7|8)\.ffn.*=CUDA0&amp;quot; -ot &amp;quot;blk\.(9|10|11|12|13|14|15|16|17)\.ffn.*=CUDA1&amp;quot; -ot &amp;quot;blk\.(18|19|20|21|22|23|24|25|26|27|28|29|30)\.ffn.*=CUDA2&amp;quot; -ot &amp;quot;blk\.(31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51|52)\.ffn.*=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speed are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 57152.69 ms / 3877 tokens ( 14.74 ms per token, 67.84 tokens per second) eval time = 38705.90 ms / 318 tokens ( 121.72 ms per token, 8.22 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;h1&gt;Qwen3 235B (Q6_K, ik llamacpp)&lt;/h1&gt; &lt;p&gt;ik llamacpp makes a huge increase in PP performance.&lt;/p&gt; &lt;p&gt;Command to run was:&lt;/p&gt; &lt;p&gt;./llama-server -m &amp;#39;/models_llm/Qwen3-235B-A22B-128K-Q6_K-00001-of-00004.gguf&amp;#39; -c 32768 --no-mmap --no-warmup -ngl 999 -ot &amp;quot;blk\.(0|1|2|3|4|5|6|7|8)\.ffn.*=CUDA0&amp;quot; -ot &amp;quot;blk\.(9|10|11|12|13|14|15|16|17)\.ffn.*=CUDA1&amp;quot; -ot &amp;quot;blk\.(18|19|20|21|22|23|24|25|26|27|28|29|30)\.ffn.*=CUDA2&amp;quot; -ot &amp;quot;blk\.(31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51|52)\.ffn.*=CUDA3&amp;quot; -ot &amp;quot;ffn.*=CPU&amp;quot; -fmoe -amb 512 -rtr&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;INFO [ print_timings] prompt eval time = 36897.66 ms / 3877 tokens ( 9.52 ms per token, 105.07 tokens per second) | tid=&amp;quot;140095757803520&amp;quot; timestamp=1746307138 id_slot=0 id_task=0 t_prompt_processing=36897.659 n_prompt_tokens_processed=3877 t_token=9.517064482847562 n_tokens_second=105.07441678075024&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;INFO [ print_timings] generation eval time = 143560.31 ms / 1197 runs ( 119.93 ms per token, 8.34 tokens per second) | tid=&amp;quot;140095757803520&amp;quot; timestamp=1746307138 id_slot=0 id_task=0 t_token_generation=143560.31 n_decoded=1197 t_token=119.93342522974102 n_tokens_second=8.337959147622348&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Basically 40-50% more PP performance and similar generation speed.&lt;/p&gt; &lt;h1&gt;Llama 3.1 Nemotron 253B (Q3_K_XL, llamacpp)&lt;/h1&gt; &lt;p&gt;This model was PAINFUL to make it work fully on GPU, as layers are uneven. Some layers near the end are 8B each.&lt;/p&gt; &lt;p&gt;This is also the only model I had to use CTK8/CTV4, else it doesn&amp;#39;t fit. &lt;/p&gt; &lt;p&gt;The commands to run it were:&lt;/p&gt; &lt;p&gt;&lt;code&gt;export CUDA_VISIBLE_DEVICES=0,1,3,2&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m /run/media/pancho/08329F4A329F3B9E/models_llm/Llama-3_1-Nemotron-Ultra-253B-v1-UD-Q3_K_XL-00001-of-00003.gguf -c 32768 -ngl 163 -ts 6.5,6,10,4 --no-warmup -fa -ctk q8_0 -ctv q4_0 -mg 2 --prio 3&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I don&amp;#39;t have the specific speeds at the moment (as to run this model I have to close any application of my desktop), but they are, from a picture I got some days ago:&lt;/p&gt; &lt;p&gt;&lt;code&gt;PP: 130 t/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Generation speed: 7.5 t/s&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Cache size is 5GB for 32K and 10GB for 64K.&lt;/p&gt; &lt;h1&gt;c4ai-command-a-03-2025 111B (Q6_K, llamacpp)&lt;/h1&gt; &lt;p&gt;I particullay have liked command a models, and I also feel this model is great. Ran on GPU only.&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m &amp;#39;/GGUFs/CohereForAI_c4ai-command-a-03-2025-Q6_K-merged.gguf&amp;#39; -c 32768 -ngl 99 -ts 10,11,17,20 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 4101.94 ms / 3403 tokens ( 1.21 ms per token, 829.61 tokens per second)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;eval time = 46452.40 ms / 472 tokens ( 98.42 ms per token, 10.16 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For reference: EXL2 with the same quant size gets ~12 t/s.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Cache size is 8GB for 32K and 16GB for 64K.&lt;/p&gt; &lt;h1&gt;Mistral Large 2411 123B (Q4_K_M, llamacpp)&lt;/h1&gt; &lt;p&gt;Also have been a fan of Mistral Large models, as they work pretty good!&lt;/p&gt; &lt;p&gt;Command to run it was:&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m &amp;#39;/run/media/pancho/DE1652041651DDD9/HuggingFaceModelDownload&lt;/code&gt;&lt;br/&gt; &lt;code&gt;er/Storage/GGUFs/Mistral-Large-Instruct-2411-Q4_K_M-merged.gguf&amp;#39; -c 32768 -ngl 99 -ts 7,7,10,5 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And speeds are:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 4427.90 ms / 3956 tokens ( 1.12 ms per token, 893.43 tokens per second)&lt;/code&gt;&lt;br/&gt; &lt;code&gt;eval time = 30739.23 ms / 387 tokens ( 79.43 ms per token, 12.59 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Cache size is quite big, 12GB for 32K and 24GB for 64K. In fact it is so big that if I want to load it on 3 GPUs (since size is 68GB) I need to use flash attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For reference: EXL2 with this same size gets 25 t/s with Tensor Parallel enabled. And 16-20 t/s on 6.5bpw EXL2 (EXL2 lets you to use TP with uneven VRAM)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That&amp;#39;s all the tests I have been running lately! I have been testing for both coding (python, C, C++) and RP. Not sure if you guys are interested in which one I prefer for each task or rank them.&lt;/p&gt; &lt;p&gt;Any question is welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/panchovix&quot;&gt; /u/panchovix &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1kezq68</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/" /><updated>2025-05-05T01:19:58+00:00</updated><published>2025-05-05T01:19:58+00:00</published><title>Speed metrics running DeepSeekV3 0324/Qwen3 235B and other models, on 128GB VRAM (5090+4090x2+A6000) + 192GB RAM on Consumer motherboard/CPU (llamacpp/ikllamacpp)</title></entry><entry><author><name>/u/____vladrad</name><uri>https://www.reddit.com/user/____vladrad</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Now this is pod racing . I have a a100 and a 4090. I tried your tests on qwen 3 and could not get those it-ot layers right 3-4 days ago. Thanks for sharing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqn1tr5</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/mqn1tr5/"/><updated>2025-05-05T02:04:26+00:00</updated><title>/u/____vladrad on Speed metrics running DeepSeekV3 0324/Qwen3 235B and other models, on 128GB VRAM (5090+4090x2+A6000) + 192GB RAM on Consumer motherboard/CPU (llamacpp/ikllamacpp)</title></entry><entry><author><name>/u/segmond</name><uri>https://www.reddit.com/user/segmond</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;how do you specify for it to use mla or not to use mla with pure llama.cpp? how are you deciding how many layers to offload to each device, it&amp;#39;s not evenly distributed.&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(0|1|2|3|4|5|6|7|8)\.ffn.*=CUDA0&amp;quot; -ot &amp;quot;blk\.(9|10|11|12|13|14|15|16|17)\.ffn.*=CUDA1&amp;quot; -ot &amp;quot;blk\.(18|19|20|21|22|23|24|25|26|27|28|29|30)\.ffn.*=CUDA2&amp;quot; -ot &amp;quot;blk\.(31|32|33|34|35|36|37|38|39|40|41|42|43|44|45|46|47|48|49|50|51|52)\.ffn.*=CUDA3&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqmvt1s</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/mqmvt1s/"/><updated>2025-05-05T01:28:13+00:00</updated><title>/u/segmond on Speed metrics running DeepSeekV3 0324/Qwen3 235B and other models, on 128GB VRAM (5090+4090x2+A6000) + 192GB RAM on Consumer motherboard/CPU (llamacpp/ikllamacpp)</title></entry><entry><author><name>/u/Ardalok</name><uri>https://www.reddit.com/user/Ardalok</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Am I blind or there is no Nemotron tests? Good job anyway!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqmxzse</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/mqmxzse/"/><updated>2025-05-05T01:40:59+00:00</updated><title>/u/Ardalok on Speed metrics running DeepSeekV3 0324/Qwen3 235B and other models, on 128GB VRAM (5090+4090x2+A6000) + 192GB RAM on Consumer motherboard/CPU (llamacpp/ikllamacpp)</title></entry><entry><author><name>/u/ProfessionUpbeat4500</name><uri>https://www.reddit.com/user/ProfessionUpbeat4500</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How about asking those LLM to provide a TLDR/ conclusion for us reddit reader ðŸ¤£&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqnoq5m</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/mqnoq5m/"/><updated>2025-05-05T04:44:42+00:00</updated><title>/u/ProfessionUpbeat4500 on Speed metrics running DeepSeekV3 0324/Qwen3 235B and other models, on 128GB VRAM (5090+4090x2+A6000) + 192GB RAM on Consumer motherboard/CPU (llamacpp/ikllamacpp)</title></entry><entry><author><name>/u/Such_Advantage_6949</name><uri>https://www.reddit.com/user/Such_Advantage_6949</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Thanks. This just help to reinforce my decision that, big vram without proper setup to ultilize tensor parallel is not a good way to go. Except exl2, all other engine requires u to have similar gpu across. So i changed my set up to 5x3090 on server mother board. Then i managed to increase my tok/s for 70B q4 model from 18 tok/s (sequential model running) to 36 tok/s tensor parallel with vllm. With speculative decoding, coding question can even reach 75 tok/s. So i also gave up my idea of adding rtx 6000 to my setup&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqn0hyp</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/mqn0hyp/"/><updated>2025-05-05T01:56:09+00:00</updated><title>/u/Such_Advantage_6949 on Speed metrics running DeepSeekV3 0324/Qwen3 235B and other models, on 128GB VRAM (5090+4090x2+A6000) + 192GB RAM on Consumer motherboard/CPU (llamacpp/ikllamacpp)</title></entry><entry><author><name>/u/Turkino</name><uri>https://www.reddit.com/user/Turkino</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey I got that same motherboard. How fast were you able to get the memory running with all four slots filled?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqnn5z9</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kezq68/speed_metrics_running_deepseekv3_0324qwen3_235b/mqnn5z9/"/><updated>2025-05-05T04:31:42+00:00</updated><title>/u/Turkino on Speed metrics running DeepSeekV3 0324/Qwen3 235B and other models, on 128GB VRAM (5090+4090x2+A6000) + 192GB RAM on Consumer motherboard/CPU (llamacpp/ikllamacpp)</title></entry></feed>