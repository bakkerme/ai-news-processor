<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-05-05T05:32:22+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>Qwen3 on Dubesor Benchmark : LocalLLaMA</title><entry><author><name>/u/AaronFeng47</name><uri>https://www.reddit.com/user/AaronFeng47</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/&quot;&gt; &lt;img src=&quot;https://b.thumbs.redditmedia.com/SK05go83T5ZWwsJO6IMatx5Z4uUx4gveCuwSgrYHpAg.jpg&quot; alt=&quot;Qwen3 on Dubesor Benchmark&quot; title=&quot;Qwen3 on Dubesor Benchmark&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://dubesor.de/benchtable.html&quot;&gt;https://dubesor.de/benchtable.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the few benchmarks that tested both thinking on/off of qwen3 &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/eim5m35nxqye1.png?width=1265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd814d571735444429331c73b4cd17a066497907&quot;&gt;https://preview.redd.it/eim5m35nxqye1.png?width=1265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd814d571735444429331c73b4cd17a066497907&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Small-scale manual performance comparison benchmark I made for myself. This table showcases the results I recorded of various AI models across different personal tasks I encountered over time (currently 83). I use a &lt;strong&gt;weighted rating system&lt;/strong&gt; and calculate the difficulty for each tasks by incorporating the results of all models. This is particularly relevant in scoring when failing easy questions or passing hard ones.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE, THAT THIS JUST ME SHARING THE RESULTS FROM MY OWN SMALL-SCALE PERSONAL TESTING. YMMV! OBVIOUSLY THE SCORES ARE JUST THAT AND MIGHT NOT REFLECT YOUR OWN PERSONAL EXPERIENCES OR OTHER WELL-KNOWN BENCHMARKS.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AaronFeng47&quot;&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1keh542</id><media:thumbnail url="https://b.thumbs.redditmedia.com/SK05go83T5ZWwsJO6IMatx5Z4uUx4gveCuwSgrYHpAg.jpg" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/" /><updated>2025-05-04T10:57:18+00:00</updated><published>2025-05-04T10:57:18+00:00</published><title>Qwen3 on Dubesor Benchmark</title></entry><entry><author><name>/u/MustBeSomethingThere</name><uri>https://www.reddit.com/user/MustBeSomethingThere</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;You&amp;#39;re currently using Q4_K_M. You might want to try Unsloth UD-Q4_K_XL (for example, Qwen3-32B-UD-Q4_K_XL.gguf) to see if it makes a difference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqiqr9f</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/mqiqr9f/"/><updated>2025-05-04T11:33:06+00:00</updated><title>/u/MustBeSomethingThere on Qwen3 on Dubesor Benchmark</title></entry><entry><author><name>/u/AaronFeng47</name><uri>https://www.reddit.com/user/AaronFeng47</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;GLM vs Qwen&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/bhkx6tei5rye1.png?width=1217&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80d7a0aadaca853cb67561c533615fcf8f8f848b&quot;&gt;https://preview.redd.it/bhkx6tei5rye1.png?width=1217&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80d7a0aadaca853cb67561c533615fcf8f8f848b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqirqh2</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/mqirqh2/"/><updated>2025-05-04T11:41:12+00:00</updated><title>/u/AaronFeng47 on Qwen3 on Dubesor Benchmark</title></entry><entry><author><name>/u/ResearchCrafty1804</name><uri>https://www.reddit.com/user/ResearchCrafty1804</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I suggest you test Qwen3-30b-A3b fp8 as well. &lt;/p&gt; &lt;p&gt;I noticed that due to the small number of activated parameters this particular model is more sensitive than the rest of the models in Qwen3 series.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqj2n3h</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/mqj2n3h/"/><updated>2025-05-04T13:00:54+00:00</updated><title>/u/ResearchCrafty1804 on Qwen3 on Dubesor Benchmark</title></entry><entry><author><name>/u/Cool-Chemical-5629</name><uri>https://www.reddit.com/user/Cool-Chemical-5629</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So Qwen3-4B (thinking) beats the old much bigger Qwen2.5-32B-Instruct (non-thinking), Qwen2.5-14B-Instruct as well as Qwen3-8B (non-thinking).&lt;/p&gt; &lt;p&gt;Qwen3-14B (non-thinking) model unsurprisingly beats Qwen3-4B (thinking), but also lands just below the older Qwen2-72B-Instruct.&lt;/p&gt; &lt;p&gt;Qwen3-30B-A3B (non-thinking) tails R1-Distill-Qwen-32B (thinking only) which is pretty impressive since it should mean that it is able to deliver comparable quality without thinking, but more importantly Qwen3-30B-A3B (non-thinking) also beats the older Qwen2-72B-Instruct.&lt;/p&gt; &lt;p&gt;QwQ-32B (thinking only) lands just above Qwen3-32B (non-thinking), but far below Qwen3-32B (thinking).&lt;/p&gt; &lt;p&gt;Interestingly Qwen3-14B (thinking) and Qwen3-8B (thinking) both beat the old big Qwen2.5-Plus (non-thinking, API only) model.&lt;/p&gt; &lt;p&gt;And finally, Qwen3-30B-A3B (thinking) tails the old biggest Qwen2.5-Max (non-thinking, API only) which is only beaten by Qwen3-32B (thinking) and the current biggest Qwen3-235B-A22B in both thinking and non-thinking modes.&lt;/p&gt; &lt;p&gt;All in all, it looks as though the Qwen3-30B-A3B in non-thinking mode is a decent sweet spot somewhere in the middle and with thinking enabled it&amp;#39;s a very competent contender, all with the higher inference speed thanks to MoE architecture as a bonus.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqk9i07</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/mqk9i07/"/><updated>2025-05-04T16:53:52+00:00</updated><title>/u/Cool-Chemical-5629 on Qwen3 on Dubesor Benchmark</title></entry><entry><author><name>/u/RickyRickC137</name><uri>https://www.reddit.com/user/RickyRickC137</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Great efforts! Are you planning to continue this rankings as new model comes out?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mql6xim</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/mql6xim/"/><updated>2025-05-04T19:44:26+00:00</updated><title>/u/RickyRickC137 on Qwen3 on Dubesor Benchmark</title></entry><entry><author><name>/u/Impressive_Half_2819</name><uri>https://www.reddit.com/user/Impressive_Half_2819</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Unslloth is a life changer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqiul6m</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/mqiul6m/"/><updated>2025-05-04T12:03:46+00:00</updated><title>/u/Impressive_Half_2819 on Qwen3 on Dubesor Benchmark</title></entry><entry><author><name>/u/Healthy-Nebula-3603</name><uri>https://www.reddit.com/user/Healthy-Nebula-3603</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;qwen 3 32b has reasoning 44% and llama 3.3 70b 49% ?&lt;/p&gt; &lt;p&gt;LOL&lt;/p&gt; &lt;p&gt;No idea what question you have for reasoning but with certain qwen 3 32b thinking is far ahead in this fled...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqkas4y</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/mqkas4y/"/><updated>2025-05-04T17:00:11+00:00</updated><title>/u/Healthy-Nebula-3603 on Qwen3 on Dubesor Benchmark</title></entry></feed>