<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-04-21T06:26:22+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>LocalAI v2.28.0 + Announcing LocalAGI: Build &amp; Run AI Agents Locally Using Your Favorite LLMs : LocalLLaMA</title><entry><author><name>/u/mudler_it</name><uri>https://www.reddit.com/user/mudler_it</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/kcMkQKbjZGZ_QZiKW5RCtauHq1PUD9qoQ4d1zIJbWdg.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc273ae475c76068af1debbaf450c1139cfecac4&quot; alt=&quot;LocalAI v2.28.0 + Announcing LocalAGI: Build &amp;amp; Run AI Agents Locally Using Your Favorite LLMs&quot; title=&quot;LocalAI v2.28.0 + Announcing LocalAGI: Build &amp;amp; Run AI Agents Locally Using Your Favorite LLMs&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey &lt;a href=&quot;/r/LocalLLaMA&quot;&gt;r/LocalLLaMA&lt;/a&gt; fam!&lt;/p&gt; &lt;p&gt;Got an update and a pretty exciting announcement relevant to running and &lt;em&gt;using&lt;/em&gt; your local LLMs in more advanced ways. We&amp;#39;ve just shipped &lt;strong&gt;LocalAI v2.28.0&lt;/strong&gt;, but the bigger news is the launch of &lt;strong&gt;LocalAGI&lt;/strong&gt;, a new platform for building AI agent workflows that leverages your local models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LocalAI (v2.28.0):&lt;/strong&gt; Our open-source inference server (acting as an OpenAI API for backends like llama.cpp, Transformers, etc.) gets updates. Link:&lt;a href=&quot;https://github.com/mudler/LocalAI&quot;&gt;https://github.com/mudler/LocalAI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LocalAGI (New!):&lt;/strong&gt; A self-hosted &lt;strong&gt;AI Agent Orchestration platform&lt;/strong&gt; (rewritten in Go) with a WebUI. Lets you build complex agent tasks (think AutoGPT-style) that are powered by &lt;strong&gt;your local LLMs&lt;/strong&gt; via an OpenAI-compatible API. Link:&lt;a href=&quot;https://github.com/mudler/LocalAGI&quot;&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LocalRecall (New-ish):&lt;/strong&gt; A companion local REST API for agent memory. Link:&lt;a href=&quot;https://github.com/mudler/LocalRecall&quot;&gt;https://github.com/mudler/LocalRecall&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Key Idea:&lt;/strong&gt; Use your preferred local models (served via LocalAI or another compatible API) as the &amp;quot;brains&amp;quot; for autonomous agents running complex tasks, all locally.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quick Context: LocalAI as your Local Inference Server&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Many of you know LocalAI as a way to slap an OpenAI-compatible API onto various model backends. You can point it at your GGUF files (using its built-in llama.cpp backend), Hugging Face models, Diffusers for image gen, etc., and interact with them via a standard API, all locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Introducing LocalAGI: Using Your Local LLMs for Agentic Tasks&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is where it gets really interesting for this community. LocalAGI is designed to let you build workflows where AI agents collaborate, use tools, and perform multi-step tasks. It works better with LocalAI as it leverages internal capabilities for structured output, but should work as well with other providers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How does it use&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;your&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;local LLMs?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LocalAGI connects to any &lt;strong&gt;OpenAI-compatible API endpoint&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;You can simply point LocalAGI to your running &lt;strong&gt;LocalAI instance&lt;/strong&gt; (which is serving your Llama 3, Mistral, Mixtral, Phi, or whatever GGUF/HF model you prefer).&lt;/li&gt; &lt;li&gt;Alternatively, if you&amp;#39;re using another OpenAI-compatible server (like &lt;code&gt;llama-cpp-python&lt;/code&gt;&amp;#39;s server mode, vLLM&amp;#39;s API, etc.), you can likely point LocalAGI to that too.&lt;/li&gt; &lt;li&gt;Your local LLM then becomes the decision-making engine for the agents within LocalAGI.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Features of LocalAGI:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Runs Locally:&lt;/strong&gt; Like LocalAI, it&amp;#39;s designed to run entirely on your hardware. No data leaves your machine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;WebUI for Management:&lt;/strong&gt; Configure agent roles, prompts, models, tool access, and multi-agent &amp;quot;groups&amp;quot; visually. No drag and drop stuff.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool Usage:&lt;/strong&gt; Allow agents to interact with external tools or APIs (potentially custom local tools too).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Connectors:&lt;/strong&gt; Ready-to-go connectors for Telegram, Discord, Slack, IRC, and more to come.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Persistent Memory:&lt;/strong&gt; Integrates with LocalRecall (also local) for long-term memory capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;API:&lt;/strong&gt; Agents can be created programmatically via API, and every agent can be used via REST-API, providing drop-in replacement for OpenAI&amp;#39;s Responses APIs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Go Backend:&lt;/strong&gt; Rewritten in Go for efficiency.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source (MIT).&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the UI for configuring agents:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/x3ud7tfxd6ve1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=456a83ed666dcf55ba70904126a3df43a7673661&quot;&gt;https://preview.redd.it/x3ud7tfxd6ve1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=456a83ed666dcf55ba70904126a3df43a7673661&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/dtjimnfxd6ve1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a06b61976c3e78d3f78bc64e4e9b1eda3864fc38&quot;&gt;https://preview.redd.it/dtjimnfxd6ve1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a06b61976c3e78d3f78bc64e4e9b1eda3864fc38&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/403v7ofxd6ve1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99880111207fae2064732ed91f6e6c3ca9db9736&quot;&gt;https://preview.redd.it/403v7ofxd6ve1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99880111207fae2064732ed91f6e6c3ca9db9736&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LocalAI v2.28.0 Updates&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The underlying LocalAI inference server also got some updates:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SYCL support via &lt;code&gt;stablediffusion.cpp&lt;/code&gt; (relevant for some Intel GPUs).&lt;/li&gt; &lt;li&gt;Support for the Lumina Text-to-Image models.&lt;/li&gt; &lt;li&gt;Various backend improvements and bug fixes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why is this Interesting for&lt;/strong&gt; &lt;a href=&quot;/r/LocalLLaMA&quot;&gt;r/LocalLLaMA&lt;/a&gt;&lt;strong&gt;?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This stack (LocalAI + LocalAGI) provides a way to leverage the powerful local models we all spend time setting up and tuning for more than just chat or single-prompt tasks. You can start building:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Autonomous research agents.&lt;/li&gt; &lt;li&gt;Code generation/debugging workflows.&lt;/li&gt; &lt;li&gt;Content summarization/analysis pipelines.&lt;/li&gt; &lt;li&gt;RAG setups with agentic interaction.&lt;/li&gt; &lt;li&gt;Anything where multiple steps or &amp;quot;thinking&amp;quot; loops powered by your local LLM would be beneficial.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Getting Started&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Docker is probably the easiest way to get both LocalAI and LocalAGI running. Check the READMEs in the repos for setup instructions and docker-compose examples. You&amp;#39;ll configure LocalAGI with the API endpoint address of your LocalAI (or other compatible) server or just run the complete stack from the docker-compose files.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LocalAI (Inference Server):&lt;/strong&gt;&lt;a href=&quot;https://github.com/mudler/LocalAI&quot;&gt;https://github.com/mudler/LocalAI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LocalAGI (Agent Platform):&lt;/strong&gt;&lt;a href=&quot;https://github.com/mudler/LocalAGI&quot;&gt;https://github.com/mudler/LocalAGI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LocalRecall (Memory):&lt;/strong&gt;&lt;a href=&quot;https://github.com/mudler/LocalRecall&quot;&gt;https://github.com/mudler/LocalRecall&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Release notes:&lt;/strong&gt; &lt;a href=&quot;https://github.com/mudler/LocalAI/releases/tag/v2.28.0&quot;&gt;&lt;strong&gt;https://github.com/mudler/LocalAI/releases/tag/v2.28.0&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We believe this combo opens up many possibilities for local LLMs. We&amp;#39;re keen to hear your thoughts! Would you try running agents with your local models? What kind of workflows would you build? Any feedback on connecting LocalAGI to different local API servers would also be great.&lt;/p&gt; &lt;p&gt;Let us know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/mudler_it&quot;&gt; /u/mudler_it &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1k0haqw</id><media:thumbnail url="https://external-preview.redd.it/kcMkQKbjZGZ_QZiKW5RCtauHq1PUD9qoQ4d1zIJbWdg.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc273ae475c76068af1debbaf450c1139cfecac4" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/" /><updated>2025-04-16T10:41:06+00:00</updated><published>2025-04-16T10:41:06+00:00</published><title>LocalAI v2.28.0 + Announcing LocalAGI: Build &amp; Run AI Agents Locally Using Your Favorite LLMs</title></entry><entry><author><name>/u/jacek2023</name><uri>https://www.reddit.com/user/jacek2023</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Would be nice to watch YouTube video with the demo, is it available somewhere?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnebu5x</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/mnebu5x/"/><updated>2025-04-16T12:27:22+00:00</updated><title>/u/jacek2023 on LocalAI v2.28.0 + Announcing LocalAGI: Build &amp; Run AI Agents Locally Using Your Favorite LLMs</title></entry><entry><author><name>/u/ForsookComparison</name><uri>https://www.reddit.com/user/ForsookComparison</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is very neat. I like the color scheme&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mne885w</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/mne885w/"/><updated>2025-04-16T12:03:13+00:00</updated><title>/u/ForsookComparison on LocalAI v2.28.0 + Announcing LocalAGI: Build &amp; Run AI Agents Locally Using Your Favorite LLMs</title></entry><entry><author><name>/u/Blues520</name><uri>https://www.reddit.com/user/Blues520</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This is super cool&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnjmi4g</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/mnjmi4g/"/><updated>2025-04-17T06:50:53+00:00</updated><title>/u/Blues520 on LocalAI v2.28.0 + Announcing LocalAGI: Build &amp; Run AI Agents Locally Using Your Favorite LLMs</title></entry><entry><author><name>/u/zoidme</name><uri>https://www.reddit.com/user/zoidme</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Does LocalAI run interference in same container or can spawn containers? Does it support parallel running multiple llms?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mng36zz</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/mng36zz/"/><updated>2025-04-16T17:51:35+00:00</updated><title>/u/zoidme on LocalAI v2.28.0 + Announcing LocalAGI: Build &amp; Run AI Agents Locally Using Your Favorite LLMs</title></entry><entry><author><name>/u/Kart_driver_bb_234</name><uri>https://www.reddit.com/user/Kart_driver_bb_234</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;can this launch multiple vllm like backends (i.e open ai compatible apis) at the same time ? or at least be able to automatically load and unload models on demand ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnkjlrq</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/mnkjlrq/"/><updated>2025-04-17T11:59:14+00:00</updated><title>/u/Kart_driver_bb_234 on LocalAI v2.28.0 + Announcing LocalAGI: Build &amp; Run AI Agents Locally Using Your Favorite LLMs</title></entry></feed>