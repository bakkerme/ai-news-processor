<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-05-05T05:31:52+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>Qwen 30B A3B performance degradation with KV quantization : LocalLLaMA</title><entry><author><name>/u/fakezeta</name><uri>https://www.reddit.com/user/fakezeta</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I came across this gist &lt;a href=&quot;https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4&quot;&gt;https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4&lt;/a&gt; that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization.&lt;/p&gt; &lt;p&gt;I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens.&lt;/p&gt; &lt;p&gt;I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn&amp;#39;t entered in any repetition loop but generated lots of thinking tokens without finding any solution.&lt;/p&gt; &lt;p&gt;Then I saw that sunpazed didn&amp;#39;t used KV quantization and tried the same: boom! First time right.&lt;/p&gt; &lt;p&gt;It worked with Q5_K_S and also with Q4_K_XL&lt;/p&gt; &lt;p&gt;For who wants more details I leave here a gist &lt;a href=&quot;https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef&quot;&gt;https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do you have any report of performance degradation with long generations on Qwen3 30B A3B and KV quantization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/fakezeta&quot;&gt; /u/fakezeta &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1kewkno</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/" /><updated>2025-05-04T22:43:11+00:00</updated><published>2025-05-04T22:43:11+00:00</published><title>Qwen 30B A3B performance degradation with KV quantization</title></entry><entry><author><name>/u/dinerburgeryum</name><uri>https://www.reddit.com/user/dinerburgeryum</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What KV quant level were you using? IMO on llama.cpp you shouldn’t push it past Q8_0. Q4_0 cache quant tanks quality in any model and especially models that heavily leverage GQA. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqm4xf3</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/mqm4xf3/"/><updated>2025-05-04T22:48:11+00:00</updated><title>/u/dinerburgeryum on Qwen 30B A3B performance degradation with KV quantization</title></entry><entry><author><name>/u/-Ellary-</name><uri>https://www.reddit.com/user/-Ellary-</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have one rule: I always test ALL new models without flash attention and with full 16bit KV cache.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqmgo6e</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/mqmgo6e/"/><updated>2025-05-04T23:56:25+00:00</updated><title>/u/-Ellary- on Qwen 30B A3B performance degradation with KV quantization</title></entry><entry><author><name>/u/pseudonym325</name><uri>https://www.reddit.com/user/pseudonym325</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Which KV quantization are you using? Don&amp;#39;t have time to run this test right now, but I usually use -ctk q8_0 -ctv q5_1 (requires -DGGML_CUDA_FA_ALL_QUANTS=on)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqm5hbv</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/mqm5hbv/"/><updated>2025-05-04T22:51:24+00:00</updated><title>/u/pseudonym325 on Qwen 30B A3B performance degradation with KV quantization</title></entry><entry><author><name>/u/danihend</name><uri>https://www.reddit.com/user/danihend</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Ive only tried KV quantization once and saw that any amount of it makes models super dumb. Not sure why anybody uses it tbh&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqmc4ee</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/mqmc4ee/"/><updated>2025-05-04T23:30:04+00:00</updated><title>/u/danihend on Qwen 30B A3B performance degradation with KV quantization</title></entry><entry><author><name>/u/FriskyFennecFox</name><uri>https://www.reddit.com/user/FriskyFennecFox</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Which quantization did you use initially?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqm53n2</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/mqm53n2/"/><updated>2025-05-04T22:49:11+00:00</updated><title>/u/FriskyFennecFox on Qwen 30B A3B performance degradation with KV quantization</title></entry><entry><author><name>/u/Turkino</name><uri>https://www.reddit.com/user/Turkino</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Interested here since I&amp;#39;m running a q6&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqma484</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/mqma484/"/><updated>2025-05-04T23:18:35+00:00</updated><title>/u/Turkino on Qwen 30B A3B performance degradation with KV quantization</title></entry><entry><author><name>/u/Steuern_Runter</name><uri>https://www.reddit.com/user/Steuern_Runter</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Use these parameters:&lt;/p&gt; &lt;p&gt;Thinking Mode Settings:&lt;/p&gt; &lt;p&gt;Temperature = 0.6&lt;/p&gt; &lt;p&gt;Min_P = 0.0&lt;/p&gt; &lt;p&gt;Top_P = 0.95&lt;/p&gt; &lt;p&gt;TopK = 20&lt;/p&gt; &lt;p&gt;Non-Thinking Mode Settings:&lt;/p&gt; &lt;p&gt;Temperature = 0.7 &lt;/p&gt; &lt;p&gt;Min_P = 0.0&lt;/p&gt; &lt;p&gt;Top_P = 0.8&lt;/p&gt; &lt;p&gt;TopK = 20&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqmacal</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/mqmacal/"/><updated>2025-05-04T23:19:53+00:00</updated><title>/u/Steuern_Runter on Qwen 30B A3B performance degradation with KV quantization</title></entry><entry><author><name>/u/tinbtb</name><uri>https://www.reddit.com/user/tinbtb</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Could you please tell us how to disable KV cache quantisation? I&amp;#39;d also like to check the difference. What is the difference in the amount of memory used with KV running at fp16 in comparison with regular q4?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqmjxd9</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/mqmjxd9/"/><updated>2025-05-05T00:15:51+00:00</updated><title>/u/tinbtb on Qwen 30B A3B performance degradation with KV quantization</title></entry><entry><author><name>/u/silenceimpaired</name><uri>https://www.reddit.com/user/silenceimpaired</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I’m confused. Isn’t K_M KV quantization? And yet you said Qwen 30b solved the rest with Q4 K_M?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqmf5dk</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/mqmf5dk/"/><updated>2025-05-04T23:47:28+00:00</updated><title>/u/silenceimpaired on Qwen 30B A3B performance degradation with KV quantization</title></entry><entry><author><name>/u/Healthy-Nebula-3603</name><uri>https://www.reddit.com/user/Healthy-Nebula-3603</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Of course &lt;/p&gt; &lt;p&gt;Cache should always be fp16 even Q8 has degradation. Only flash attention is ok...ish ( as is fp16 )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqmq3ln</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/mqmq3ln/"/><updated>2025-05-05T00:53:20+00:00</updated><title>/u/Healthy-Nebula-3603 on Qwen 30B A3B performance degradation with KV quantization</title></entry></feed>