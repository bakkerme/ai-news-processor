<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-05-05T05:31:43+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison : LocalLLaMA</title><entry><author><name>/u/AaronFeng47</name><uri>https://www.reddit.com/user/AaronFeng47</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/jJ4wm0NIfgUy0MSOkw2YI6r-EjpVW_Y_SPR-xICfNk4.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bf4c693cb7ebd3ae7a7b3eb2dc65cfbfc6e1d6d&quot; alt=&quot;Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison&quot; title=&quot;Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Since IQ4_XS is my favorite quant for 32B models, I decided to run some benchmarks to compare IQ4_XS GGUFs from different sources.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MMLU-PRO 0.25 subset(3003 questions), 0 temp, No Think, IQ4_XS, Q8 KV Cache&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The entire benchmark took &lt;strong&gt;&lt;em&gt;11 hours, 37 minutes, and 30 seconds.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/9ptc0cl2svye1.png?width=2475&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06a3b551fba60a33877f8e67af9932e381a15cc6&quot;&gt;https://preview.redd.it/9ptc0cl2svye1.png?width=2475&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06a3b551fba60a33877f8e67af9932e381a15cc6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The difference is apparently minimum, so just keep using whatever iq4 quant you already downloaded. &lt;/p&gt; &lt;p&gt;&lt;em&gt;The official MMLU-PRO leaderboard is listing the score of Qwen3 base model instead of instruct, that&amp;#39;s why these iq4 quants score higher than the one on MMLU-PRO leaderboard.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;gguf source:&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/unsloth/Qwen3-32B-GGUF/blob/main/Qwen3-32B-IQ4_XS.gguf&quot;&gt;https://huggingface.co/unsloth/Qwen3-32B-GGUF/blob/main/Qwen3-32B-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/unsloth/Qwen3-32B-128K-GGUF/blob/main/Qwen3-32B-128K-IQ4_XS.gguf&quot;&gt;https://huggingface.co/unsloth/Qwen3-32B-128K-GGUF/blob/main/Qwen3-32B-128K-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF/blob/main/Qwen_Qwen3-32B-IQ4_XS.gguf&quot;&gt;https://huggingface.co/bartowski/Qwen_Qwen3-32B-GGUF/blob/main/Qwen_Qwen3-32B-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://huggingface.co/mradermacher/Qwen3-32B-i1-GGUF/blob/main/Qwen3-32B.i1-IQ4_XS.gguf&quot;&gt;https://huggingface.co/mradermacher/Qwen3-32B-i1-GGUF/blob/main/Qwen3-32B.i1-IQ4_XS.gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AaronFeng47&quot;&gt; /u/AaronFeng47 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1kf1yg9</id><media:thumbnail url="https://external-preview.redd.it/jJ4wm0NIfgUy0MSOkw2YI6r-EjpVW_Y_SPR-xICfNk4.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8bf4c693cb7ebd3ae7a7b3eb2dc65cfbfc6e1d6d" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/" /><updated>2025-05-05T03:21:49+00:00</updated><published>2025-05-05T03:21:49+00:00</published><title>Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison</title></entry><entry><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;[deleted]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqnefl9</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/mqnefl9/"/><updated>2025-05-05T03:25:42+00:00</updated><title>/u/[deleted] on Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison</title></entry><entry><author><name>/u/the_masel</name><uri>https://www.reddit.com/user/the_masel</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Interesting, thanks for the effort!&lt;/p&gt; &lt;p&gt;Anything special about IQ4_XS? How does it compare with others?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqnfour</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/mqnfour/"/><updated>2025-05-05T03:34:36+00:00</updated><title>/u/the_masel on Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison</title></entry><entry><author><name>/u/My_Unbiased_Opinion</name><uri>https://www.reddit.com/user/My_Unbiased_Opinion</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I appreciate all the people putting in work. Unsloth quants I have found to be the best. &lt;/p&gt; &lt;p&gt;Any chance perhaps you can take a look at 30B A3B? Some reports that quanting really hurts performance. Maybe test a more CPU optimal quant like Q4K_XL and even Q2K_XL since Q2 according to the documentation is the best in terms of performance relative to model size in GB. This would mean it runs faster on CPU inference. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqnkesa</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kf1yg9/qwen332biq4_xs_ggufs_mmlupro_benchmark_comparison/mqnkesa/"/><updated>2025-05-05T04:09:32+00:00</updated><title>/u/My_Unbiased_Opinion on Qwen3-32B-IQ4_XS GGUFs - MMLU-PRO benchmark comparison</title></entry></feed>