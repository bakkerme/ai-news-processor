<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-05-05T05:32:03+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>Which coding model is best for 48GB VRAM : LocalLLaMA</title><entry><author><name>/u/Su1tz</name><uri>https://www.reddit.com/user/Su1tz</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;It is for data science, mostly excel data manipulation in python. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Su1tz&quot;&gt; /u/Su1tz &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1kemt2m</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/" /><updated>2025-05-04T15:42:29+00:00</updated><published>2025-05-04T15:42:29+00:00</published><title>Which coding model is best for 48GB VRAM</title></entry><entry><author><name>/u/RoyalCities</name><uri>https://www.reddit.com/user/RoyalCities</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;GLM-4 has been my go to. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/s/Xz5Pxn5OaP&quot;&gt;https://www.reddit.com/r/LocalLLaMA/s/Xz5Pxn5OaP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqk08dx</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/mqk08dx/"/><updated>2025-05-04T16:06:17+00:00</updated><title>/u/RoyalCities on Which coding model is best for 48GB VRAM</title></entry><entry><author><name>/u/AppearanceHeavy6724</name><uri>https://www.reddit.com/user/AppearanceHeavy6724</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Qwen 3 32b, Qwen 2.5 coder 32b. &lt;/p&gt; &lt;p&gt;30B is okay too, but make sure you use a good quant; with your VRAM I&amp;#39;d go with Q8.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqk136n</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/mqk136n/"/><updated>2025-05-04T16:10:45+00:00</updated><title>/u/AppearanceHeavy6724 on Which coding model is best for 48GB VRAM</title></entry><entry><author><name>/u/coding_workflow</name><uri>https://www.reddit.com/user/coding_workflow</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Qwen 3 32B / 14B / Gemma 3 / Phi 4.&lt;/p&gt; &lt;p&gt;Not sure if I missed any. Avoid the Deepseek overhyped as the real Deepseek never fit in 48 GB.&lt;/p&gt; &lt;p&gt;Edit: fixed typo&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqjxlpd</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/mqjxlpd/"/><updated>2025-05-04T15:52:38+00:00</updated><title>/u/coding_workflow on Which coding model is best for 48GB VRAM</title></entry><entry><author><name>/u/Ok-Fault-9142</name><uri>https://www.reddit.com/user/Ok-Fault-9142</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;For my personal tasks mistral-small is the best. You should try all of them and make your own conclusions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqkcehp</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/mqkcehp/"/><updated>2025-05-04T17:08:13+00:00</updated><title>/u/Ok-Fault-9142 on Which coding model is best for 48GB VRAM</title></entry><entry><author><name>/u/tingshuo</name><uri>https://www.reddit.com/user/tingshuo</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Codestral is a very good model and outperforms a lot of other larger models on coding tasks and is very fast&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqjyd2t</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kemt2m/which_coding_model_is_best_for_48gb_vram/mqjyd2t/"/><updated>2025-05-04T15:56:34+00:00</updated><title>/u/tingshuo on Which coding model is best for 48GB VRAM</title></entry></feed>