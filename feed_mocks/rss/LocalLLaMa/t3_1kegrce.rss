<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-05-05T05:32:13+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>Qwen3 no reasoning vs Qwen2.5 : LocalLLaMA</title><entry><author><name>/u/No-Bicycle-132</name><uri>https://www.reddit.com/user/No-Bicycle-132</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;It seems evident that Qwen3 with reasoning beats Qwen2.5. But I wonder if the Qwen3 dense models with reasoning turned off also outperforms Qwen2.5. Essentially what I am wondering is if the improvements mostly come from the reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/No-Bicycle-132&quot;&gt; /u/No-Bicycle-132 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1kegrce</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/" /><updated>2025-05-04T10:31:46+00:00</updated><published>2025-05-04T10:31:46+00:00</published><title>Qwen3 no reasoning vs Qwen2.5</title></entry><entry><author><name>/u/frivolousfidget</name><uri>https://www.reddit.com/user/frivolousfidget</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;On aider polyglot there was a huge boost from 16.4% of qwen 2.5 coder whole to qwen 3 32B (no thinking) achieving 45.8% being ahead of gpt 4.5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqikpl2</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/mqikpl2/"/><updated>2025-05-04T10:38:43+00:00</updated><title>/u/frivolousfidget on Qwen3 no reasoning vs Qwen2.5</title></entry><entry><author><name>/u/segmond</name><uri>https://www.reddit.com/user/segmond</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Don&amp;#39;t stop at wondering. Why don&amp;#39;t you test it and share your result with us?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqj1e30</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/mqj1e30/"/><updated>2025-05-04T12:52:37+00:00</updated><title>/u/segmond on Qwen3 no reasoning vs Qwen2.5</title></entry><entry><author><name>/u/AaronFeng47</name><uri>https://www.reddit.com/user/AaronFeng47</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/&quot;&gt;https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqj5di9</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/mqj5di9/"/><updated>2025-05-04T13:18:24+00:00</updated><title>/u/AaronFeng47 on Qwen3 no reasoning vs Qwen2.5</title></entry><entry><author><name>/u/raul3820</name><uri>https://www.reddit.com/user/raul3820</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Depends on the task. For code autocomplete Qwen/Qwen3-14B-AWQ nothink is awful. I like Qwen2.5-coder:14b.&lt;/p&gt; &lt;p&gt;Additionally: some quants might be broken.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqinren</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/mqinren/"/><updated>2025-05-04T11:07:02+00:00</updated><title>/u/raul3820 on Qwen3 no reasoning vs Qwen2.5</title></entry><entry><author><name>/u/13henday</name><uri>https://www.reddit.com/user/13henday</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The 2.5 coders are better at complex one shots. 3.0 seems to generalize better and retains logic over a multiturn edit. My work involves updating lots of legacy Fortran and cobol that is written with very specific formatting and comment practices. 3.0 is the first open model that can run reasonably at 48gb vram that can reliably port my code. Also I think, for coding one shot diffs, reasoning turned off produces better results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqk1bo0</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/mqk1bo0/"/><updated>2025-05-04T16:11:59+00:00</updated><title>/u/13henday on Qwen3 no reasoning vs Qwen2.5</title></entry><entry><author><name>/u/Admirable-Star7088</name><uri>https://www.reddit.com/user/Admirable-Star7088</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have compared them far too little to be able to draw a serious conclusion, but from the very few comparisons I have made in coding, Qwen3 (no thinking) outputs better code, more accordingly to the prompt, than Qwen2.5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqj0spu</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/mqj0spu/"/><updated>2025-05-04T12:48:36+00:00</updated><title>/u/Admirable-Star7088 on Qwen3 no reasoning vs Qwen2.5</title></entry><entry><author><name>/u/sxales</name><uri>https://www.reddit.com/user/sxales</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The short answer is it entirely depends on your use case. In my limited testing, their overall performance was pretty close, with Qwen 3 probably being better overall. &lt;/p&gt; &lt;p&gt;I know the benchmarks say otherwise, but when translating Japanese to English, I found Qwen 2.5 to sound more natural. &lt;/p&gt; &lt;p&gt;However, when summarizing short stories, Qwen 2.5 dissected the story like a technical manual, whereas Qwen 3 wrote (or tried to write) in the tone of the original story. &lt;/p&gt; &lt;p&gt;Qwen 3 seems to lose less when quantized than Qwen 2.5. I was shocked at how well Qwen 3 32b functioned even down to IQ2 (except for factual retrieval which as usual takes a big hit).&lt;/p&gt; &lt;p&gt;Coding, logical puzzles, and problem-solving seemed like a toss up. They both did it with more or less the same success; although, enabling reason will likely give Qwen 3 the edge.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqju6qy</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/mqju6qy/"/><updated>2025-05-04T15:35:03+00:00</updated><title>/u/sxales on Qwen3 no reasoning vs Qwen2.5</title></entry><entry><author><name>/u/Conscious_Cut_6144</name><uri>https://www.reddit.com/user/Conscious_Cut_6144</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Yes from what I have seen for apples to apples.&lt;/p&gt; &lt;p&gt;But the 2.5 coding models will probably still hold tier own vs regular 3 models with thinking off.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqjg3r3</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/mqjg3r3/"/><updated>2025-05-04T14:20:45+00:00</updated><title>/u/Conscious_Cut_6144 on Qwen3 no reasoning vs Qwen2.5</title></entry><entry><author><name>/u/AppearanceHeavy6724</name><uri>https://www.reddit.com/user/AppearanceHeavy6724</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;They do. Qwen3 8b outperforms 7b 2.5; at least because of that extra 1b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqikex8</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/mqikex8/"/><updated>2025-05-04T10:35:53+00:00</updated><title>/u/AppearanceHeavy6724 on Qwen3 no reasoning vs Qwen2.5</title></entry></feed>