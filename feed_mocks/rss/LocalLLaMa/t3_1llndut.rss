<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-06-28T00:31:02+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/?depth=1" type="text/html" /><subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle><title>Hunyuan-A13B released : LocalLLaMA</title><entry><author><name>/u/kristaller486</name><uri>https://www.reddit.com/user/kristaller486</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/B1uwVS2BmhDOjFW0XJ6pW7-r7n5zECGun4YlOmky9YY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=975cbb18dc0dd9f2342d47d40a0f9fb8fe177327&quot; alt=&quot;Hunyuan-A13B released&quot; title=&quot;Hunyuan-A13B released&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;From HF repo:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Model Introduction&lt;/p&gt; &lt;p&gt;With the rapid advancement of artificial intelligence technology, large language models (LLMs) have achieved remarkable progress in natural language processing, computer vision, and scientific tasks. However, as model scales continue to expand, optimizing resource consumption while maintaining high performance has become a critical challenge. To address this, we have explored Mixture of Experts (MoE) architectures. The newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters. It not only delivers high-performance results but also achieves optimal resource efficiency, successfully balancing computational power and resource utilization.&lt;/p&gt; &lt;p&gt;Key Features and Advantages&lt;/p&gt; &lt;p&gt;Compact yet Powerful: With only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models.&lt;/p&gt; &lt;p&gt;Hybrid Inference Support: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.&lt;/p&gt; &lt;p&gt;Ultra-Long Context Understanding: Natively supports a 256K context window, maintaining stable performance on long-text tasks.&lt;/p&gt; &lt;p&gt;Enhanced Agent Capabilities: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3 and Ï„-Bench.&lt;/p&gt; &lt;p&gt;Efficient Inference: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/kristaller486&quot;&gt; /u/kristaller486 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://huggingface.co/tencent/Hunyuan-A13B-Instruct&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1llndut</id><media:thumbnail url="https://external-preview.redd.it/B1uwVS2BmhDOjFW0XJ6pW7-r7n5zECGun4YlOmky9YY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=975cbb18dc0dd9f2342d47d40a0f9fb8fe177327" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/" /><updated>2025-06-27T06:59:21+00:00</updated><published>2025-06-27T06:59:21+00:00</published><title>Hunyuan-A13B released</title></entry><entry><author><name>/u/vincentz42</name><uri>https://www.reddit.com/user/vincentz42</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The evals are incredible and trade blows with DeepSeek R1-0120. &lt;/p&gt; &lt;p&gt;Note this model has 80B parameters in total and 13B active parameters. So it requires roughly the same amount of memory compared to Llama 3 70B while offering 5x throughput because of MoE.&lt;/p&gt; &lt;p&gt;This is what the Llama 4 Maverick should have been.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n00wjwn</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n00wjwn/"/><updated>2025-06-27T07:07:54+00:00</updated><title>/u/vincentz42 on Hunyuan-A13B released</title></entry><entry><author><name>/u/jferments</name><uri>https://www.reddit.com/user/jferments</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;80B-A13B is such a perfect sweet spot of power vs. VRAM usage .... and native 256k context ðŸ« ðŸ« ðŸ« &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n00wy61</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n00wy61/"/><updated>2025-06-27T07:11:36+00:00</updated><title>/u/jferments on Hunyuan-A13B released</title></entry><entry><author><name>/u/Admirable-Star7088</name><uri>https://www.reddit.com/user/Admirable-Star7088</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Perfect size for 64GB RAM systems, this is exactly the MoE size the community has wanted for a long time! Let&amp;#39;s goooooo!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n0197gn</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n0197gn/"/><updated>2025-06-27T09:12:01+00:00</updated><title>/u/Admirable-Star7088 on Hunyuan-A13B released</title></entry><entry><author><name>/u/lothariusdark</name><uri>https://www.reddit.com/user/lothariusdark</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;This doesnt work with llama.cpp yet, right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n00wmb1</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n00wmb1/"/><updated>2025-06-27T07:08:31+00:00</updated><title>/u/lothariusdark on Hunyuan-A13B released</title></entry><entry><author><name>/u/TeakTop</name><uri>https://www.reddit.com/user/TeakTop</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Wow this is a perfectly sized MoE. If the benchmarks live up, this model is one hell of a gift for local ai.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n00z313</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n00z313/"/><updated>2025-06-27T07:32:06+00:00</updated><title>/u/TeakTop on Hunyuan-A13B released</title></entry><entry><author><name>/u/ResearchCrafty1804</name><uri>https://www.reddit.com/user/ResearchCrafty1804</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What a great release!&lt;/p&gt; &lt;p&gt;They even provide benchmark for the q8 and q4 quants, I wish every model author would do that. &lt;/p&gt; &lt;p&gt;Looking forward to testing myself.&lt;/p&gt; &lt;p&gt;Kudos Hunyuan!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n01jwzn</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n01jwzn/"/><updated>2025-06-27T10:48:39+00:00</updated><title>/u/ResearchCrafty1804 on Hunyuan-A13B released</title></entry><entry><author><name>/u/kristaller486</name><uri>https://www.reddit.com/user/kristaller486</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The license allows commercial use of up to 100 million users per month and prohibits the use of the model in the UK, EU and South Korea.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n00vz9o</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n00vz9o/"/><updated>2025-06-27T07:02:28+00:00</updated><title>/u/kristaller486 on Hunyuan-A13B released</title></entry><entry><author><name>/u/Wonderful_Second5322</name><uri>https://www.reddit.com/user/Wonderful_Second5322</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;GGUFs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n00wgyx</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n00wgyx/"/><updated>2025-06-27T07:07:07+00:00</updated><title>/u/Wonderful_Second5322 on Hunyuan-A13B released</title></entry><entry><author><name>/u/Classic_Pair2011</name><uri>https://www.reddit.com/user/Classic_Pair2011</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Who will provide this model on Openrouter? I hope somebody pick it up&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n0114aw</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n0114aw/"/><updated>2025-06-27T07:52:11+00:00</updated><title>/u/Classic_Pair2011 on Hunyuan-A13B released</title></entry><entry><author><name>/u/ResidentPositive4122</name><uri>https://www.reddit.com/user/ResidentPositive4122</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Interesting, it&amp;#39;s a 80B_13A model, which gives ~32B dense equivalent. &lt;/p&gt; &lt;p&gt;Evals look amazing (beating qwen3-32B across the board, close to qwen3-A22B and even better on some). I guess we&amp;#39;ll have to wait for 3rd party evals to see if they match this in real-world scenarios. Interesting that this scores significantly higher on agentic benchmarks. &lt;/p&gt; &lt;p&gt;With only 13B_active it should be considerably faster to run, if you have the vram.&lt;/p&gt; &lt;p&gt;License sux tho, kinda like meta (&amp;lt;100M monthly users) but with added restrictions for EU. Oh well...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n00wocj</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n00wocj/"/><updated>2025-06-27T07:09:02+00:00</updated><title>/u/ResidentPositive4122 on Hunyuan-A13B released</title></entry><entry><author><name>/u/Dr_Me_123</name><uri>https://www.reddit.com/user/Dr_Me_123</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The online demo didn&amp;#39;t yield any surprising results. So perhaps just be an upgrade of Qwen3 30B with more VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n014h9j</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n014h9j/"/><updated>2025-06-27T08:25:26+00:00</updated><title>/u/Dr_Me_123 on Hunyuan-A13B released</title></entry><entry><author><name>/u/Capable-Ad-7494</name><uri>https://www.reddit.com/user/Capable-Ad-7494</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;does anybody remember the command to throw the important bits into vram again?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n00yrjn</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n00yrjn/"/><updated>2025-06-27T07:29:00+00:00</updated><title>/u/Capable-Ad-7494 on Hunyuan-A13B released</title></entry><entry><author><name>/u/Barry_22</name><uri>https://www.reddit.com/user/Barry_22</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Wow, great. How many languages it supports?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n012x5h</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n012x5h/"/><updated>2025-06-27T08:09:54+00:00</updated><title>/u/Barry_22 on Hunyuan-A13B released</title></entry><entry><author><name>/u/05032-MendicantBias</name><uri>https://www.reddit.com/user/05032-MendicantBias</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;It feels like this should work wonders with 64GB RAM + 24GB VRAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n01xrp8</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n01xrp8/"/><updated>2025-06-27T12:26:32+00:00</updated><title>/u/05032-MendicantBias on Hunyuan-A13B released</title></entry><entry><author><name>/u/jacek2023</name><uri>https://www.reddit.com/user/jacek2023</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Looks perfect!!! What a great time we are living now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n01152c</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n01152c/"/><updated>2025-06-27T07:52:24+00:00</updated><title>/u/jacek2023 on Hunyuan-A13B released</title></entry><entry><author><name>/u/ivari</name><uri>https://www.reddit.com/user/ivari</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;At 13B active experts, and Q4, that is around 8 gb vram and 48GB ram requirements right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n019x8y</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n019x8y/"/><updated>2025-06-27T09:19:04+00:00</updated><title>/u/ivari on Hunyuan-A13B released</title></entry><entry><author><name>/u/m98789</name><uri>https://www.reddit.com/user/m98789</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Fine tune how&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n0134zq</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n0134zq/"/><updated>2025-06-27T08:12:04+00:00</updated><title>/u/m98789 on Hunyuan-A13B released</title></entry><entry><author><name>/u/kyazoglu</name><uri>https://www.reddit.com/user/kyazoglu</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Looks promising.&lt;/p&gt; &lt;p&gt;I could not make it work with vLLM and gave up after 2 hours of battling with dependencies. I didn&amp;#39;t try the published docker image. Can someone who was able to run it share some important dependencies? versions of vllm, transformers, torch, flash-attn, cuda etc.?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n023h1q</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n023h1q/"/><updated>2025-06-27T13:00:50+00:00</updated><title>/u/kyazoglu on Hunyuan-A13B released</title></entry><entry><author><name>/u/martinerous</name><uri>https://www.reddit.com/user/martinerous</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Tried the demo for creative writing. Liked the style - no annoying slop, good story flow and details. Disappointed about intelligence - it often mixes up characters and actions even in a single sentence. Based on math and science eval results, I expected the total opposite - a stiff and smart model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n04e2jz</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n04e2jz/"/><updated>2025-06-27T19:41:55+00:00</updated><title>/u/martinerous on Hunyuan-A13B released</title></entry><entry><author><name>/u/xxPoLyGLoTxx</name><uri>https://www.reddit.com/user/xxPoLyGLoTxx</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Looks great! Quick someone make an mlx 8 bit version.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n02ygbw</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n02ygbw/"/><updated>2025-06-27T15:37:04+00:00</updated><title>/u/xxPoLyGLoTxx on Hunyuan-A13B released</title></entry><entry><author><name>/u/starshade16</name><uri>https://www.reddit.com/user/starshade16</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Wtf do we have to do to get these guys to include tools in their LLMs? Come on guys.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n04efgb</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n04efgb/"/><updated>2025-06-27T19:43:43+00:00</updated><title>/u/starshade16 on Hunyuan-A13B released</title></entry><entry><author><name>/u/Radiant_Hair_2739</name><uri>https://www.reddit.com/user/Radiant_Hair_2739</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can&amp;#39;t wait for llama.cpp or LM Studio!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n01il0u</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n01il0u/"/><updated>2025-06-27T10:37:47+00:00</updated><title>/u/Radiant_Hair_2739 on Hunyuan-A13B released</title></entry><entry><author><name>/u/MagicaItux</name><uri>https://www.reddit.com/user/MagicaItux</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Detected Pickle imports (4)&lt;/p&gt; &lt;p&gt;&amp;quot;torch._utils._rebuild_tensor_v2&amp;quot;, &amp;quot;torch.BFloat16Storage&amp;quot;, &amp;quot;torch.FloatStorage&amp;quot;, &amp;quot;collections.OrderedDict&amp;quot;&lt;/p&gt; &lt;p&gt;If you really want to run it with keeping that in mind, I&amp;#39;d just drop the uri of the .bin file in the right hyena hierarchy&lt;/p&gt; &lt;p&gt;Detected Pickle imports (4)&lt;/p&gt; &lt;p&gt;So could you explain this?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;torch._utils._rebuild_tensor_v2&amp;quot;, &amp;quot;torch.BFloat16Storage&amp;quot;, &amp;quot;torch.FloatStorage&amp;quot;, &amp;quot;collections.OrderedDict&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n02y8on</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n02y8on/"/><updated>2025-06-27T15:36:04+00:00</updated><title>/u/MagicaItux on Hunyuan-A13B released</title></entry><entry><author><name>/u/OmarBessa</name><uri>https://www.reddit.com/user/OmarBessa</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;someone please tag the gguf troopers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n03me69</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n03me69/"/><updated>2025-06-27T17:29:04+00:00</updated><title>/u/OmarBessa on Hunyuan-A13B released</title></entry><entry><author><name>/u/BumbleSlob</name><uri>https://www.reddit.com/user/BumbleSlob</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;64Gb or higher Unified Memory gang, rise up!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n03p0sj</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n03p0sj/"/><updated>2025-06-27T17:41:12+00:00</updated><title>/u/BumbleSlob on Hunyuan-A13B released</title></entry><entry><author><name>/u/Googulator</name><uri>https://www.reddit.com/user/Googulator</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;At first I read &amp;quot;Hunyadi-A13B&amp;quot;, and thought, a Hungarian LLM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n04n3g5</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n04n3g5/"/><updated>2025-06-27T20:27:18+00:00</updated><title>/u/Googulator on Hunyuan-A13B released</title></entry><entry><author><name>/u/iansltx_</name><uri>https://www.reddit.com/user/iansltx_</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;...and now to wait until it shows up in ollama-compatible q4. 64GB unified RAM here so this should perform nicely.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n025hvx</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n025hvx/"/><updated>2025-06-27T13:12:22+00:00</updated><title>/u/iansltx_ on Hunyuan-A13B released</title></entry><entry><author><name>/u/Expensive-Apricot-25</name><uri>https://www.reddit.com/user/Expensive-Apricot-25</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I dont have enough VRAM :&amp;#39;(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n02h487</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n02h487/"/><updated>2025-06-27T14:13:45+00:00</updated><title>/u/Expensive-Apricot-25 on Hunyuan-A13B released</title></entry><entry><author><name>/u/Mybrandnewaccount95</name><uri>https://www.reddit.com/user/Mybrandnewaccount95</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hopefully that 256k context is legit&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n04set9</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n04set9/"/><updated>2025-06-27T20:54:12+00:00</updated><title>/u/Mybrandnewaccount95 on Hunyuan-A13B released</title></entry><entry><author><name>/u/elij7</name><uri>https://www.reddit.com/user/elij7</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Iâ€™m new to the whole build your own LLM thing. Would this be a good starting point to build my own model? Better than Mixtral 8x7B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n02zm9p</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n02zm9p/"/><updated>2025-06-27T15:42:31+00:00</updated><title>/u/elij7 on Hunyuan-A13B released</title></entry><entry><author><name>/u/rdmkyran</name><uri>https://www.reddit.com/user/rdmkyran</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Jjjjjjjjjjjjjjjjjjjjjjjk.jjjjkjjjjjjjjjjj jjjjjjjjjjjjjjj jjjjjjjj njj jjjjjjnjjjjjjjjjjjjjjjjjjn&amp;#39;&amp;#39;&amp;#39;&amp;#39;&amp;#39;k j j j kkkjnknk nj nnj. j nn n. Nnknkk knk nk k j n k. K k k n k j knnn k n kn n kn. n n nnnnn un k&amp;#39;&amp;#39;&amp;#39;kkkkkkkk&amp;#39;&amp;#39;kkkkkkk k nk kk kk&amp;#39;&amp;#39;&amp;#39;&amp;#39;&amp;#39;kkkk.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n03r1q2</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n03r1q2/"/><updated>2025-06-27T17:50:27+00:00</updated><title>/u/rdmkyran on Hunyuan-A13B released</title></entry><entry><author><name>/u/Alkaided</name><uri>https://www.reddit.com/user/Alkaided</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The first paragraph has a very very strong smell of Chineseâ€¦&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n00zx26</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n00zx26/"/><updated>2025-06-27T07:40:17+00:00</updated><title>/u/Alkaided on Hunyuan-A13B released</title></entry><entry><author><name>/u/lochyw</name><uri>https://www.reddit.com/user/lochyw</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;256k is not ultra long..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n011t4r</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/n011t4r/"/><updated>2025-06-27T07:59:00+00:00</updated><title>/u/lochyw on Hunyuan-A13B released</title></entry></feed>