<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-04-21T06:26:21+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max : LocalLLaMA</title><entry><author><name>/u/IonizedRay</name><uri>https://www.reddit.com/user/IonizedRay</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;When running the llama.cpp WebUI with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server -m Gemma-3-27B-Instruct-Q6_K.gguf \ --seed 42 \ --mlock \ --n-gpu-layers -1 \ --ctx-size 8096 \ --port 10000 \ --temp 1.0 \ --top-k 64 \ --top-p 0.95 \ --min-p 0.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And running Ollama trough OpenWebUI using the same temp, top-p, top-k, min-p, i get incredibly worse quality.&lt;/p&gt; &lt;p&gt;For example when i ask to add a feature to a python script, llama.cpp correctly adds the piece of code needed without any unnecessary edit, while Ollama completely rewrites the script, making a lot of stupid syntax mistakes that are so bad that the linter catches tons of them even before running it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/IonizedRay&quot;&gt; /u/IonizedRay &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1k0r9pi</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/" /><updated>2025-04-16T18:12:44+00:00</updated><published>2025-04-16T18:12:44+00:00</published><title>Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max</title></entry><entry><author><name>/u/GortKlaatu_</name><uri>https://www.reddit.com/user/GortKlaatu_</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What if you ran: &lt;code&gt;launchctl setenv OLLAMA_CONTEXT_LENGTH &amp;quot;8192&amp;quot;&lt;/code&gt; then restart ollama?&lt;/p&gt; &lt;p&gt;In the logs, you might find that ollama is ignoring what you set for the Open WebUI context window if it&amp;#39;s larger than 2048 and you haven&amp;#39;t manually adjusted the model file.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnga277</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/mnga277/"/><updated>2025-04-16T18:25:12+00:00</updated><title>/u/GortKlaatu_ on Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max</title></entry><entry><author><name>/u/mtomas7</name><uri>https://www.reddit.com/user/mtomas7</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Just interesting, why do you use seed unless you are aiming at a specific answer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mngb0iu</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/mngb0iu/"/><updated>2025-04-16T18:29:59+00:00</updated><title>/u/mtomas7 on Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max</title></entry><entry><author><name>/u/grubnenah</name><uri>https://www.reddit.com/user/grubnenah</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Are you also using Q6 on ollama? AFIK ollama almost always defaults to Q4.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnhmk96</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/mnhmk96/"/><updated>2025-04-16T22:36:01+00:00</updated><title>/u/grubnenah on Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max</title></entry><entry><author><name>/u/Sidran</name><uri>https://www.reddit.com/user/Sidran</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Some people reported QWQ having drastically better output when properly sequencing samplers (sequence reported to work the best: &amp;quot;top_k;dry;min_p;temperature;typ_p;xtc&amp;quot; )&lt;br/&gt; I am suspecting sampler sequence is the culprit. But I know very little about it. Maybe Llama.cpp and Ollama use different sequences by default, resulting in inferior output of Ollama.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnmd40w</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/mnmd40w/"/><updated>2025-04-17T17:39:05+00:00</updated><title>/u/Sidran on Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max</title></entry><entry><author><name>/u/iwinux</name><uri>https://www.reddit.com/user/iwinux</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Gonna ignore any threads that mention &amp;quot;Ollama&amp;quot; :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnie6qj</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/mnie6qj/"/><updated>2025-04-17T01:19:07+00:00</updated><title>/u/iwinux on Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max</title></entry></feed>