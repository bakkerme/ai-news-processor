<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-05-05T05:31:54+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster! : LocalLLaMA</title><entry><author><name>/u/VoidAlchemy</name><uri>https://www.reddit.com/user/VoidAlchemy</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/&quot;&gt; &lt;img src=&quot;https://a.thumbs.redditmedia.com/bM9LC8PSLdBmtmFQOjBwlZthNsiYL5J4IXaOzEPqwY4.jpg&quot; alt=&quot;LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!&quot; title=&quot;LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/3bwwfd4epsye1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=adbb0bce2c13bc560499b0d3459329d16d0a3291&quot;&gt;You can&amp;#39;t go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/m4x5z2sposye1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=26b2ff50d960dd957e86feb04a8c21030ef0195c&quot;&gt;mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert)&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;tl;dr;&lt;/h1&gt; &lt;p&gt;I highly recommend doing a &lt;code&gt;git pull&lt;/code&gt; and re-building your &lt;code&gt;ik_llama.cpp&lt;/code&gt; or &lt;code&gt;llama.cpp&lt;/code&gt; repo to take advantage of recent major performance improvements just released.&lt;/p&gt; &lt;p&gt;The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving &lt;code&gt;r/LocalLLaMA&lt;/code&gt; community!&lt;/p&gt; &lt;p&gt;If you have enough VRAM to fully offload and already have an existing &amp;quot;normal&amp;quot; quant of Qwen3 MoE then you&amp;#39;ll get a little more speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork!&lt;/p&gt; &lt;h1&gt;Details&lt;/h1&gt; &lt;p&gt;I spent yesterday compiling and running benhmarks on the newest versions of both &lt;a href=&quot;https://github.com/ikawrakow/ik_llama.cpp&quot;&gt;ik_llama.cpp&lt;/a&gt; and mainline &lt;a href=&quot;https://github.com/ggml-org/llama.cpp&quot;&gt;llama.cpp&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For those that don&amp;#39;t know, ikawrakow was an early contributor to mainline llama.cpp working on important features that have since trickled down into ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my understanding) the &lt;code&gt;ik_llama.cpp&lt;/code&gt; fork was built and has a number of interesting features including SotA &lt;code&gt;iqN_k&lt;/code&gt; quantizations that pack in a lot of quality for the size while retaining good speed performance. (These new quants are &lt;em&gt;not&lt;/em&gt; available in ollma, lmstudio, koboldcpp, etc.)&lt;/p&gt; &lt;p&gt;A few recent PRs made by ikawrakow to &lt;code&gt;ik_llama.cpp&lt;/code&gt; and by JohannesGaessler to mainline have &lt;em&gt;boosted performance across the board&lt;/em&gt; and especially on CUDA with Flash Attention implementations for Grouped Query Attention (GQA) models and also Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B releases!&lt;/p&gt; &lt;h1&gt;References&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://github.com/ikawrakow/ik_llama.cpp/pull/370&quot;&gt;ikawrakow/ik_llama.cpp/pull/370&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/VoidAlchemy&quot;&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1keoint</id><media:thumbnail url="https://a.thumbs.redditmedia.com/bM9LC8PSLdBmtmFQOjBwlZthNsiYL5J4IXaOzEPqwY4.jpg" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/" /><updated>2025-05-04T16:55:10+00:00</updated><published>2025-05-04T16:55:10+00:00</published><title>LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title></entry><entry><author><name>/u/ortegaalfredo</name><uri>https://www.reddit.com/user/ortegaalfredo</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m currently running ik_llama.cpp with Qwen3-235B-A22 on a Xeon E5-2680v4, that&amp;#39;s a 10 year old CPU with 128GB ddr4 memory, and a single RTX3090.&lt;/p&gt; &lt;p&gt;I&amp;#39;m getting 7 tok/s generation, very usable if you don&amp;#39;t use reasoning.&lt;/p&gt; &lt;p&gt;BTW the server is multi-GPU but ik_llama.cpp just crash trying to use multiple-gpus, but I don&amp;#39;t think it would improve speed a lot, as the CPU is always the bottleneck.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqkzwsd</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/mqkzwsd/"/><updated>2025-05-04T19:07:08+00:00</updated><title>/u/ortegaalfredo on LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title></entry><entry><author><name>/u/jacek2023</name><uri>https://www.reddit.com/user/jacek2023</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Could you explain how to read your pictures?&lt;/p&gt; &lt;p&gt;I see orange plot below red plot, so ik_llama.cpp is slower than llama.cpp?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqkhkuo</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/mqkhkuo/"/><updated>2025-05-04T17:33:48+00:00</updated><title>/u/jacek2023 on LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title></entry><entry><author><name>/u/bullerwins</name><uri>https://www.reddit.com/user/bullerwins</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Can you post some of the commands you use for the benchmarks? I want to tinker to see what is best for my use case&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqkda5h</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/mqkda5h/"/><updated>2025-05-04T17:12:34+00:00</updated><title>/u/bullerwins on LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title></entry><entry><author><name>/u/smflx</name><uri>https://www.reddit.com/user/smflx</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Oh, just updated. My rig is busy for running deepseek &amp;amp; ik_llama (1 week jobs). I will update after that :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqkn1zn</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/mqkn1zn/"/><updated>2025-05-04T18:01:01+00:00</updated><title>/u/smflx on LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title></entry><entry><author><name>/u/No_Conversation9561</name><uri>https://www.reddit.com/user/No_Conversation9561</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Maybe GGUF will now give same speed as MLX on Mac devices&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqkzibc</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/mqkzibc/"/><updated>2025-05-04T19:05:00+00:00</updated><title>/u/No_Conversation9561 on LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title></entry><entry><author><name>/u/Linkpharm2</name><uri>https://www.reddit.com/user/Linkpharm2</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a 3090. Doesn&amp;#39;t this say it&amp;#39;s slower, not faster?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqkefin</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/mqkefin/"/><updated>2025-05-04T17:18:17+00:00</updated><title>/u/Linkpharm2 on LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title></entry><entry><author><name>/u/VoidAlchemy</name><uri>https://www.reddit.com/user/VoidAlchemy</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;&lt;a href=&quot;https://preview.redd.it/0zroyhg1qsye1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3e55128b1aac3f6d2ddfbd22597b9cd6d7dd02c&quot;&gt;https://preview.redd.it/0zroyhg1qsye1.png?width=3404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3e55128b1aac3f6d2ddfbd22597b9cd6d7dd02c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my limited testing you probably want to go with ik_llama.cpp for fully offloaded non-MoE models like the recent GLM-4 which is crazy efficient on kv-cache VRAM usage due to its GQA design.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqkajq5</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/mqkajq5/"/><updated>2025-05-04T16:59:03+00:00</updated><title>/u/VoidAlchemy on LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title></entry><entry><author><name>/u/enoughalready</name><uri>https://www.reddit.com/user/enoughalready</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I just pulled and rebuilt and I&amp;#39;m now actually going about 15 tps slower.&lt;/p&gt; &lt;p&gt;My previous build was from about a week ago, and I was getting an eval time of about 54 tps.&lt;br/&gt; Now I&amp;#39;m only getting 39 tokens per second, so pretty significant drop.&lt;/p&gt; &lt;p&gt;I just downloaded the latest unsloth model&lt;/p&gt; &lt;p&gt;I&amp;#39;m running on 2 3090s, using this command:&lt;/p&gt; &lt;p&gt;```&lt;br/&gt; .\bin\Release\llama-server.exe -m C:\shared-drive\llm_models\unsloth-2-Qwen3-30B-A3B-128K-Q8_0.gguf --host &lt;a href=&quot;http://0.0.0.0&quot;&gt;0.0.0.0&lt;/a&gt; --ctx-size 50000 --n-predict 10000 --jinja --tensor-split 14,14 --top_k 20 --min_p 0.0 --top_p 0.8 --flash-attn --n-gpu-layers 9999 --threads 24&lt;br/&gt; ```&lt;/p&gt; &lt;p&gt;Prompt: &amp;quot;tell me a 2 paragraph story&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqlpfb8</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/mqlpfb8/"/><updated>2025-05-04T21:21:04+00:00</updated><title>/u/enoughalready on LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title></entry><entry><author><name>/u/FrostyContribution35</name><uri>https://www.reddit.com/user/FrostyContribution35</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How close is llamacpp to vLLM and exllama now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqmckpc</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/mqmckpc/"/><updated>2025-05-04T23:32:38+00:00</updated><title>/u/FrostyContribution35 on LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title></entry><entry><author><name>/u/Zestyclose_Yak_3174</name><uri>https://www.reddit.com/user/Zestyclose_Yak_3174</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Seems like it is related to CUDA only, so I guess only for people with Nvidia cards and not folks on Apple Silicon and others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqm3b51</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/mqm3b51/"/><updated>2025-05-04T22:38:41+00:00</updated><title>/u/Zestyclose_Yak_3174 on LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!</title></entry></feed>