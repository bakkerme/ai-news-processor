<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-06-28T00:31:11+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/?depth=1" type="text/html" /><subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle><title>Arch-Router: The first (and fastest) LLM router that can align to your usage preferences. : LocalLLaMA</title><entry><author><name>/u/AdditionalWeb107</name><uri>https://www.reddit.com/user/AdditionalWeb107</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/&quot;&gt; &lt;img src=&quot;https://preview.redd.it/6zqw0rkhzi9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b84cb94dea055e799bbb2285e64e2b597538da36&quot; alt=&quot;Arch-Router: The first (and fastest) LLM router that can align to your usage preferences.&quot; title=&quot;Arch-Router: The first (and fastest) LLM router that can align to your usage preferences.&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Excited to share Arch-Router, our research and model for LLM routing. Routing to the right LLM is still an elusive problem, riddled with nuance and gotchas. For example:&lt;/p&gt; &lt;p&gt;‚ÄúEmbedding-based‚Äù (or simple intent-classifier) routers sound good on paper‚Äîlabel each prompt via embeddings as ‚Äúsupport,‚Äù ‚ÄúSQL,‚Äù ‚Äúmath,‚Äù then hand it to the matching model‚Äîbut real chats don‚Äôt stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that can‚Äôt keep up with multi-turn conversations or fast-moving product requirements.&lt;/p&gt; &lt;p&gt;&amp;quot;Performance-based&amp;quot; routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: ‚ÄúWill Legal accept this clause?‚Äù ‚ÄúDoes our support tone still feel right?‚Äù Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arch-Router skips both pitfalls by routing on&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;preferences you write in plain language.&lt;/em&gt;&lt;/strong&gt; Drop rules like ‚Äúcontract clauses ‚Üí GPT-4o‚Äù or ‚Äúquick travel tips ‚Üí Gemini-Flash,‚Äù and our 1.5B auto-regressive router model maps prompt along with the context to your routing policies‚Äîno retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tiny footprint&lt;/strong&gt; ‚Äì 1.5 B params ‚Üí runs on one modern GPU (or CPU while you play).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plug-n-play&lt;/strong&gt; ‚Äì points at any mix of LLM endpoints; adding models needs &lt;em&gt;zero&lt;/em&gt; retraining.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SOTA query-to-policy matching&lt;/strong&gt; ‚Äì beats bigger closed models on conversational datasets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost / latency smart&lt;/strong&gt; ‚Äì push heavy stuff to premium models, everyday queries to the fast ones.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Exclusively available in Arch (the AI-native proxy for agents): &lt;a href=&quot;https://github.com/katanemo/archgw&quot;&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;br/&gt; üîó Model + code: &lt;a href=&quot;https://huggingface.co/katanemo/Arch-Router-1.5B&quot;&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;br/&gt; üìÑ Paper / longer read: &lt;a href=&quot;https://arxiv.org/abs/2506.16655&quot;&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/AdditionalWeb107&quot;&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://i.redd.it/6zqw0rkhzi9f1.png&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1lm3jvm</id><media:thumbnail url="https://preview.redd.it/6zqw0rkhzi9f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b84cb94dea055e799bbb2285e64e2b597538da36" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/" /><updated>2025-06-27T20:00:37+00:00</updated><published>2025-06-27T20:00:37+00:00</published><title>Arch-Router: The first (and fastest) LLM router that can align to your usage preferences.</title></entry><entry><author><name>/u/DeepInEvil</name><uri>https://www.reddit.com/user/DeepInEvil</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;So this is a powerful intent classifier? How good/bad it understands the context of the underlying data/content wrt to the task?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n052uml</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/n052uml/"/><updated>2025-06-27T21:49:16+00:00</updated><title>/u/DeepInEvil on Arch-Router: The first (and fastest) LLM router that can align to your usage preferences.</title></entry><entry><author><name>/u/SomeOddCodeGuy</name><uri>https://www.reddit.com/user/SomeOddCodeGuy</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I take a little offense to the &amp;quot;first&amp;quot;, since this is exactly&lt;a href=&quot;https://github.com/SomeOddCodeGuy/WilmerAI&quot;&gt; what Wilmer does lol&lt;/a&gt;. Wilmer was ported to Github in May of 2024, two months before Arch kicked off in July; it&amp;#39;s not fair to those of us who have also done this to try to just write them out of history.&lt;/p&gt; &lt;p&gt;I don&amp;#39;t doubt that Arch is bigger or faster and better, and it&amp;#39;s a really cool project, but do be kind on the &amp;quot;first&amp;quot; claims.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n05b8gf</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/n05b8gf/"/><updated>2025-06-27T22:36:12+00:00</updated><title>/u/SomeOddCodeGuy on Arch-Router: The first (and fastest) LLM router that can align to your usage preferences.</title></entry></feed>