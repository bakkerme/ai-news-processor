<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-05-05T05:31:52+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows) : LocalLLaMA</title><entry><author><name>/u/intofuture</name><uri>https://www.reddit.com/user/intofuture</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/JTz5zGA4pu0kVgU5PcVKCkISLorm6EJbAz9pvaqpZXQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b29e8ff81b9a05957e855ea181b9201334e650a3&quot; alt=&quot;Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)&quot; title=&quot;Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Hey LocalLlama!&lt;/p&gt; &lt;p&gt;We&amp;#39;ve started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon.&lt;/p&gt; &lt;p&gt;We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device).&lt;/p&gt; &lt;p&gt;Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist.&lt;/p&gt; &lt;p&gt;We figured we&amp;#39;d kickstart this since we already maintain this benchmarking infra/tooling at RunLocal for our enterprise customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and TFLite to date, so a few things are still WIP for GGUF support. &lt;/p&gt; &lt;p&gt;Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill Tokens=512, Num Generation Tokens=128). &lt;a href=&quot;https://huggingface.co/collections/unsloth/qwen3-680edabfb790c8c34a242f95&quot;&gt;GGUFs are from Unsloth&lt;/a&gt; üêê&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/l59qu1gxysye1.png?width=961&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=381abad7b25e1d719265826441b51aa50177d143&quot;&gt;Qwen3 GGUF benchmarks on laptops&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/z5qxhpc1zsye1.png?width=913&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c48aa6c5c753c7dc74c4397aac34f92383d17afe&quot;&gt;Qwen3 GGUF benchmarks on phones&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can see more of the benchmark data for Qwen3 &lt;a href=&quot;https://edgemeter.runlocal.ai/public/pipelines/a240f768-2847-4e06-8df9-156ea3c2c321&quot;&gt;here&lt;/a&gt;. We realize there are so many variables (devices, backends, etc.) that interpreting the data is currently harder than it should be. We&amp;#39;ll work on that!&lt;/p&gt; &lt;p&gt;You can also see benchmarks for a few other models &lt;a href=&quot;https://edgemeter.runlocal.ai/public/pipelines&quot;&gt;here&lt;/a&gt;. If you want to see benchmarks for any others, feel free to request them and we‚Äôll try to publish ASAP!&lt;/p&gt; &lt;p&gt;Lastly, you can run your own benchmarks on our devices for free (limited to some degree to avoid our devices melting!).&lt;/p&gt; &lt;p&gt;This free/public version is a bit of a frankenstein fork of our enterprise product, so any benchmarks you run would be private to your account. But if there&amp;#39;s interest, we can add a way for you to also publish them so that the public benchmarks aren‚Äôt bottlenecked by us. &lt;/p&gt; &lt;p&gt;It‚Äôs still very early days for us with this, so please let us know what would make it better/cooler for the community: &lt;a href=&quot;https://edgemeter.runlocal.ai/public/pipelines&quot;&gt;https://edgemeter.runlocal.ai/public/pipelines&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To more on-device AI in production! üí™&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://i.redd.it/aev5rgjazsye1.gif&quot;&gt;https://i.redd.it/aev5rgjazsye1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/intofuture&quot;&gt; /u/intofuture &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1kepuli</id><media:thumbnail url="https://external-preview.redd.it/JTz5zGA4pu0kVgU5PcVKCkISLorm6EJbAz9pvaqpZXQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b29e8ff81b9a05957e855ea181b9201334e650a3" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/" /><updated>2025-05-04T17:51:10+00:00</updated><published>2025-05-04T17:51:10+00:00</published><title>Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)</title></entry><entry><author><name>/u/swagonflyyyy</name><uri>https://www.reddit.com/user/swagonflyyyy</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Iphone 16&amp;#39;s Metal performance is pretty impressive for 1.6b-q8.&lt;/p&gt; &lt;p&gt;But I do wonder why q8&amp;#39;s performance is faster than q4 in that particular setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqkunpq</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/mqkunpq/"/><updated>2025-05-04T18:39:49+00:00</updated><title>/u/swagonflyyyy on Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)</title></entry><entry><author><name>/u/AOHKH</name><uri>https://www.reddit.com/user/AOHKH</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;It‚Äôs interesting to see that performance in m4 is pretty similar in both cpu and gpu&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqkw3v7</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/mqkw3v7/"/><updated>2025-05-04T18:47:16+00:00</updated><title>/u/AOHKH on Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)</title></entry><entry><author><name>/u/AXYZE8</name><uri>https://www.reddit.com/user/AXYZE8</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;There&amp;#39;s one edge factor you missed - on Metal backend when you get OOM you get completely wrong results.&lt;/p&gt; &lt;p&gt;For example on Qwen3 8B Q4 your results are like this:&lt;br/&gt; - MacBook Pro M1, 8GB = 99232.83tok/s prefill, 2133.70tok/s generation&lt;br/&gt; - MacBook Pro M3, 8GB = 90508.66tok/s prefill, 2507.50tok/s generation&lt;/p&gt; &lt;p&gt;If you wouldn&amp;#39;t get OOM the correct results for that model should be around ~100-150tok/s prefill and ~10tok/s generation.&lt;/p&gt; &lt;p&gt;Additionally, all results for RAM usage on Apple silicon &amp;amp; Metal are not correct.&lt;/p&gt; &lt;p&gt;In terms of your UX/UI there&amp;#39;s tons of stuff that should be improved. but to not make this into very long post I&amp;#39;ll write about biggest problems that can be fixed rather easily.&lt;/p&gt; &lt;p&gt;First, add option to hide columns, there&amp;#39;s too much redundant information that should be possible to hide with just couple of clicks.&lt;/p&gt; &lt;p&gt;Second, decide on some naming scheme for components and stick with it.&lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/8idjxghteuye1.png?width=91&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12d1d9317495b745768b9dd012288be1eb804964&quot;&gt;https://preview.redd.it/8idjxghteuye1.png?width=91&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=12d1d9317495b745768b9dd012288be1eb804964&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would suggest to get rid of &amp;#39;Apple&amp;#39;/&amp;#39;Bionic&amp;#39; names altogether - it just adds to complexity and cognitive load to a table that is already very dense. There is no non-Apple M1 in Macbooks or non-Bionic A12 in iPad, so you don&amp;#39;t need to clarify that much in a first place and additionally this page is aimed at technical people. Exact same problem with Samsung/Google vs Snapdragon.&lt;/p&gt; &lt;p&gt;Third, if both CPU and Metal failed don&amp;#39;t create two entries. Table is 2x longer than it should be with results that are non-comparable to anything. Just combine it into one entry.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqm6fc7</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/mqm6fc7/"/><updated>2025-05-04T22:56:58+00:00</updated><title>/u/AXYZE8 on Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)</title></entry><entry><author><name>/u/Tonylu99</name><uri>https://www.reddit.com/user/Tonylu99</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How to run on metal on iphone 16 pro? I have pocketpal app and how to switch from cpu to metal?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqky4k2</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/mqky4k2/"/><updated>2025-05-04T18:57:44+00:00</updated><title>/u/Tonylu99 on Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)</title></entry><entry><author><name>/u/renaissancelife</name><uri>https://www.reddit.com/user/renaissancelife</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;if i&amp;#39;m reading this correctly the load time on cpu is better than gpu/metal for macbook pro but the gpu/metal is less memory intensive?&lt;/p&gt; &lt;p&gt;also metal perf on iphone 16 is pretty impressive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mql6vr2</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/mql6vr2/"/><updated>2025-05-04T19:44:10+00:00</updated><title>/u/renaissancelife on Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)</title></entry><entry><author><name>/u/stunbots</name><uri>https://www.reddit.com/user/stunbots</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How do I run this on Android? Rn it just crashes&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqlkaf5</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/mqlkaf5/"/><updated>2025-05-04T20:54:01+00:00</updated><title>/u/stunbots on Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)</title></entry><entry><author><name>/u/Expensive-Apricot-25</name><uri>https://www.reddit.com/user/Expensive-Apricot-25</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Why is Q8 faster than Q4???&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqlnqtx</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/mqlnqtx/"/><updated>2025-05-04T21:12:07+00:00</updated><title>/u/Expensive-Apricot-25 on Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)</title></entry><entry><author><name>/u/T2WIN</name><uri>https://www.reddit.com/user/T2WIN</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;For laptops, is vulkan using the igpu ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqlr2oq</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/mqlr2oq/"/><updated>2025-05-04T21:29:49+00:00</updated><title>/u/T2WIN on Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)</title></entry><entry><author><name>/u/jacek2023</name><uri>https://www.reddit.com/user/jacek2023</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;according to this data on iphone 16 you have 24 t/s on Q8 and 22 t/s on Q4&lt;/p&gt; &lt;p&gt;why so tiny models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqkneal</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/mqkneal/"/><updated>2025-05-04T18:02:45+00:00</updated><title>/u/jacek2023 on Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)</title></entry><entry><author><name>/u/KageYume</name><uri>https://www.reddit.com/user/KageYume</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The iPhone 16e is listed to have the A18 Pro SoC but it actually has the A18. &lt;/p&gt; &lt;p&gt;&lt;a href=&quot;https://preview.redd.it/h1dx2hgphvye1.png?width=623&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2f8794bf27c6a9042b81040ebefa09873eae989&quot;&gt;https://preview.redd.it/h1dx2hgphvye1.png?width=623&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2f8794bf27c6a9042b81040ebefa09873eae989&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mqn3sdy</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/mqn3sdy/"/><updated>2025-05-05T02:16:57+00:00</updated><title>/u/KageYume on Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)</title></entry></feed>