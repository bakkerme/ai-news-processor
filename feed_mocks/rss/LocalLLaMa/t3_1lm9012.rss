<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-06-28T00:31:23+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/?depth=1" type="text/html" /><subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle><title>I keep returning to Llama-3.1-8B : LocalLLaMA</title><entry><author><name>/u/entsnack</name><uri>https://www.reddit.com/user/entsnack</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I am working on porting a GPT-4.1 project over to an open-source model to deal with a GDPR-compliant client. The task is basically fine-tuning the model to classify text in a western European language.&lt;/p&gt; &lt;p&gt;I tried Qwen3 (0.6B, 1.7B, 8B) without making much progress (the fine-tuned model is far behind GPT-4.1) and finally went back to Llama-3.1-8B, which was what worked for me over a year ago. This is super surprising to me, because Qwen3&amp;#39;s zero-shot performance in English is almost 2x that of Llama&amp;#39;s for similar model sizes.&lt;/p&gt; &lt;p&gt;Does anyone else run fine-tuning heavy workloads in European languages? What&amp;#39;s the best model for this workload that I can fine-tune on an H100 96GB (note: I don&amp;#39;t do PEFT)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/entsnack&quot;&gt; /u/entsnack &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1lm9012</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/" /><updated>2025-06-27T23:58:14+00:00</updated><published>2025-06-27T23:58:14+00:00</published><title>I keep returning to Llama-3.1-8B</title></entry><entry><author><name>/u/jacek2023</name><uri>https://www.reddit.com/user/jacek2023</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;look at Bielik&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n05qchz</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/n05qchz/"/><updated>2025-06-28T00:03:24+00:00</updated><title>/u/jacek2023 on I keep returning to Llama-3.1-8B</title></entry></feed>