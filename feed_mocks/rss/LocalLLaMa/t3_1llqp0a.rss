<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-06-28T00:31:12+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/?depth=1" type="text/html" /><subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle><title>The more LLMs think, the worse they translate : LocalLLaMA</title><entry><author><name>/u/Nuenki</name><uri>https://www.reddit.com/user/Nuenki</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/&quot;&gt; &lt;img src=&quot;https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c116c7e6295d776b6382e425434256d0d8559943&quot; alt=&quot;The more LLMs think, the worse they translate&quot; title=&quot;The more LLMs think, the worse they translate&quot; /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/Nuenki&quot;&gt; /u/Nuenki &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://nuenki.app/blog/the_more_llms_think_the_worse_they_translate&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content><id>t3_1llqp0a</id><media:thumbnail url="https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c116c7e6295d776b6382e425434256d0d8559943" /><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/" /><updated>2025-06-27T10:41:40+00:00</updated><published>2025-06-27T10:41:40+00:00</published><title>The more LLMs think, the worse they translate</title></entry><entry><author><name>/u/stddealer</name><uri>https://www.reddit.com/user/stddealer</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wonder if the results would be the same for a model like R1 zero, which can mix languages in the chain of thought.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n01kepk</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n01kepk/"/><updated>2025-06-27T10:52:36+00:00</updated><title>/u/stddealer on The more LLMs think, the worse they translate</title></entry><entry><author><name>/u/datbackup</name><uri>https://www.reddit.com/user/datbackup</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Guess this explains why v3 0324 has become my goto for translating. Qwen3 with nothink is good too though&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n01o2gj</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n01o2gj/"/><updated>2025-06-27T11:20:37+00:00</updated><title>/u/datbackup on The more LLMs think, the worse they translate</title></entry><entry><author><name>/u/bones10145</name><uri>https://www.reddit.com/user/bones10145</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Kinda like people. Ever overthink something and make it worse? Lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n021gbw</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n021gbw/"/><updated>2025-06-27T12:49:04+00:00</updated><title>/u/bones10145 on The more LLMs think, the worse they translate</title></entry><entry><author><name>/u/FullOf_Bad_Ideas</name><uri>https://www.reddit.com/user/FullOf_Bad_Ideas</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Read this if you haven&amp;#39;t - &lt;a href=&quot;https://arxiv.org/abs/2410.21333&quot;&gt;https://arxiv.org/abs/2410.21333&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It looks like you&amp;#39;re also mostly testing non-reasoning models and asking them to reason, that&amp;#39;s substantially different than using models specifically trained to reason before answering.&lt;/p&gt; &lt;p&gt;I think translation should actually benefit form the pre-response reasoning chain, given that it would allow for self-critique to happen.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n02d6kj</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n02d6kj/"/><updated>2025-06-27T13:53:48+00:00</updated><title>/u/FullOf_Bad_Ideas on The more LLMs think, the worse they translate</title></entry><entry><author><name>/u/s101c</name><uri>https://www.reddit.com/user/s101c</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Not true in my tests. Gemini 2.5 Experimental has provided the most correct, contextually-aware translation.&lt;/p&gt; &lt;p&gt;The original text was a one-page document from our contractor with specific terminology which translates differently if mentioned in a general conversation.&lt;/p&gt; &lt;p&gt;Claude, R1, Mistral Le Chat, GPT-4o all failed and provided vague or incorrect bits in the translation. Gemini 2.5 succeeded &lt;strong&gt;because&lt;/strong&gt; it was thinking, it was selecting the contextually correct translation inside the thinking process, word-by-word.&lt;/p&gt; &lt;p&gt;The only downside is that Gemini 2.5 was not able to translate long texts, this worked only with texts the size of a long e-mail.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n01yi8j</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n01yi8j/"/><updated>2025-06-27T12:31:10+00:00</updated><title>/u/s101c on The more LLMs think, the worse they translate</title></entry><entry><author><name>/u/davidgutierrezpalma</name><uri>https://www.reddit.com/user/davidgutierrezpalma</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I&amp;#39;m not sure if I&amp;#39;m understanding it correctly and I haven&amp;#39;t looked at the source code at the repository yet, but...&lt;/p&gt; &lt;p&gt;Does this article mean &amp;quot;a translation generated from the combined outputs of several non-thinking models&amp;quot; is better than the translation generated by a single model... but if you can only use a single model, it&amp;#39;s better to use a non-thinking model than a thinking model?&lt;/p&gt; &lt;p&gt;Can anybody confirm if I have understood it correctly?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n01vlgz</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n01vlgz/"/><updated>2025-06-27T12:12:45+00:00</updated><title>/u/davidgutierrezpalma on The more LLMs think, the worse they translate</title></entry><entry><author><name>/u/Quagmirable</name><uri>https://www.reddit.com/user/Quagmirable</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Interesting, that&amp;#39;s exactly what I observed in these two recent posts as well:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lkls2v/has_anybody_else_found_deepseek_r1_0528_qwen3_8b/&quot;&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lkls2v/has_anybody_else_found_deepseek_r1_0528_qwen3_8b/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1lkls2v/has_anybody_else_found_deepseek_r1_0528_qwen3_8b/mztntnu/&quot;&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lkls2v/has_anybody_else_found_deepseek_r1_0528_qwen3_8b/mztntnu/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n027sou</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n027sou/"/><updated>2025-06-27T13:25:07+00:00</updated><title>/u/Quagmirable on The more LLMs think, the worse they translate</title></entry><entry><author><name>/u/ahmetegesel</name><uri>https://www.reddit.com/user/ahmetegesel</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Not really sure, R1 and Qwen3 were better with reasoning in English-Finnish translation. Isnâ€™t it also depending on prompting, models own capabilities, training set etc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n025d0j</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n025d0j/"/><updated>2025-06-27T13:11:37+00:00</updated><title>/u/ahmetegesel on The more LLMs think, the worse they translate</title></entry><entry><author><name>/u/viag</name><uri>https://www.reddit.com/user/viag</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;It&amp;#39;s great to see people actually evaluating models! Maybe I read through your blog a bit too quickly, but I can&amp;#39;t seem to find which metric you used to evaluate the translation quality? Is it a LLM-as-a-judge ? (and the judge would be google/gemini-2.5-flash-preview ?) Or is it something like BLEU ?&lt;/p&gt; &lt;p&gt;It would be interesting to check with various metrics, because each one might bias the results a certain way..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n02k43x</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n02k43x/"/><updated>2025-06-27T14:28:22+00:00</updated><title>/u/viag on The more LLMs think, the worse they translate</title></entry><entry><author><name>/u/BidWestern1056</name><uri>https://www.reddit.com/user/BidWestern1056</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;we touch on this a bit in this paper: &lt;a href=&quot;https://arxiv.org/pdf/2506.10077&quot;&gt;https://arxiv.org/pdf/2506.10077&lt;/a&gt;&lt;/p&gt; &lt;p&gt;essentially any such natural language translation task is beleaguered by these fundamental limitations inherent to natural language itself. it is non-algorithmic, it cannot be &amp;quot;encoded&amp;quot; in a truly meaningful way with the current ways we are doing things, and it will always fail at these edge cases when complexity gets too high for it to manage all the potential dependencies.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n04vj04</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n04vj04/"/><updated>2025-06-27T21:10:11+00:00</updated><title>/u/BidWestern1056 on The more LLMs think, the worse they translate</title></entry><entry><author><name>/u/Possible-Moment-6313</name><uri>https://www.reddit.com/user/Possible-Moment-6313</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Right tool for the job.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n05o3gy</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n05o3gy/"/><updated>2025-06-27T23:50:02+00:00</updated><title>/u/Possible-Moment-6313 on The more LLMs think, the worse they translate</title></entry><entry><author><name>/u/Kooky-Somewhere-2883</name><uri>https://www.reddit.com/user/Kooky-Somewhere-2883</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;We have an overthinking section in Jan-nano technical report coming soon&lt;/p&gt; &lt;p&gt;Iâ€™m Alan author of Jan-nano&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_n0212rz</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n0212rz/"/><updated>2025-06-27T12:46:51+00:00</updated><title>/u/Kooky-Somewhere-2883 on The more LLMs think, the worse they translate</title></entry></feed>