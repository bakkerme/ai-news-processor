<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
  <category term="LocalLLaMA" label="r/LocalLLaMA"/>
  <updated>2025-04-21T04:24:44+00:00</updated>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <id>/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/.rss?depth=1</id>
  <link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/.rss?depth=1" type="application/atom+xml"/>
  <link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/?depth=1" type="text/html"/>
  <subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle>
  <title>Has anyone done a quant comparison for qwen2.5-coder:32b? : LocalLLaMA</title>
  <entry>
    <author>
      <name>/u/Brilliant-Sun2643</name>
      <uri>https://www.reddit.com/user/Brilliant-Sun2643</uri>
    </author>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <content type="html"><![CDATA[<!-- SC_OFF --><div class="md"><p>I&#39;m running on cpu so testing a dozen quants against each other won&#39;t be fast, would love to hear other&#39;s experiences</p> </div><!-- SC_ON --> &#32; submitted by &#32; <a href="https://www.reddit.com/user/Brilliant-Sun2643"> /u/Brilliant-Sun2643 </a> <br/> <span><a href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/">[link]</a></span> &#32; <span><a href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/">[comments]</a></span>]]></content>
    <id>t3_1gqztju</id>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/"/>
    <updated>2024-11-14T07:48:54+00:00</updated>
    <published>2024-11-14T07:48:54+00:00</published>
    <title>Has anyone done a quant comparison for qwen2.5-coder:32b?</title>
  </entry>
  <entry>
    <author>
      <name>/u/glowcialist</name>
      <uri>https://www.reddit.com/user/glowcialist</uri>
    </author>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <content type="html"><![CDATA[<!-- SC_OFF --><div class="md"><p>I ran aider benchmarks on a 4bpw exl2 with q6 cache and it slightly outperformed the official benchmarks, so I think 4ish is still pretty sensible to use.</p> </div><!-- SC_ON -->]]></content>
    <id>t1_lx26iq3</id>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/lx26iq3/"/>
    <updated>2024-11-14T08:54:09+00:00</updated>
    <title>/u/glowcialist on Has anyone done a quant comparison for qwen2.5-coder:32b?</title>
  </entry>
  <entry>
    <author>
      <name>/u/CheatCodesOfLife</name>
      <uri>https://www.reddit.com/user/CheatCodesOfLife</uri>
    </author>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <content type="html"><![CDATA[<!-- SC_OFF --><div class="md"><p><a href="https://qwen.readthedocs.io/en/latest/benchmark/quantization_benchmark.html">https://qwen.readthedocs.io/en/latest/benchmark/quantization_benchmark.html</a></p> <p>Qwen have done it themselves for some quants.</p> <p>Honestly, these are some of the most comprehensive docs for any open weights model.</p> </div><!-- SC_ON -->]]></content>
    <id>t1_lx3chgq</id>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/lx3chgq/"/>
    <updated>2024-11-14T14:43:11+00:00</updated>
    <title>/u/CheatCodesOfLife on Has anyone done a quant comparison for qwen2.5-coder:32b?</title>
  </entry>
  <entry>
    <author>
      <name>/u/Ok_Mine189</name>
      <uri>https://www.reddit.com/user/Ok_Mine189</uri>
    </author>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <content type="html"><![CDATA[<!-- SC_OFF --><div class="md"><p>If you can wait a day or two I can provide HumanEval benchmarks for EXL2 quants ranging from 2.5bpw to 8.0bpw with 0.5 intervals.</p> </div><!-- SC_ON -->]]></content>
    <id>t1_lx41ko0</id>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/lx41ko0/"/>
    <updated>2024-11-14T16:56:18+00:00</updated>
    <title>/u/Ok_Mine189 on Has anyone done a quant comparison for qwen2.5-coder:32b?</title>
  </entry>
  <entry>
    <author>
      <name>/u/Everlier</name>
      <uri>https://www.reddit.com/user/Everlier</uri>
    </author>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <content type="html"><![CDATA[<!-- SC_OFF --><div class="md"><p>To be honest, if that&#39;s a code completion (not code chat or agentic coding) use-case, even Qwen 2.5 1.5B does the trick surprisingly well (especially with repo-level template).</p> </div><!-- SC_ON -->]]></content>
    <id>t1_lx2clrl</id>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/lx2clrl/"/>
    <updated>2024-11-14T10:03:11+00:00</updated>
    <title>/u/Everlier on Has anyone done a quant comparison for qwen2.5-coder:32b?</title>
  </entry>
  <entry>
    <author>
      <name>/u/winkler1</name>
      <uri>https://www.reddit.com/user/winkler1</uri>
    </author>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <content type="html"><![CDATA[<!-- SC_OFF --><div class="md"><p>For some reason the mlx quants are returning empty strings, but GGUF just fine. I&#39;m generating code in an obscure language and 14B is doing fine even at Q4_0.</p> </div><!-- SC_ON -->]]></content>
    <id>t1_lx2vs8q</id>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/lx2vs8q/"/>
    <updated>2024-11-14T12:57:03+00:00</updated>
    <title>/u/winkler1 on Has anyone done a quant comparison for qwen2.5-coder:32b?</title>
  </entry>
  <entry>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://www.reddit.com/user/danielhanchen</uri>
    </author>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <content type="html"><![CDATA[<!-- SC_OFF --><div class="md"><p>I normally test models and quants by forcing it to complete the Fibonacci sequence ie prompt it by 1, 1, 2, 3, 5, 8, 13, 21, 34, and see how far the model remembers the sequence. Don&#39;t do 1, 1, 2, 3, 5, 8 -&gt; Llama 3.1 8b sometimes does 11 or 13.</p> <p>Another approach is simply just let it complete 1, 2, 3, 4, 5, 6, until the max context window. Both sequences should be in most training datasets via Wikipedia, maths sites etc, and it tests some basic understanding of maths.</p> <p>Interestingly although silly, 2bit quants on the small Qwen 2.5 variants don&#39;t do well. I would stick to 4bit or 8bit quants for smaller models, and larger variants 2-3-4bit should be OK.</p> <p>I upload 2, 3, 4, 5, 6, 8 bit quants here <a href="https://huggingface.co/collections/unsloth/qwen-25-coder-6732bc833ed65dd1964994d4">https://huggingface.co/collections/unsloth/qwen-25-coder-6732bc833ed65dd1964994d4</a> if that helps</p> </div><!-- SC_ON -->]]></content>
    <id>t1_lx24asb</id>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/lx24asb/"/>
    <updated>2024-11-14T08:28:36+00:00</updated>
    <title>/u/danielhanchen on Has anyone done a quant comparison for qwen2.5-coder:32b?</title>
  </entry>
  <entry>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <content type="html"><![CDATA[<!-- SC_OFF --><div class="md"><p>[deleted]</p> </div><!-- SC_ON -->]]></content>
    <id>t1_lx229pe</id>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/lx229pe/"/>
    <updated>2024-11-14T08:05:51+00:00</updated>
    <title>/u/[deleted] on Has anyone done a quant comparison for qwen2.5-coder:32b?</title>
  </entry>
  <entry>
    <author>
      <name>/u/a_beautiful_rhind</name>
      <uri>https://www.reddit.com/user/a_beautiful_rhind</uri>
    </author>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <content type="html"><![CDATA[<!-- SC_OFF --><div class="md"><p>I grabbed Q6 EXL. Won&#39;t help you on CPU, but the assumption is 8bit produces same outputs as BF16. That holds on touchy image models and it should here too.</p> </div><!-- SC_ON -->]]></content>
    <id>t1_lx4yzwx</id>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/lx4yzwx/"/>
    <updated>2024-11-14T19:45:53+00:00</updated>
    <title>/u/a_beautiful_rhind on Has anyone done a quant comparison for qwen2.5-coder:32b?</title>
  </entry>
  <entry>
    <author>
      <name>/u/rbgo404</name>
      <uri>https://www.reddit.com/user/rbgo404</uri>
    </author>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <content type="html"><![CDATA[<!-- SC_OFF --><div class="md"><p>I am running fp on A100. Here are some stats:</p> <p><a href="https://preview.redd.it/yhkqvmhaewoe1.png?width=1326&amp;format=png&amp;auto=webp&amp;s=535e27d85466949a2f45fba6006038d1f9e47682">https://preview.redd.it/yhkqvmhaewoe1.png?width=1326&amp;format=png&amp;auto=webp&amp;s=535e27d85466949a2f45fba6006038d1f9e47682</a></p> <p>You can check the blog here: <a href="https://docs.inferless.com/how-to-guides/deploy-Qwen2.5-Coder-32B-Instruct">https://docs.inferless.com/how-to-guides/deploy-Qwen2.5-Coder-32B-Instruct</a></p> </div><!-- SC_ON -->]]></content>
    <id>t1_mhyn4gy</id>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/mhyn4gy/"/>
    <updated>2025-03-15T18:38:13+00:00</updated>
    <title>/u/rbgo404 on Has anyone done a quant comparison for qwen2.5-coder:32b?</title>
  </entry>
  <entry>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <content type="html"><![CDATA[<!-- SC_OFF --><div class="md"><p>[deleted]</p> </div><!-- SC_ON -->]]></content>
    <id>t1_lx2190y</id>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/lx2190y/"/>
    <updated>2024-11-14T07:54:50+00:00</updated>
    <title>/u/[deleted] on Has anyone done a quant comparison for qwen2.5-coder:32b?</title>
  </entry>
  <entry>
    <author>
      <name>/u/jjboi8708</name>
      <uri>https://www.reddit.com/user/jjboi8708</uri>
    </author>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <content type="html"><![CDATA[<!-- SC_OFF --><div class="md"><p>Wym by this?? Why would you use CPU?</p> </div><!-- SC_ON -->]]></content>
    <id>t1_lx235h8</id>
    <link href="https://www.reddit.com/r/LocalLLaMA/comments/1gqztju/has_anyone_done_a_quant_comparison_for/lx235h8/"/>
    <updated>2024-11-14T08:15:36+00:00</updated>
    <title>/u/jjboi8708 on Has anyone done a quant comparison for qwen2.5-coder:32b?</title>
  </entry>
</feed>
