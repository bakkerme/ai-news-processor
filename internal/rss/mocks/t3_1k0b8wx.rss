<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-04-21T06:26:22+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>Yes, you could have 160gb of vram for just about $1000. : LocalLLaMA</title><entry><author><name>/u/segmond</name><uri>https://www.reddit.com/user/segmond</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Please see my original post that posted about this journey - &lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/&quot;&gt;https://www.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This will be up to par to readily beat DIGITs and the AMD MAX AI integrated 128gb systems.... &lt;/p&gt; &lt;p&gt;Sorry, I&amp;#39;m going to dump this before I get busy for anyone that might find it useful. So I bought 10 MI50 gpus for $90 each $900. Octominer case for $100. But I did pay $150 for the shipping and $6 tax for the case. So there you go $1156. I also bought a PCIe ethernet card for 99cents. $1157.&lt;/p&gt; &lt;p&gt;Octominer XULTRA 12 has 12 PCIe slots, it&amp;#39;s designed for mining, it has weak celeron CPU, the one I got has only 4gb of ram. But it works and is a great system for low budget GPU inference workload.&lt;/p&gt; &lt;p&gt;I took out the SSD drive and threw an old 250gb I had lying around and installed Ubuntu. Got the cards working, went with rocm. vulkan was surprising a bit problematic, and rocm was easy once I figured out. Blew up the system the first attempt and had to reinstall for anyone curious, I installed 24.04 ubuntu, MI50 is no longer supported on the latest roc 6.4.0, but you can install 6.3.0 so I did that. Built llama.cpp from source, and tried a few models. I&amp;#39;ll post data later.&lt;/p&gt; &lt;p&gt;Since the card has 12 slots, it has 1 8 pin for each slot, for a total of 12 cables. The cards have 2 8 pin each, so I had a choice, use an 8 pin to dual 8 pin cable or 2 to 1. To play it safe for starters, I did 2 to 1. For a total of 6 cards installed. The cards also supposedly have a peak of 300watts, so 10 cards would be 3000 watts. I have 3 power supplies of 750watts for a total of 2250watts. The cool thing about the power supply is that it&amp;#39;s hot swappable, I can plug in and take out while it&amp;#39;s running. You don&amp;#39;t need all 3 to run, only 1. The good news is that this thing doesn&amp;#39;t draw power! The cards are a bit high idle at about 20watts, so 6 cards 120watts, system idles really at &amp;lt; 130 watts. I&amp;#39;m measuring at the outlet with an electrical measurement meter. During inference across the cards, peak was about 340watt. I&amp;#39;m using llama.cpp so inference is serial and not parallel. You can see the load move from one card to the other. This as you can guess is &amp;quot;inefficient&amp;quot; so llama.cpp is not as far as say using vLLM with tensor parallel. But it does support multi users, so you can push it by running parallel requests if you are sharing the rig with others, running agents or custom code. In such a situation, you can have the cards all max out. I didn&amp;#39;t power limit the cards, system reports them at 250watts, I saw about 230watt max while inferring.&lt;/p&gt; &lt;p&gt;The case fan at 100% sounds like a jet engine, but the great thing is they are easy to control and at 10% you can&amp;#39;t hear it. The cards run cooler than my Nvidia cards that are on an open rig, my Nvidia cards idle at 30-40C, these cards idle in the 20C range with 5% fan. I can&amp;#39;t hear the fan until about 25% and it&amp;#39;s very quiet and blends in. It takes about 50-60% before anyone that walks into the room will notice.&lt;/p&gt; &lt;p&gt;I just cut and paste and took some rough notes, I don&amp;#39;t have any blogs or anything to sell, just sharing for those that might be interested. One of the cards seems to have issue. llama.cpp crashes when I try to use it both local and via RPC. I&amp;#39;ll swap and move it around to see if it makes a difference. I have 2 other rigs, llama.cpp won&amp;#39;t let me infer across more than 16 cards.&lt;/p&gt; &lt;p&gt;I&amp;#39;m spending time trying to figure it out, updated the *_MAX_DEVICES and MAX_BACKENDS, MAX_SERVERS in code from 16 to 32, it sometimes works. I did build with -DGGML_SCHED_MAX_BACKENDS=48 makes no difference. So if you have any idea, let me know. :)&lt;/p&gt; &lt;p&gt;Now on power and electricity. Save it, don&amp;#39;t care. With that said, the box idles at about 120watts, my other rigs probably idle more. Between the 3 rigs, maybe idle of 600watts. I have experimented with &amp;quot;wake on lan&amp;quot; That means I can suspend the machines and then wake them up remotely. One of my weekend plans is to put a daemon that will monitor the GPUs and system, if idle and nothing going on for 30 minutes. Hibernate the system, when I&amp;#39;m ready to use them wake them up remotely. Do this for all rig and don&amp;#39;t keep them running. I don&amp;#39;t know how loaded models will behave, my guess is that it would need to be reloaded, it&amp;#39;s &amp;quot;vram&amp;quot; aka &amp;quot;RAM&amp;quot; after all, and unlike system ram that gets saved to disk, GPU doesn&amp;#39;t. I&amp;#39;m still shocked at the low power use.&lt;/p&gt; &lt;p&gt;So on PCIe electrical x1 speed. I read it was 1GBps, but hey, there&amp;#39;s a difference from 1Gbps and that. So PCie3x1 is capable of 985 MB/s. My network cards are 1Gbps which are more around 125 MB/s. So upgrading to a 10Gbps network should theoretically allow for much faster load. 7x. In practice, I think it would be less. llama.cpp hackers are just programmers getting it done by any means necessary, the goal is to infer models not the best program, from my wandering around the rpc code today and observed behavior it&amp;#39;s not that performant. So if you&amp;#39;re into unix network programming and wanna contribute, that would be a great area. ;-)&lt;/p&gt; &lt;p&gt;With all this said, yes, for a just about $1000, 160gb of vram is sort of possible. There was a lot of MI50 on ebay and I suppose some other hawks saw them as well and took their chance so it&amp;#39;s sold out. Keep your eyes out for deals. I even heard I didn&amp;#39;t get the best deal, some lucky sonomabbb got the MI50&amp;#39;s that were 32gb. It might just be that companies might start replacing more of their old cards and we will see more of these or even better ones. Don&amp;#39;t be scared, don&amp;#39;t worry about that mess of you need a power plant and it&amp;#39;s no longer supported. Most of the things folks argued about on here are flat out wrong from my practical experience, so risk it all.&lt;/p&gt; &lt;p&gt;Oh yeah, largest model I did run was llama405b, and had it write code and was getting about 2tk/s. Yes it&amp;#39;s a large dense model. It would perform the worse, MoE like deepseekv3, llama4 are going to fly. I&amp;#39;ll get some numbers up on those if I remember to.&lt;/p&gt; &lt;p&gt;Future stuff.&lt;br/&gt; Decide if I&amp;#39;m going to pack all the GPUs in one server or another server. From the load observed today, one server will handle it fine. Unlike newer Nvidia GPUs with cable going in from the top, this one has the cables going in from the back and it&amp;#39;s quite a tight fit to get in. PCI standards from what I understand expect cards to pull a max of 75w and an 8pin cable can supply 150w, for a max of 225w. So I could power them with a single cable, figure out how to limit power to 200w and be good to go. As a matter of fact, some of the cables had those adapter and I took them out. I saw a video of a crypto bro running an Octominer with 3080s and those have more power demand than MI50s.&lt;/p&gt; &lt;p&gt;Here goes data from my notes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama3.1-8b-instruct-q8&lt;/strong&gt; inference, same prompt, same seed&lt;/p&gt; &lt;pre&gt;&lt;code&gt;MI50 local &amp;gt; llama_perf_sampler_print: sampling time = 141.03 ms / 543 runs ( 0.26 ms per token, 3850.22 tokens per second) llama_perf_context_print: load time = 164330.99 ms *** SSD through PCIe3x1 slot*** llama_perf_context_print: prompt eval time = 217.66 ms / 42 tokens ( 5.18 ms per token, 192.97 tokens per second) llama_perf_context_print: eval time = 12046.14 ms / 500 runs ( 24.09 ms per token, 41.51 tokens per second) llama_perf_context_print: total time = 18773.63 ms / 542 tokens 3090 local &amp;gt; llama_perf_context_print: load time = 3088.11 ms *** NVME through PCIex16 *** llama_perf_context_print: prompt eval time = 27.76 ms / 42 tokens ( 0.66 ms per token, 1512.91 tokens per second) llama_perf_context_print: eval time = 6472.99 ms / 510 runs ( 12.69 ms per token, 78.79 tokens per second) 3080ti local &amp;gt; llama_perf_context_print: prompt eval time = 41.82 ms / 42 tokens ( 1.00 ms per token, 1004.26 tokens per second) llama_perf_context_print: eval time = 5976.19 ms / 454 runs ( 13.16 ms per token, 75.97 tokens per second) 3060 local &amp;gt; llama_perf_sampler_print: sampling time = 392.98 ms / 483 runs ( 0.81 ms per token, 1229.09 tokens per second) llama_perf_context_print: eval time = 12351.84 ms / 440 runs ( 28.07 ms per token, 35.62 tokens per second) p40 local &amp;gt; llama_perf_context_print: prompt eval time = 95.65 ms / 42 tokens ( 2.28 ms per token, 439.12 tokens per second) llama_perf_context_print: eval time = 12083.73 ms / 376 runs ( 32.14 ms per token, 31.12 tokens per second) MI50B local *** different GPU from above, consistent *** llama_perf_context_print: prompt eval time = 229.34 ms / 42 tokens ( 5.46 ms per token, 183.14 tokens per second) llama_perf_context_print: eval time = 12186.78 ms / 500 runs ( 24.37 ms per token, 41.03 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you are paying attention MI50s are not great at prompt processing.&lt;/p&gt; &lt;p&gt;a little bit larger context, demonstrates that MI50 sucks at prompt processing... and demonstrating performance over RPC. I got these to see if I could use them via RPC for very huge models.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;p40 local llama_perf_context_print: prompt eval time = 512.56 ms / 416 tokens ( 1.23 ms per token, 811.61 tokens per second) llama_perf_context_print: eval time = 12582.57 ms / 370 runs ( 34.01 ms per token, 29.41 tokens per second) 3060 local llama_perf_context_print: prompt eval time = 307.63 ms / 416 tokens ( 0.74 ms per token, 1352.27 tokens per second) llama_perf_context_print: eval time = 10149.66 ms / 357 runs ( 28.43 ms per token, 35.17 tokens per second) 3080ti local llama_perf_context_print: prompt eval time = 141.43 ms / 416 tokens ( 0.34 ms per token, 2941.45 tokens per second) llama_perf_context_print: eval time = 6079.14 ms / 451 runs ( 13.48 ms per token, 74.19 tokens per second) 3090 local llama_perf_context_print: prompt eval time = 140.91 ms / 416 tokens ( 0.34 ms per token, 2952.30 tokens per second) llama_perf_context_print: eval time = 4170.36 ms / 314 runs ( 13.28 ms per token, 75.29 tokens per second MI50 local llama_perf_context_print: prompt eval time = 1391.44 ms / 416 tokens ( 3.34 ms per token, 298.97 tokens per second) llama_perf_context_print: eval time = 8497.04 ms / 340 runs ( 24.99 ms per token, 40.01 tokens per second) MI50 over RPC (1GPU) llama_perf_context_print: prompt eval time = 1177.23 ms / 416 tokens ( 2.83 ms per token, 353.37 tokens per second) llama_perf_context_print: eval time = 16800.55 ms / 340 runs ( 49.41 ms per token, 20.24 tokens per second) MI50 over RPC (2xGPU) llama_perf_context_print: prompt eval time = 1400.72 ms / 416 tokens ( 3.37 ms per token, 296.99 tokens per second) llama_perf_context_print: eval time = 17539.33 ms / 340 runs ( 51.59 ms per token, 19.39 tokens per second) MI50 over RPC (3xGPU) llama_perf_context_print: prompt eval time = 1562.64 ms / 416 tokens ( 3.76 ms per token, 266.22 tokens per second) llama_perf_context_print: eval time = 18325.72 ms / 340 runs ( 53.90 ms per token, 18.55 tokens per second) p40 over RPC (3xGPU) llama_perf_context_print: prompt eval time = 968.91 ms / 416 tokens ( 2.33 ms per token, 429.35 tokens per second) llama_perf_context_print: eval time = 22888.16 ms / 370 runs ( 61.86 ms per token, 16.17 tokens per second) MI50 over RPC (5xGPU) (1 token a second loss for every RPC?) llama_perf_context_print: prompt eval time = 1955.87 ms / 416 tokens ( 4.70 ms per token, 212.69 tokens per second) llama_perf_context_print: eval time = 22217.03 ms / 340 runs ( 65.34 ms per token, 15.30 tokens per second) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;max inference over RPC observed with rocm-smi was 100w, lower than when running locally, saw 240w&lt;/p&gt; &lt;p&gt;max watt observed at outlet before RPC was 361w, max watt after 361w&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama-70b-q8&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;if you want to approximate how fast it will run in q4, just multiple by 2. This was done with llama.cpp, yes vLLM is faster, someone already did q4 llama8 with vLLM and tensor parallel for 25tk/s&lt;/p&gt; &lt;pre&gt;&lt;code&gt;3090 5xGPU llama-70b llama_perf_context_print: prompt eval time = 785.20 ms / 416 tokens ( 1.89 ms per token, 529.80 tokens per second) llama_perf_context_print: eval time = 26483.01 ms / 281 runs ( 94.25 ms per token, 10.61 tokens per second) llama_perf_context_print: total time = 133787.93 ms / 756 tokens MI50 over RPC (5xGPU) llama-70b llama_perf_context_print: prompt eval time = 11841.23 ms / 416 tokens ( 28.46 ms per token, 35.13 tokens per second) llama_perf_context_print: eval time = 84088.80 ms / 415 runs ( 202.62 ms per token, 4.94 tokens per second) llama_perf_context_print: total time = 101548.44 ms / 831 tokens RPC across 17GPUs, 6 main 3090l and 11 remote GPUs (3090, 3080ti,3060, 3xP40, 5xMI50) true latency test llama_perf_context_print: prompt eval time = 8172.69 ms / 416 tokens ( 19.65 ms per token, 50.90 tokens per second) llama_perf_context_print: eval time = 74990.44 ms / 345 runs ( 217.36 ms per token, 4.60 tokens per second) llama_perf_context_print: total time = 556723.90 ms / 761 tokens Misc notes idle watt at outlet = 126watts temp about 25-27C across GPUs idle power across individual 21-26watts powercap - 250watts inference across 3GPUs at outlet - 262watts highest power on one GPU = 223W at 10% speed, fan got to 60C, at 20% speed highest is 53C while GPU is active. turned up to 100% it brought the GPUs down to high 20&amp;#39;s in under 2 minutes &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/segmond&quot;&gt; /u/segmond &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1k0b8wx</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/" /><updated>2025-04-16T03:46:47+00:00</updated><published>2025-04-16T03:46:47+00:00</published><title>Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/Psychological_Ear393</name><uri>https://www.reddit.com/user/Psychological_Ear393</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Just a quick note about this:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;MI50 is no longer supported on the latest roc 6.4.0&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;That&amp;#39;s a mistake in the docs, I installed it and it works on my MI50s. If you check the Radeon and Radeon pro tab gfx906 is still supported for the Radeon VII.&lt;/p&gt; &lt;p&gt;And also:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The cards also supposedly have a peak of 300watts, so 10 cards would be 3000 watts.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I found cutting power to 150 watts only took performance down 20%, if that&amp;#39;s a concern to keep usage lower. And even at 90 watts it&amp;#39;s still &amp;quot;reasonably good&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mncx0ih</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mncx0ih/"/><updated>2025-04-16T04:40:03+00:00</updated><title>/u/Psychological_Ear393 on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/ForsookComparison</name><uri>https://www.reddit.com/user/ForsookComparison</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;5 tokens/second on Q8 llama3.3 70B is really nice for $1k. Good job&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnctx9p</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnctx9p/"/><updated>2025-04-16T04:16:09+00:00</updated><title>/u/ForsookComparison on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/MatterMean5176</name><uri>https://www.reddit.com/user/MatterMean5176</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;That 50% performance drop for the MI50 in the middle chart makes me sad. I was tooling around with RPC today and experienced heavy drops also. I&amp;#39;m not being negative, just talking shop.&lt;/p&gt; &lt;p&gt;What are your thoughts on the numbers? Also the MoE numbers would be interesting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mncv9bm</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mncv9bm/"/><updated>2025-04-16T04:26:19+00:00</updated><title>/u/MatterMean5176 on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/the__storm</name><uri>https://www.reddit.com/user/the__storm</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Nice, I thought about something like this when MI25s were ~$50 but feared the driver/software nightmare. Kudos for getting it all working.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnczkah</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnczkah/"/><updated>2025-04-16T04:59:58+00:00</updated><title>/u/the__storm on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/JamieRRSS</name><uri>https://www.reddit.com/user/JamieRRSS</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Stupid question, why not get one rtx 4060 16go or any amd similar just so it can manage the prompt and let the rest of the mi50 take care of the interference? Does the prompt itself need 120gb vram? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnddp5f</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnddp5f/"/><updated>2025-04-16T07:14:21+00:00</updated><title>/u/JamieRRSS on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/heartprairie</name><uri>https://www.reddit.com/user/heartprairie</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Any updated photos?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mncr9ul</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mncr9ul/"/><updated>2025-04-16T03:56:16+00:00</updated><title>/u/heartprairie on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/Shouldhaveknown2015</name><uri>https://www.reddit.com/user/Shouldhaveknown2015</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I don&amp;#39;t see the point of using power hungry GPU&amp;#39;s when you are getting CPU interference speeds. &lt;/p&gt; &lt;p&gt;I get those speeds on my M1 Studio Max (5.7t/s on watermelon prompt on 70b for GGUF) or 8.1t/s for MX 70b.&lt;/p&gt; &lt;p&gt;At least this was the reason I purchased a discounted M1 Max Studio over spending 1000-1400 for a CPU interference setup or a Digits or similar, or something like your setup. &lt;/p&gt; &lt;p&gt;Add in the hassle of getting all those parts, some might be broken, diagnosing, etc. &lt;/p&gt; &lt;p&gt;It&amp;#39;s definitely a option.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnfon21</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnfon21/"/><updated>2025-04-16T16:42:48+00:00</updated><title>/u/Shouldhaveknown2015 on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/segmond</name><uri>https://www.reddit.com/user/segmond</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;prompt processing gets faster the larger your prompts. sharing so folks don&amp;#39;t get discouraged.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 10.76 ms / 1 tokens ( 10.76 ms per token, 92.90 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval time = 9239.62 ms / 857 tokens ( 10.78 ms per token, 92.75 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 2438.71 ms / 15 tokens ( 162.58 ms per token, 6.15 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval time = 13430.91 ms / 1042 tokens ( 12.89 ms per token, 77.58 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 520.00 ms / 1026 tokens ( 0.51 ms per token, 1973.08 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval time = 16342.38 ms / 1050 tokens ( 15.56 ms per token, 64.25 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 604.28 ms / 991 tokens ( 0.61 ms per token, 1639.95 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval time = 17087.18 ms / 908 tokens ( 18.82 ms per token, 53.14 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 39.08 ms / 14 tokens ( 2.79 ms per token, 358.21 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval time = 11857.57 ms / 595 tokens ( 19.93 ms per token, 50.18 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 976.72 ms / 1373 tokens ( 0.71 ms per token, 1405.73 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval time = 14532.16 ms / 612 tokens ( 23.75 ms per token, 42.11 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnek8e0</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnek8e0/"/><updated>2025-04-16T13:16:18+00:00</updated><title>/u/segmond on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/stc2828</name><uri>https://www.reddit.com/user/stc2828</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;But 5 token per second is just slightly faster than a good CPU RAM setup…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mndlwqp</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mndlwqp/"/><updated>2025-04-16T08:43:05+00:00</updated><title>/u/stc2828 on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/jxjq</name><uri>https://www.reddit.com/user/jxjq</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Bottom Line:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generation speed: ~4.9 tokens/sec&lt;/li&gt; &lt;li&gt;Time to first token small context: 12 seconds&lt;/li&gt; &lt;li&gt;Time to first token large context: 2 minutes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;on the $1,000 MI50 build using a 70b Q8 model&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnctzow</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnctzow/"/><updated>2025-04-16T04:16:41+00:00</updated><title>/u/jxjq on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/tengo_harambe</name><uri>https://www.reddit.com/user/tengo_harambe</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;bruh this post is so long it took 160gb of VRAM to render it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mncvaja</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mncvaja/"/><updated>2025-04-16T04:26:36+00:00</updated><title>/u/tengo_harambe on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/cantgetthistowork</name><uri>https://www.reddit.com/user/cantgetthistowork</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;How you get the octominers? They&amp;#39;re sold out&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnd1hhb</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnd1hhb/"/><updated>2025-04-16T05:16:13+00:00</updated><title>/u/cantgetthistowork on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/LevianMcBirdo</name><uri>https://www.reddit.com/user/LevianMcBirdo</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;MI50s are 200+ € where I live sadly. This could&amp;#39;ve been a cool project... Adding rising energy costs the framework PC still looks promising for a lot of interference usecases and just as a general work and sometimes gaming setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnd41f5</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnd41f5/"/><updated>2025-04-16T05:38:59+00:00</updated><title>/u/LevianMcBirdo on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/Huge-Promotion492</name><uri>https://www.reddit.com/user/Huge-Promotion492</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;cost going down,,,, i like&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnda4qx</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnda4qx/"/><updated>2025-04-16T06:37:50+00:00</updated><title>/u/Huge-Promotion492 on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/Different_Fix_2217</name><uri>https://www.reddit.com/user/Different_Fix_2217</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;At that speed / price just get a used server and run a moe like deepseek.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnddf4y</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnddf4y/"/><updated>2025-04-16T07:11:27+00:00</updated><title>/u/Different_Fix_2217 on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/AppearanceHeavy6724</name><uri>https://www.reddit.com/user/AppearanceHeavy6724</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Sadly prompt processing is awful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mndj7qv</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mndj7qv/"/><updated>2025-04-16T08:13:25+00:00</updated><title>/u/AppearanceHeavy6724 on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/KebabCompletChef</name><uri>https://www.reddit.com/user/KebabCompletChef</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wonder how it will works with Moe models like Llama 4&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mndmu0u</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mndmu0u/"/><updated>2025-04-16T08:53:03+00:00</updated><title>/u/KebabCompletChef on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/Not_your_guy_buddy42</name><uri>https://www.reddit.com/user/Not_your_guy_buddy42</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I upvoted as soon as I read you don&amp;#39;t have any blogs or anything to sell THANK YOU for that breath of fresh air&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mndnf5s</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mndnf5s/"/><updated>2025-04-16T08:59:22+00:00</updated><title>/u/Not_your_guy_buddy42 on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/loadsamuny</name><uri>https://www.reddit.com/user/loadsamuny</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Great post, my takeaway from the numbers is if its just inference its probably worth paying a bit more for a P40, but I guess everyone has a view on that price / speed / cuda balancing act&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mndp0fh</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mndp0fh/"/><updated>2025-04-16T09:16:43+00:00</updated><title>/u/loadsamuny on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/Flimsy_Monk1352</name><uri>https://www.reddit.com/user/Flimsy_Monk1352</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I never got Wake on Lan to run reliably, but most BIOSes allow to define what happens after a power outage (off, last state, on). I set that to on and put the machine behind a WiFi plug.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mndu4rp</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mndu4rp/"/><updated>2025-04-16T10:09:28+00:00</updated><title>/u/Flimsy_Monk1352 on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/klop2031</name><uri>https://www.reddit.com/user/klop2031</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I wanna see that deepseek speed. Cuz if its good...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnejjb9</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnejjb9/"/><updated>2025-04-16T13:12:09+00:00</updated><title>/u/klop2031 on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/MoffKalast</name><uri>https://www.reddit.com/user/MoffKalast</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Cheap, fast, power efficient.&lt;/p&gt; &lt;p&gt;Choose two.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnejzkx</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnejzkx/"/><updated>2025-04-16T13:14:50+00:00</updated><title>/u/MoffKalast on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/rorowhat</name><uri>https://www.reddit.com/user/rorowhat</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;All this and no photos?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnekaw1</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnekaw1/"/><updated>2025-04-16T13:16:42+00:00</updated><title>/u/rorowhat on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/jsllls</name><uri>https://www.reddit.com/user/jsllls</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;At that point why not just buy a used Mac?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnes5o3</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnes5o3/"/><updated>2025-04-16T14:00:44+00:00</updated><title>/u/jsllls on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/ohgoditsdoddy</name><uri>https://www.reddit.com/user/ohgoditsdoddy</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Is this possible for all autoregressive models provided they fit in the aggregate VRAM across all cards, or do they need to support this natively? Is it a feature specifically implemented by Llama.cpp? Would I also be able to use Ollama for it?&lt;/p&gt; &lt;p&gt;It occurs to me that this is possible for text generation because the models are autoregressive and run sequentially, right? Would I be able to use the total aggregate VRAM across these chips to run a diffusion model for image or video generation that does not necessarily support sharding?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnex3sl</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnex3sl/"/><updated>2025-04-16T14:26:25+00:00</updated><title>/u/ohgoditsdoddy on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/coding_workflow</name><uri>https://www.reddit.com/user/coding_workflow</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Then you get the Vram, but slow BW on PCI. Not sure how this is better than pure RAM then.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnex8hs</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnex8hs/"/><updated>2025-04-16T14:27:04+00:00</updated><title>/u/coding_workflow on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/zelkovamoon</name><uri>https://www.reddit.com/user/zelkovamoon</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;What&amp;#39;s the specific CPU you have? I ran into issues with my server build because mine doesn&amp;#39;t support avx2; so if yours does that might be a good path for me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnf67mg</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnf67mg/"/><updated>2025-04-16T15:11:35+00:00</updated><title>/u/zelkovamoon on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/Monkey_1505</name><uri>https://www.reddit.com/user/Monkey_1505</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt; 3000 watts&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnf9ueg</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnf9ueg/"/><updated>2025-04-16T15:29:35+00:00</updated><title>/u/Monkey_1505 on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/Thetitangaming</name><uri>https://www.reddit.com/user/Thetitangaming</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Sadly mi50s are 120-150 now, still a good deal&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnfqfae</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mnfqfae/"/><updated>2025-04-16T16:51:23+00:00</updated><title>/u/Thetitangaming on Yes, you could have 160gb of vram for just about $1000.</title></entry><entry><author><name>/u/__Maximum__</name><uri>https://www.reddit.com/user/__Maximum__</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;160gb is a lot, but still not enough for deepseek? Yes, it&amp;#39;s MoE, but q4 quants weight about 400gb, why do you think will it fly?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mndj8fn</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/mndj8fn/"/><updated>2025-04-16T08:13:38+00:00</updated><title>/u/__Maximum__ on Yes, you could have 160gb of vram for just about $1000.</title></entry></feed>