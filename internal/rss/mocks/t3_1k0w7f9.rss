<?xml version="1.0" encoding="UTF-8"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/"><category term="LocalLLaMA" label="r/LocalLLaMA"/><updated>2025-04-21T06:26:23+00:00</updated><icon>https://www.redditstatic.com/icon.png/</icon><id>/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/.rss?depth=1</id><link rel="self" href="https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/.rss?depth=1" type="application/atom+xml" /><link rel="alternate" href="https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/?depth=1" type="text/html" /><subtitle>Subreddit to discuss about Llama, the large language model created by Meta AI.</subtitle><title>What is the best option for running eight GPUs in a single motherboard? : LocalLLaMA</title><entry><author><name>/u/MLDataScientist</name><uri>https://www.reddit.com/user/MLDataScientist</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA"/><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;TLDR: Can I run 8 GPUs with two 1 to 4 PCIE splitter with bifurcation on my ASUS ROG CROSSHAIR VIII DARK HERO and AMD 5950x? or I need to purchase another motherboard?&lt;/p&gt; &lt;p&gt;----&lt;/p&gt; &lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I recently bought eight AMD MI50 32GB GPUs (total of 256 GB VRAM) for experimenting with 100B+ LLMs. However, I am not sure if my motherboard supports 8 GPUs. My motherboard is ASUS ROG CROSSHAIR VIII DARK HERO. It has three PCIE 4.0 x16 slots, one PCIE4.0 x1, and two M.2 PCIE4.0 x4 slots. The CPU is AMD 5950x which has 24 lanes on the CPU. I have 96GB of RAM.&lt;/p&gt; &lt;p&gt;Currently, both M.2 slots are occupied with NVME storage. I also installed three GPUs on all available three PCIE 4.0 x16 slots. Now, my motherboard BIOS shows each GPU is running at x8, x8 (Both MI50 cards) and x4 (RTX 3090).&lt;/p&gt; &lt;p&gt;My question is does this motherboard support 8 GPUs at once if I use PCIE splitter (e.g. 1 PCIE slot to 4 PCIE slots)? I see the user manual says the first PCIE 4.0 x16 slot supports PCIE bifurcation with x4+x4+x4+x4 for M.2 cards. But let&amp;#39;s say I install 1 to 4 PCIE splitter on the first and second slot both running at x8. Can I install eight GPUs and run each of them at PCIE4.0 x2 with bifurcation (not sure if I need to purchase some other part other than 1 to 4 splitter for this)?&lt;/p&gt; &lt;p&gt;If not, what is the alternative? I do not want to buy a server for $1000.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href=&quot;https://www.reddit.com/user/MLDataScientist&quot;&gt; /u/MLDataScientist &lt;/a&gt; &lt;br/&gt; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/&quot;&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href=&quot;https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/&quot;&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content><id>t3_1k0w7f9</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/" /><updated>2025-04-16T21:37:38+00:00</updated><published>2025-04-16T21:37:38+00:00</published><title>What is the best option for running eight GPUs in a single motherboard?</title></entry><entry><author><name>/u/FullstackSensei</name><uri>https://www.reddit.com/user/FullstackSensei</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;The short answer is: you need a new motherboard with an older HEDT CPU or better a server board and CPU.&lt;/p&gt; &lt;p&gt;You can&amp;#39;t &amp;quot;split&amp;quot; lanes beyond the bifurcation options in the BIOS, unless you find a splitter with an active PCIe switch. And even then, it&amp;#39;ll be a hassle and you&amp;#39;ll only be able to run models sequentially across cards, leading to very slow performance.&lt;/p&gt; &lt;p&gt;Your motherboard, as you said, has 24 lanes coming from the CPU, that second x16 slot is only mechanically X16. If you use both, the motherboard will switch both X16 slots to run at X8 each.&lt;/p&gt; &lt;p&gt;To have any chance at running those cards with tensor parallelism you&amp;#39;ll need a X4 link to each card, or better yet X8 per card to get decent performance. If you go the X4 route, you can get a X99 board and CPU pretty cheap. You&amp;#39;ll get 40 lanes from the CPU, and at least two real x16 slots. &lt;/p&gt; &lt;p&gt;A much better option IMO is the cheapest EPYC motherboard you can find. Epyc will give you 128 lanes even if you go for 7001 CPUs. Any server board will let you bifurcate the x16 slots into dual X8 slots, and then you can use those cheap X16 to dual X8 splitters to connect the cards. The board might be more expensive than an X99 board, but you&amp;#39;ll more than make up the difference with how cheap ECC DDR4 RAM is vs desktop RAM.&lt;/p&gt; &lt;p&gt;You might also have issues with BIOS not supporting that much VRAM addressing, which would require modding the BIOS to solve the issue.&lt;/p&gt; &lt;p&gt;Hope you have enough PSUs to feed all those cards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnhji0t</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/mnhji0t/"/><updated>2025-04-16T22:18:41+00:00</updated><title>/u/FullstackSensei on What is the best option for running eight GPUs in a single motherboard?</title></entry><entry><author><name>/u/csobrinho</name><uri>https://www.reddit.com/user/csobrinho</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;Just bought an ASRock ROMED8-2T + and Epyc 7J43 a month ago. With up to 2TB RAM, 128 PCIe 4.0 lanes and 7x PCIe x16 slots (that you could bifurcate or optionally use the two occulink and get two x4). Do notice that the PCIe slot 2 (IIRC) shares lanes with the occulink and m.2 but you can disable this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnjcnfq</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/mnjcnfq/"/><updated>2025-04-17T05:17:24+00:00</updated><title>/u/csobrinho on What is the best option for running eight GPUs in a single motherboard?</title></entry><entry><author><name>/u/zipperlein</name><uri>https://www.reddit.com/user/zipperlein</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I do not know if that would be technically possible. My guess would be that it depends on the layout of the bitfurication card, if u get 2x4 or 4x2 from the x16 slot running in x8 mode.&lt;br/&gt; This manual for a 4 nvme pcie extension card for example says it supports x8, so I think MBs could support that:&lt;br/&gt; &lt;a href=&quot;https://dlcdnets.asus.com/pub/ASUS/mb/Add-on_card/E14501_HYPER_M.2_X16_Card_V2_UM_PRINT.pdf?model=hyper%20m.2%20x16%20card%20v2&quot;&gt;https://dlcdnets.asus.com/pub/ASUS/mb/Add-on_card/E14501_HYPER_M.2_X16_Card_V2_UM_PRINT.pdf?model=hyper%20m.2%20x16%20card%20v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Your best bet is probabbly just trying it out. Consider using a supplier with a return policy. If it does not work, u could just return your parts. Otherwise like the other comment says, it&amp;#39;s probabby good to take a look at used server/workstation parts. Too add to that: Older 2 socket MBs are pretty cheap and each socket has its own lanes. Much more affordable than EPYC. From my personal expierence: Get at least something with AVX2 support.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnhrhbu</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/mnhrhbu/"/><updated>2025-04-16T23:03:42+00:00</updated><title>/u/zipperlein on What is the best option for running eight GPUs in a single motherboard?</title></entry><entry><author><name>/u/Lissanro</name><uri>https://www.reddit.com/user/Lissanro</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;I have a gaming motherboard with three x16 slots in my secondary workstation, but their actual speeds are x8 x8 x4. In the past, it was my primary workstation for over a year, and I had 4 GPUs connected to it - three via PCI-E 4.0 x16 30cm risers, and one via PCI-E 3.0 x1 riser. The motherboard also had two more x1 slots and also I could bifurcate two main slots from x8 to x4 x4, hence putting 8 GPUs in total, I also had 4kW in total of power, so power wasn&amp;#39;t an issue.&lt;/p&gt; &lt;p&gt;What was the issue, is lack of speed due to limited PCI-E lanes - even just loading a model was painfully slow, easily taking over 5 minutes, it would be even worse with 8 GPUs (and even more so with 32GB per GPU like in your case). But more than that, I could not use tensor parallelism efficiently, so was losing out a lot of performance too.&lt;/p&gt; &lt;p&gt;Then, when came time to upgrade, I decided to invest into getting EPYC based system rather than buying more GPUs, this is because on 4 GPUs, I already could fit Mistral Large 123B 5bpw along with Mistral-7B-instruct-v0.3-2.8bpw as a draft model for speculative decoding and 60K context size (with Q6 cache) with just 96 GB of VRAM (4x3090 GPUs). But I was getting only about 20 tokens/s on the gaming motherboard due its limited lanes.&lt;/p&gt; &lt;p&gt;Then, after I upgraded to an EPYC platform where I could put all four cards in x16 with actual 16 lanes in each, I am getting 36-42 tokens/s with Mistral Large 123B 5bpw (EXL2 quant running with TabbyAPI):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd ~/pkgs/tabbyAPI/ &amp;amp;&amp;amp; ./start.sh \ --model-name Mistral-Large-Instruct-2411-5.0bpw-exl2-131072seq \ --cache-mode Q6 --max-seq-len 60000 \ --draft-model-name Mistral-7B-instruct-v0.3-2.8bpw-exl2-32768seq \ --draft-rope-alpha=2.5 --draft-cache-mode=Q4 --tensor-parallel True &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In this case, I had to use Rope Alpha parameter for the draft model since it has only 32K context length natively while the main one can take up to 128K.&lt;/p&gt; &lt;p&gt;So, can you run 8 GPUs on your current motherboard? Yes, you can, but you have to tolerate very long loading times, and you will be leaving a lot of performance on the table.&lt;/p&gt; &lt;p&gt;On the other hand, if you consider getting inexpensive used EPYC platform with DDR4 memory, you can make it under $1000 budget if you go with lower core count (since for pure GPU inference, it is better to have faster cores, even if there is fewer of them), and perhaps 256GB or 512GB RAM. I recommend having more RAM than VRAM if your budget allows or if you find a good deal on it.&lt;/p&gt; &lt;p&gt;With your amount of VRAM, you will be able to run 8-bit quants of Mistral Large, Command A and other medium size models in 70B-141B range, along with higher precision draft models (however, I do not recommend going beyound 4bpw for a draft model, since beyond that it has little precision gains but starts to lose performance)..&lt;/p&gt; &lt;p&gt;Also, with so much VRAM, you may be able to run R1 as well, but using ik_llama.cpp (since it has the best performance for CPU+GPU inference on AMD CPU compared to other backends). But in this case, you will need EPYC 7763 at very least - this is what I have, and it is fully saturated during CPU+GPU inference, so I end up CPU bound rather RAM bandwidth bound. On EPYC 7736 + 4x3090 I get around 8 tokens/s with R1 and V3 UD-Q4_K_XL quant from Unsloth.&lt;/p&gt; &lt;p&gt;That said, this is likely to go beyond your $1000 budget, so if you do not need R1/V3, it is better to focus on GPU-only inference, since this way you can go with much cheaper CPU and will not only need less RAM, but also do not have to look for fastest 3200MHz DDR4 memory, and can go with something cheaper instead. Alternatively, like mentioned above, you can stay on your current motherboard, but may get not the best performance with 8 GPUs due to limited PCI-E lanes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mni7hlk</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/mni7hlk/"/><updated>2025-04-17T00:38:50+00:00</updated><title>/u/Lissanro on What is the best option for running eight GPUs in a single motherboard?</title></entry><entry><author><name>/u/ieatrox</name><uri>https://www.reddit.com/user/ieatrox</uri></author><category term="LocalLLaMA" label="r/LocalLLaMA" /><content type="html">&lt;!-- SC_OFF --&gt;&lt;div class=&quot;md&quot;&gt;&lt;p&gt;oof that&amp;#39;s a lot of effort for some pretty poorly supported hardware.&lt;/p&gt; &lt;p&gt;estimating $500 per card you&amp;#39;ve got $4000 in gpus, and you&amp;#39;re likely going to need at least another $1600 for a server chassis.&lt;/p&gt; &lt;p&gt;You might get lucky and find one with 8x pcie available for that much $.... (&lt;a href=&quot;https://www.ebay.ca/itm/387307003713&quot;&gt;https://www.ebay.ca/itm/387307003713&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I don&amp;#39;t think you have much option to avoid buying a server for at least $1000 and likely more.&lt;/p&gt; &lt;p&gt;But even if you use your existing parts you&amp;#39;re now at price parity with a 256gb m3 ultra studio, and if your motherboard or your cpu aren&amp;#39;t compatible with the pcie risers and you need to swap out to a server board, ecc ram, an epyc... well now the m3 ultra looks pretty cheap by comparison. You might even get into the 512gb studio price ranges.&lt;/p&gt; &lt;p&gt;&amp;quot;but will it work?&amp;quot; Not really. Your rocm version is ancient, your multi-card support is extremely buggy. You can&amp;#39;t do speculative decoding or use other modern techniques. You&amp;#39;re going to be googling esoteric fixes like needing to use make instead of cmake when compiling everything yourself because everything you run spits out garbled gibberish... at some point you&amp;#39;re going to stop being impressed with getting poor quality answers from small models super fast and want that huge pool to actually work correctly with a big maverick or R1 monster, and I don&amp;#39;t think it&amp;#39;ll match the Mac for speed when you&amp;#39;re actually using all 8 cards.&lt;/p&gt; &lt;p&gt;Get it if you want to tinker with a hobby system, for that it would be fuckin cool as hell. As long as you can deal with the heat and noise. And if you paid significantly less for the cards that makes it way more enticing to tinker with. Just don&amp;#39;t expect performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt;</content><id>t1_mnii26p</id><link href="https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/mnii26p/"/><updated>2025-04-17T01:42:26+00:00</updated><title>/u/ieatrox on What is the best option for running eight GPUs in a single motherboard?</title></entry></feed>