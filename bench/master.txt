---

**Title:** IBM Granite 3.3 Models  
**Should Include:** true
**ID:** t3_1k0mesv  
**Published:** 2025-04-16T14:54:48+00:00  
**Content:**  
Announcement Post: [Link](https://www.ibm.com/new/announcements/ibm-granite-3-3-speech-recognition-refined-reasoning-rag-loras)  
3.3 Speech Model: [Link](https://huggingface.co/ibm-granite/granite-speech-3.3-8b)  
Submitted by /u/suitable_cowboy.  
[Link](https://huggingface.co/collections/ibm-granite/granite-33-language-models-67f65d0cca24bcbd1d3a08e3) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0mesv/ibm_granite_33_models/)  

---

**Title:** Somebody needs to tell Nvidia to calm down with these new model names.  
**Should Include:** false
**ID:** t3_1k0u8ew  
**Published:** 2025-04-16T20:14:27+00:00  
**Content:**  
Submitted by /u/Porespellar.  
[Link](https://i.redd.it/hl0xrywo89ve1.jpeg) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0u8ew/somebody_needs_to_tell_nvidia_to_calm_down_with/)  

---

**Title:** Massive 5000 tokens per second on 2x3090  
**Should Include:** true
**ID:** t3_1k0tkca  
**Published:** 2025-04-16T19:47:07+00:00  
**Content:**  
For research purposes, I need to process huge amounts of data as quickly as possible.  

**The Model:**  
Tested Qwen2.5-7B, which is "just good enough." Larger models are better but slower. Performance tests included MMLU-pro (language understanding) and BBH (various tasks).  

**Processing Engine:**  
Used Aphrodite engine due to tests with speculative decoding.  

**Model Quantization:**  
Tested unquantized, AWQ/GPTQ, and W4A16-G128/W8A8. W8A8 was superior in performance.  

**Speculative Decoding:**  
7B has a different tokenizer, so smaller models couldn't be used as draft models.  

**Final Optimizations:**  
Command to run OpenAI REST API:  
`aphrodite run ./Qwen2.5-7B-Instruct_W8A8_custom --port 8000 -tp 2 --max_seq_len 8192 --max_model_len 8192 --max_num_seqs 32 --tensor-parallel-size 2 --gpu-memory-utilization 0.75`  

**Results:**  
- 4500 t/s ingesting  
- 825 t/s generation  
- Context size: ~5k tokens  

Submitted by /u/woozzz123.  
[Link](https://www.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/)  

---

**Title:** OpenAI Introducing OpenAI o3 and o4-mini  
**Should Include:** true
**ID:** t3_1k0pnvl  
**Published:** 2025-04-16T17:08:52+00:00  
**Content:**  
OpenAI released o3 and o4-mini, the latest models trained to think longer before responding. These are the smartest models they've released to date.  
Submitted by /u/stocksavvy_ai.  
[Link](https://openai.com/index/introducing-o3-and-o4-mini/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0pnvl/openai_introducing_openai_o3_and_o4mini/)  

---

**Title:** Droidrun is now Open Source  
**Should Include:** true
**ID:** t3_1k0h641  
**Published:** 2025-04-16T10:32:33+00:00  
**Content:**  
Droidrun framework is now public and open-source on GitHub.  
GitHub Repo: [Link](https://github.com/droidrun/droidrun)  
Submitted by /u/Sleyn7.  
[Link](https://i.redd.it/9zbo1emvc6ve1.jpeg) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0h641/droidrun_is_now_open_source/)  

---

**Title:** Price vs LiveBench Performance of non-reasoning LLMs  
**Should Include:** true
**ID:** t3_1k0kape  
**Published:** 2025-04-16T13:22:09+00:00  
**Content:**  
Submitted by /u/Balance-.  
[Link](https://i.redd.it/eiojps9w67ve1.png) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0kape/price_vs_livebench_performance_of_nonreasoning/)  

---

**Title:** Results of Ollama Leakage  
**Should Include:** true
**ID:** t3_1k0p3h0  
**Published:** 2025-04-16T16:46:35+00:00  
**Content:**  
Many servers still seem to be missing basic security.  
[Link](https://www.freeollama.com/)  
Submitted by /u/zxbsmk.  
[Link](https://i.redd.it/kl4bv7ne78ve1.png) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0p3h0/results_of_ollama_leakage/)  

---

**Title:** OpenAI introduces codex: a lightweight coding agent that runs in your terminal  
**Should Include:** true
**ID:** t3_1k0qisr  
**Published:** 2025-04-16T17:42:48+00:00  
**Content:**  
Submitted by /u/MorroWtje.  
[Link](https://github.com/openai/codex) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0qisr/openai_introduces_codex_a_lightweight_coding/)  

---

**Title:** o4-mini is 186ᵗʰ best coder, sleep well platter! Enjoy retirement!  
**Should Include:** true
**ID:** t3_1k0qbme  
**Published:** 2025-04-16T17:34:46+00:00  
**Content:**  
Submitted by /u/BidHot8598.  
[Link](https://i.redd.it/0p5ymcc7g8ve1.jpeg) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0qbme/o4mini_is_186ᵗʰ_best_coder_sleep_well_platter/)  

---

**Title:** Announcing RealHarm: A Collection of Real-World Language Model Application Failure  
**Should Include:** true
**ID:** t3_1k0iu5z  
**Published:** 2025-04-16T12:10:26+00:00  
**Content:**  
RealHarm is a dataset of real-world problematic interactions with AI agents, drawn from publicly reported incidents. Key findings:  
- Reputational damage was the most common organizational harm.  
- Misinformation and hallucination were the most frequent hazards.  
- State-of-the-art guardrails have failed to catch many incidents.  
Paper and dataset: [Link](https://realharm.giskard.ai/)  
Submitted by /u/chef1957.  
[Link](https://www.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/)  

---

**Title:** We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed  
**Should Include:** true
**ID:** t3_1k0c40c  
**Published:** 2025-04-16T04:38:13+00:00  
**Content:**  
Trained a model with a retry_reward to improve search performance. Achieved 46% performance score vs 20% baseline.  
Paper: [Link](https://arxiv.org/abs/2504.11001)  
Hugging Face: [Link](https://huggingface.co/Menlo/ReZero-v0.1-llama-3.2-3b-it-grpo-250404)  
GitHub: [Link](https://github.com/menloresearch/ReZero)  
Submitted by /u/Kooky-Somewhere-2883.  
[Link](https://v.redd.it/x9c46kt8l4ve1) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/)  

---

**Title:** KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...  
**Should Include:** true
**ID:** t3_1k0odhq  
**Published:** 2025-04-16T16:16:39+00:00  
**Content:**  
Submitted by /u/Eisenstein.  
[Link](https://i.imgur.com/py5Tvae.png) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/)  

---

**Title:** the budget rig goes bigger, 5060tis bought! test results incoming tonight  
**Should Include:** false
**ID:** t3_1k0kzgn  
**Published:** 2025-04-16T13:53:42+00:00  
**Content:**  
Bought 5060ti cards for £400 each (16GB). Testing performance with LLMs.  
Submitted by /u/gaspoweredcat.  
[Link](https://www.reddit.com/r/LocalLLaMA/comments/1k0kzgn/the_budget_rig_goes_bigger_5060tis_bought_test/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0kzgn/the_budget_rig_goes_bigger_5060tis_bought_test/)  

---

**Title:** Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max  
**Should Include:** true
**ID:** t3_1k0r9pi  
**Published:** 2025-04-16T18:12:44+00:00  
**Content:**  
Running llama.cpp with Gemma 3 27B on M4 Max yields better quality than Ollama, especially for code generation tasks.  
Submitted by /u/IonizedRay.  
[Link](https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/)  

---

**Title:** Yes, you could have 160gb of vram for just about $1000.  
**Should Include:** true
**ID:** t3_1k0b8wx  
**Published:** 2025-04-16T03:46:47+00:00  
**Content:**  
Built a rig with 10 MI50 GPUs ($90 each) for ~$1000. Achieved 160GB VRAM. Tested with llama.cpp and various models.  
Submitted by /u/segmond.  
[Link](https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/)  

---

**Title:** LocalAI v2.28.0 + Announcing LocalAGI: Build & Run AI Agents Locally Using Your Favorite LLMs  
**Should Include:** true
**ID:** t3_1k0haqw  
**Published:** 2025-04-16T10:41:06+00:00  
**Content:**  
LocalAI v2.28.0 updates include SYCL support and Lumina Text-to-Image models. LocalAGI is a new platform for building AI agent workflows locally.  
GitHub: [Link](https://github.com/mudler/LocalAI)  
LocalAGI: [Link](https://github.com/mudler/LocalAGI)  
LocalRecall: [Link](https://github.com/mudler/LocalRecall)  
Submitted by /u/mudler_it.  
[Link](https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/)  

---

**Title:** Hugging Face has launched a reasoning datasets competition with Bespoke Labs and Together AI  
**Should Include:** true
**ID:** t3_1k0q0bc  
**Published:** 2025-04-16T17:22:21+00:00  
**Content:**  
Competition to create reasoning datasets in underexplored domains. Prizes include $3,000+ in cash/credits. Deadline: May 1, 2025.  
Details: [Link](https://huggingface.co/blog/bespokelabs/reasoning-datasets-competition)  
Submitted by /u/dvanstrien.  
[Link](https://www.reddit.com/r/LocalLLaMA/comments/1k0q0bc/hugging_face_has_launched_a_reasoning_datasets/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0q0bc/hugging_face_has_launched_a_reasoning_datasets/)  

---

**Title:** It is almost May of 2025. What do you consider to be the best coding tools?  
**Should Include:** false
**ID:** t3_1k0nxlb  
**Published:** 2025-04-16T15:58:33+00:00  
**Content:**  
Discussion on the best coding tools and IDEs for programming projects, including game development.  
Submitted by /u/Material_Key7014.  
[Link](https://www.reddit.com/r/LocalLLaMA/comments/1k0nxlb/it_is_almost_may_of_2025_what_do_you_consider_to/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0nxlb/it_is_almost_may_of_2025_what_do_you_consider_to/)  

---

**Title:** What is the best option for running eight GPUs in a single motherboard?  
**Should Include:** false
**ID:** t3_1k0w7f9  
**Published:** 2025-04-16T21:37:38+00:00  
**Content:**  
Question about running 8 AMD MI50 GPUs on an ASUS ROG CROSSHAIR VIII DARK HERO motherboard using PCIE splitters.  
Submitted by /u/MLDataScientist.  
[Link](https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/)  

---

**Title:** ByteDance releases Liquid model family of multimodal auto-regressive models (like GTP-4o)  
**Should Include:** true
**ID:** t3_1k05wpt  
**Published:** 2025-04-15T23:11:34+00:00  
**Content:**  
Liquid is an auto-regressive model for text and image generation.  
Hugging Face: [Link](https://huggingface.co/Junfeng5/Liquid_V1_7B)  
Demo: [Link](https://huggingface.co/spaces/Junfeng5/Liquid_demo)  
Submitted by /u/ResearchCrafty1804.  
[Link](https://i.redd.it/393vjiodz2ve1.jpeg) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k05wpt/bytedance_releases_liquid_model_family_of/)  

---

**Title:** InternVL3: Advanced MLLM series just got a major update – InternVL3-14B seems to match the older InternVL2.5-78B in performance  
**Should Include:** true
**ID:** t3_1k0fjny  
**Published:** 2025-04-16T08:37:27+00:00  
**Content:**  
OpenGVLab released InternVL3 with models ranging from 1B to 78B parameters. InternVL3-14B matches InternVL2.5-78B in performance.  
Hugging Face: [Link](https://huggingface.co/collections/OpenGVLab/internvl3-67f7f690be79c2fe9d74fe9d)  
Submitted by /u/Mr_Moonsilver.  
[Link](https://www.reddit.com/r/LocalLLaMA/comments/1k0fjny/internvl3_advanced_mllm_series_just_got_a_major/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0fjny/internvl3_advanced_mllm_series_just_got_a_major/)  

---

**Title:** Open Source tool from OpenAI for Coding Agent in terminal  
**Should Include:** true
**ID:** t3_1k0qw6k  
**Published:** 2025-04-16T17:57:46+00:00  
**Content:**  
GitHub: [Link](https://github.com/openai/codex)  
Question: Can it be used with local reasoning models?  
Submitted by /u/_anotherRandomGuy.  
[Link](https://www.reddit.com/r/LocalLLaMA/comments/1k0qw6k/open_source_tool_from_openai_for_coding_agent_in/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0qw6k/open_source_tool_from_openai_for_coding_agent_in/)  

---

**Title:** What are some Local search offerings that are competitive with OpenAI/Google, if such a thing can exist?  
**Should Include:** false
**ID:** t3_1k0s2cx  
**Published:** 2025-04-16T18:45:08+00:00  
**Content:**  
Discussion on local search alternatives to OpenAI/Google, highlighting issues with hallucinations and slow response times.  
Submitted by /u/m1tm0.  
[Link](https://www.reddit.com/r/LocalLLaMA/comments/1k0s2cx/what_are_some_local_search_offerings_that_are/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0s2cx/what_are_some_local_search_offerings_that_are/)  

---

**Title:** Setting Power Limit on RTX 3090 – LLM Test  
**Should Include:** true
**ID:** t3_1k0mrrt  
**Published:** 2025-04-16T15:09:42+00:00  
**Content:**  
Video: [Link](https://youtu.be/4KzetHrFHAE)  
Submitted by /u/1BlueSpork.  
[Link](https://youtu.be/4KzetHrFHAE) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0mrrt/setting_power_limit_on_rtx_3090_llm_test/)  

---

**Title:** What is your favorite uncensored model?  
**Should Include:** true
**ID:** t3_1k0967d  
**Published:** 2025-04-16T01:55:36+00:00  
**Content:**  
Looking for models that don't refuse requests like cooking meth, making pipe bombs, or other controversial topics.  
Submitted by /u/HornyGooner4401.  
[Link](https://www.reddit.com/r/LocalLLaMA/comments/1k0967d/what_is_your_favorite_uncensored_model/) | [Comments](https://www.reddit.com/r/LocalLLaMA/comments/1k0967d/what_is_your_favorite_uncensored_model/)  

---
