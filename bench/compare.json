[
  {
    "Title": "IBM Granite 3.3 Models",
    "ShouldInclude": true,
    "ID": "t3_1k0mesv",
    "Published": "2025-04-16T14:54:48+00:00",
    "Content": "Announcement Post: [Link](https://www.ibm.com/new/announcements/ibm-granite-3-3-speech-recognition-refined-reasoning-rag-loras)\n3.3 Speech Model: [Link](https://huggingface.co/ibm-granite/granite-speech-3.3-8b)",
    "Link": "https://huggingface.co/collections/ibm-granite/granite-33-language-models-67f65d0cca24bcbd1d3a08e3",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0mesv/ibm_granite_33_models/"
  },
  {
    "Title": "Somebody needs to tell Nvidia to calm down with these new model names.",
    "ShouldInclude": false,
    "ID": "t3_1k0u8ew",
    "Published": "2025-04-16T20:14:27+00:00",
    "Content": "",
    "Link": "https://i.redd.it/hl0xrywo89ve1.jpeg",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0u8ew/somebody_needs_to_tell_nvidia_to_calm_down_with/"
  },
  {
    "Title": "Massive 5000 tokens per second on 2x3090",
    "ShouldInclude": true,
    "ID": "t3_1k0tkca",
    "Published": "2025-04-16T19:47:07+00:00",
    "Content": "For research purposes, I need to process huge amounts of data as quickly as possible.\n\nThe Model:\nTested Qwen2.5-7B, which is \"just good enough.\" Larger models are better but slower. Performance tests included MMLU-pro (language understanding) and BBH (various tasks).",
    "Link": "https://www.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/"
  },
  {
    "Title": "OpenAI Introducing OpenAI o3 and o4-mini",
    "ShouldInclude": true,
    "ID": "t3_1k0pnvl",
    "Published": "2025-04-16T17:08:52+00:00",
    "Content": "OpenAI released o3 and o4-mini, the latest models trained to think longer before responding. These are the smartest models they've released to date.",
    "Link": "https://openai.com/index/introducing-o3-and-o4-mini/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0pnvl/openai_introducing_openai_o3_and_o4mini/"
  },
  {
    "Title": "Droidrun is now Open Source",
    "ShouldInclude": true,
    "ID": "t3_1k0h641",
    "Published": "2025-04-16T10:32:33+00:00",
    "Content": "Droidrun framework is now public and open-source on GitHub.\nGitHub Repo: [Link](https://github.com/droidrun/droidrun)",
    "Link": "https://i.redd.it/9zbo1emvc6ve1.jpeg",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0h641/droidrun_is_now_open_source/"
  },
  {
    "Title": "Price vs LiveBench Performance of non-reasoning LLMs",
    "ShouldInclude": true,
    "ID": "t3_1k0kape",
    "Published": "2025-04-16T13:22:09+00:00",
    "Content": "",
    "Link": "https://i.redd.it/eiojps9w67ve1.png",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0kape/price_vs_livebench_performance_of_nonreasoning/"
  },
  {
    "Title": "Results of Ollama Leakage",
    "ShouldInclude": true,
    "ID": "t3_1k0p3h0",
    "Published": "2025-04-16T16:46:35+00:00",
    "Content": "Many servers still seem to be missing basic security.\n[Link](https://www.freeollama.com/)",
    "Link": "https://i.redd.it/kl4bv7ne78ve1.png",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0p3h0/results_of_ollama_leakage/"
  },
  {
    "Title": "OpenAI introduces codex: a lightweight coding agent that runs in your terminal",
    "ShouldInclude": true,
    "ID": "t3_1k0qisr",
    "Published": "2025-04-16T17:42:48+00:00",
    "Content": "",
    "Link": "https://github.com/openai/codex",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0qisr/openai_introduces_codex_a_lightweight_coding/"
  },
  {
    "Title": "o4-mini is 186ᵗʰ best coder, sleep well platter! Enjoy retirement!",
    "ShouldInclude": false,
    "ID": "t3_1k0qbme",
    "Published": "2025-04-16T17:34:46+00:00",
    "Content": "",
    "Link": "https://i.redd.it/0p5ymcc7g8ve1.jpeg",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0qbme/o4mini_is_186ᵗʰ_best_coder_sleep_well_platter/"
  },
  {
    "Title": "Announcing RealHarm: A Collection of Real-World Language Model Application Failure",
    "ShouldInclude": true,
    "ID": "t3_1k0iu5z",
    "Published": "2025-04-16T12:10:26+00:00",
    "Content": "RealHarm is a dataset of real-world problematic interactions with AI agents, drawn from publicly reported incidents. Key findings:\n- Reputational damage was the most common organizational harm.\n- Misinformation and hallucination were the most frequent hazards.",
    "Link": "https://www.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/"
  },
  {
    "Title": "We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed",
    "ShouldInclude": true,
    "ID": "t3_1k0c40c",
    "Published": "2025-04-16T04:38:13+00:00",
    "Content": "Trained a model with a retry_reward to improve search performance. Achieved 46% performance score vs 20% baseline.\nPaper: [Link](https://arxiv.org/abs/2504.11001)\nHugging Face: [Link](https://huggingface.co/Menlo/ReZero-v0.1-llama-3.2-3b-it-grpo-250404)",
    "Link": "https://v.redd.it/x9c46kt8l4ve1",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/"
  },
  {
    "Title": "KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...",
    "ShouldInclude": true,
    "ID": "t3_1k0odhq",
    "Published": "2025-04-16T16:16:39+00:00",
    "Content": "",
    "Link": "https://i.imgur.com/py5Tvae.png",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/"
  },
  {
    "Title": "the budget rig goes bigger, 5060tis bought! test results incoming tonight",
    "ShouldInclude": false,
    "ID": "t3_1k0kzgn",
    "Published": "2025-04-16T13:53:42+00:00",
    "Content": "Bought 5060ti cards for £400 each (16GB). Testing performance with LLMs.",
    "Link": "https://www.reddit.com/r/LocalLLaMA/comments/1k0kzgn/the_budget_rig_goes_bigger_5060tis_bought_test/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0kzgn/the_budget_rig_goes_bigger_5060tis_bought_test/"
  },
  {
    "Title": "Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max",
    "ShouldInclude": true,
    "ID": "t3_1k0r9pi",
    "Published": "2025-04-16T18:12:44+00:00",
    "Content": "Running llama.cpp with Gemma 3 27B on M4 Max yields better quality than Ollama, especially for code generation tasks.",
    "Link": "https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/"
  },
  {
    "Title": "Yes, you could have 160gb of vram for just about $1000.",
    "ShouldInclude": true,
    "ID": "t3_1k0b8wx",
    "Published": "2025-04-16T03:46:47+00:00",
    "Content": "Built a rig with 10 MI50 GPUs ($90 each) for ~$1000. Achieved 160GB VRAM. Tested with llama.cpp and various models.",
    "Link": "https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/"
  },
  {
    "Title": "LocalAI v2.28.0 + Announcing LocalAGI: Build & Run AI Agents Locally Using Your Favorite LLMs",
    "ShouldInclude": true,
    "ID": "t3_1k0haqw",
    "Published": "2025-04-16T10:41:06+00:00",
    "Content": "LocalAI v2.28.0 updates include SYCL support and Lumina Text-to-Image models. LocalAGI is a new platform for building AI agent workflows locally.\nGitHub: [Link](https://github.com/mudler/LocalAI)\nLocalAGI: [Link](https://github.com/mudler/LocalAGI)",
    "Link": "https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/"
  },
  {
    "Title": "Hugging Face has launched a reasoning datasets competition with Bespoke Labs and Together AI",
    "ShouldInclude": true,
    "ID": "t3_1k0q0bc",
    "Published": "2025-04-16T17:22:21+00:00",
    "Content": "Competition to create reasoning datasets in underexplored domains. Prizes include $3,000+ in cash/credits. Deadline: May 1, 2025.\nDetails: [Link](https://huggingface.co/blog/bespokelabs/reasoning-datasets-competition)",
    "Link": "https://www.reddit.com/r/LocalLLaMA/comments/1k0q0bc/hugging_face_has_launched_a_reasoning_datasets/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0q0bc/hugging_face_has_launched_a_reasoning_datasets/"
  },
  {
    "Title": "It is almost May of 2025. What do you consider to be the best coding tools?",
    "ShouldInclude": true,
    "ID": "t3_1k0nxlb",
    "Published": "2025-04-16T15:58:33+00:00",
    "Content": "Discussion on the best coding tools and IDEs for programming projects, including game development.",
    "Link": "https://www.reddit.com/r/LocalLLaMA/comments/1k0nxlb/it_is_almost_may_of_2025_what_do_you_consider_to/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0nxlb/it_is_almost_may_of_2025_what_do_you_consider_to/"
  },
  {
    "Title": "What is the best option for running eight GPUs in a single motherboard?",
    "ShouldInclude": true,
    "ID": "t3_1k0w7f9",
    "Published": "2025-04-16T21:37:38+00:00",
    "Content": "Question about running 8 AMD MI50 GPUs on an ASUS ROG CROSSHAIR VIII DARK HERO motherboard using PCIE splitters.",
    "Link": "https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/"
  },
  {
    "Title": "ByteDance releases Liquid model family of multimodal auto-regressive models (like GTP-4o)",
    "ShouldInclude": true,
    "ID": "t3_1k05wpt",
    "Published": "2025-04-15T23:11:34+00:00",
    "Content": "Liquid is an auto-regressive model for text and image generation.\nHugging Face: [Link](https://huggingface.co/Junfeng5/Liquid_V1_7B)\nDemo: [Link](https://huggingface.co/spaces/Junfeng5/Liquid_demo)",
    "Link": "https://i.redd.it/393vjiodz2ve1.jpeg",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k05wpt/bytedance_releases_liquid_model_family_of/"
  },
  {
    "Title": "InternVL3: Advanced MLLM series just got a major update – InternVL3-14B seems to match the older InternVL2.5-78B in performance",
    "ShouldInclude": true,
    "ID": "t3_1k0fjny",
    "Published": "2025-04-16T08:37:27+00:00",
    "Content": "OpenGVLab released InternVL3 with models ranging from 1B to 78B parameters. InternVL3-14B matches InternVL2.5-78B in performance.\nHugging Face: [Link](https://huggingface.co/collections/OpenGVLab/internvl3-67f7f690be79c2fe9d74fe9d)",
    "Link": "https://www.reddit.com/r/LocalLLaMA/comments/1k0fjny/internvl3_advanced_mllm_series_just_got_a_major/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0fjny/internvl3_advanced_mllm_series_just_got_a_major/"
  },
  {
    "Title": "Open Source tool from OpenAI for Coding Agent in terminal",
    "ShouldInclude": true,
    "ID": "t3_1k0qw6k",
    "Published": "2025-04-16T17:57:46+00:00",
    "Content": "GitHub: [Link](https://github.com/openai/codex)\nQuestion: Can it be used with local reasoning models?",
    "Link": "https://www.reddit.com/r/LocalLLaMA/comments/1k0qw6k/open_source_tool_from_openai_for_coding_agent_in/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0qw6k/open_source_tool_from_openai_for_coding_agent_in/"
  },
  {
    "Title": "What are some Local search offerings that are competitive with OpenAI/Google, if such a thing can exist?",
    "ShouldInclude": true,
    "ID": "t3_1k0s2cx",
    "Published": "2025-04-16T18:45:08+00:00",
    "Content": "Discussion on local search alternatives to OpenAI/Google, highlighting issues with hallucinations and slow response times.",
    "Link": "https://www.reddit.com/r/LocalLLaMA/comments/1k0s2cx/what_are_some_local_search_offerings_that_are/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0s2cx/what_are_some_local_search_offerings_that_are/"
  },
  {
    "Title": "Setting Power Limit on RTX 3090 – LLM Test",
    "ShouldInclude": true,
    "ID": "t3_1k0mrrt",
    "Published": "2025-04-16T15:09:42+00:00",
    "Content": "Video: [Link](https://youtu.be/4KzetHrFHAE)",
    "Link": "https://youtu.be/4KzetHrFHAE",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0mrrt/setting_power_limit_on_rtx_3090_llm_test/"
  },
  {
    "Title": "What is your favorite uncensored model?",
    "ShouldInclude": true,
    "ID": "t3_1k0967d",
    "Published": "2025-04-16T01:55:36+00:00",
    "Content": "Looking for models that don't refuse requests like cooking meth, making pipe bombs, or other controversial topics.",
    "Link": "https://www.reddit.com/r/LocalLLaMA/comments/1k0967d/what_is_your_favorite_uncensored_model/",
    "Comments": "https://www.reddit.com/r/LocalLLaMA/comments/1k0967d/what_is_your_favorite_uncensored_model/"
  }
]
