# AI News Processor Benchmark Tool

This tool evaluates the quality of AI news summaries generated by the main processor. It uses the `qwen2.5-72b-instruct` model via an OpenAI-compatible API to analyze both the quality of summaries and the accuracy of relevance flags based on a specified persona.

## Prerequisites

1.  A `benchmark.json` file located in the `bench/results/` directory. This file should contain the raw input data, the generated summaries (`results`), and the name of the `persona` used during generation.
2.  Personas defined in the main `personas/` directory.
3.  Access to a Qwen2.5-72B-Instruct model (specifically `qwen2.5-72b-instruct`) via an OpenAI-compatible API endpoint.
4.  Environment variables configured (e.g., in a `.env` file) and loaded by the application:
    *   `LLM_API_KEY`: Your API key for accessing the LLM.
    *   `LLM_URL`: Base URL for the OpenAI-compatible API endpoint.

## Running the Benchmark

1.  Ensure the `benchmark.json` file exists in `bench/results/`.
2.  Ensure the required personas are present in the `personas/` directory relative to the main project root.
3.  Ensure your environment variables (`LLM_API_KEY`, `LLM_URL`) are correctly set up (e.g., in a `.env` file in the project root).
4.  Navigate to the `bench` directory.
5.  Run the benchmark:
    ```bash
    go run .
    ```

## Output

The benchmark tool produces:

1.  Console output detailing:
    *   Configuration loading and client initialization steps.
    *   Loading of benchmark data and the specified persona.
    *   Processing status for each entry (by ID).
    *   Evaluation results (Quality Rating, Relevance Correct) for each entry.
    *   Aggregate metrics calculation.
    *   Final summary including total items, relevance accuracy, quality score, and output file location.

2.  A JSON file named `benchmark_results_<timestamp>.json` in the `bench/results/` directory containing:
    *   `total_items`: Number of items evaluated.
    *   `relevance_accuracy`: Percentage of items where the relevance flag was deemed correct.
    *   `quality_score`: Average quality score (Excellent=100, Good=75, Fair=50, Poor=0).
    *   `persona_name`: The name of the persona used for evaluation.
    *   `persona_focus_areas`: The focus areas defined for the persona.
    *   `detailed_evaluations`: A map where keys are item IDs and values are objects containing:
        *   `quality_rating`: "Excellent", "Good", "Fair", or "Poor".
        *   `quality_explanation`: LLM's reasoning for the quality rating.
        *   `relevance_correct`: Boolean indicating if the original `IsRelevant` flag was correct.
        *   `relevance_explanation`: LLM's reasoning for the relevance assessment.

## Evaluation Criteria

The evaluation uses the prompt defined in the selected **persona** file for consistency, combined with a specific evaluation structure.

### Summary Quality (Descriptive Rubric -> Score)
The LLM assigns one of the following ratings, which are then converted to a numerical score for the aggregate `quality_score`:
- Excellent (100): Meets all criteria with high quality and insight.
- Good (75): Meets most criteria, minor issues.
- Fair (50): Some important criteria are missing or weak.
- Poor (0): Fails to meet most criteria.

Considerations: Accuracy of technical details, completeness, clarity, emphasis, technical depth, comment analysis, and relevance explanation (as interpreted by the evaluation LLM based on the persona).