# AI News Processor Benchmark Tool

This tool evaluates the quality of AI news summaries generated by the main processor using the Qwen2.5-72B model through an OpenAI-compatible API. It analyzes both the quality of summaries and the accuracy of relevance flags.

## Prerequisites

1. Debug output from the main processor in the `testoutput` directory
2. Access to a Qwen2.5-72B-Instruct-4bit model via an OpenAI-compatible API endpoint
3. Environment variables:
   - `LLM_API_KEY`: Your API key for accessing the LLM
   - `LLM_URL`: Base URL for the OpenAI-compatible API endpoint

## Running the Benchmark

1. Ensure you have the debug output files in the `testoutput` directory
2. Set the required environment variables:
   ```bash
   export LLM_API_KEY="your-api-key"
   export LLM_URL="https://your-api-endpoint"
   ```
3. Run the benchmark:
   ```bash
   go run .
   ```

## Output

The benchmark tool produces:

1. Console output with:
   - Total items evaluated
   - Relevance accuracy percentage
   - Quality rating for each item (Excellent, Good, Fair, Poor)
   - Detailed evaluations for each item

2. A JSON file (`benchmark_results.json`) containing:
   - Aggregate metrics
   - Detailed evaluations for each item
   - Quality explanations
   - Relevance assessments

## Evaluation Criteria

The evaluation uses the same prompt as the main application for consistency, with additional criteria for evaluating the quality of summaries:

### Summary Quality (Descriptive Rubric)
- Excellent: Meets all criteria with high quality and insight.
- Good: Meets most criteria, minor issues.
- Fair: Some important criteria are missing or weak.
- Poor: Fails to meet most criteria.

Consider: Accuracy of technical details, completeness, clarity, emphasis, technical depth, comment analysis, and relevance explanation.

### Relevance Assessment
The tool evaluates if the IsRelevant flag was correctly set based on the original criteria:

Items should be marked as relevant if they contain:
- New LLM models, runners or infrastructure releases
- New tooling around LLMs
- Big AI lab news
- Security news
- Innovative techniques for LLM performance/quality
- Innovations in offline/uncensored models
- Cost effective AI
- Benchmarks

Items should be marked as NOT relevant if they contain:
- Random complaints/opinions
- Politics
- Hardware purchases without benchmarks
- Negative comments 