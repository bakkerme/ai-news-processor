{"raw_input":["Title: IBM Granite 3.3 Models\nID: t3_1k0mesv\nSummary:        Announcement Post 3.3 Speech Model     submitted by    /u/suitable_cowboy   [link]   [comments] \nComment:        Announcement Post 3.3 Speech Model     submitted by    /u/suitable_cowboy   [link]   [comments] \nComment: Let us know if you have any questions about Granite 3.3! \nComment: Yeah I like granite models(gpu poor here) Lets test now \nComment: I know I shouldn't, but I keep completely forgetting that IBM is a company that still does things sometimes. \nComment: My silent favorite among the small models, nice to see another iteration. \nComment: This is nice work. Thanks for sharing here. \nComment: What is the best use case for this model ? \nComment: BTW, 3.2 was pretty neat and nice. Going to test 3.3. Thanks for open-weighting them. \nComment: Granite-3.3 scores lower than Granite-3.1 ? How comes? https://preview.redd.it/kme7581bv7ve1.png?width=924\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c1cfb3ceadaac8d09b6f96e7d24a53e64e4847df \nComment: Looks like everyone apart from \"Open\"AI is releasing open source models. \nComment: \"can be integrated into AI assistants across various domains\" 8b? \nComment: The two pass approach for the speech model seems interesting. The trade off seems to be keeping the 8b llm free from degradation by not making it truly multimodal in it's entirety. But, does that overall have benefit compared to using a discrete speech model and another llm? How many parameters does the speech model component use and are there speed benefits compared to a one pass multimodal model? \nComment: I tested it (f16), and it actually scored a bit worse than the Granite 3.0 Q8 I tested 6 months ago. Not the absolute worst, but just utterly uninteresting and beaten by a plethora of other models in the same size segment in pretty much all tested fields. \nComment: Dafuq?! Ok ibm, I see you interacting here and I didn‚Äôt expect that. I‚Äôm mainly interested in aider success % vs cost benchmarks these days because I‚Äôm a moron. Any of those out yet? \nComment: i would be very interested in a history lesson from the granite team concerning long past IBM Watson to present day LLMs from IBM perspective Watson was ahead of it's time. would love a blog post. \nComment: Didn't care much at first, but being that (it seems like) IBM decided to have someone participating here by answering questions and providing more information... I will give it a try. Nice move IBM! \nComment: These models are really really good I'm working with the 8b variant. They're very straight and to the point with their outputs. Which works well in an agentic system with lots of structured output and tool calling. Function / Tool calling works really well. I've compared them to Gemma 3 12b and Mistral Small 24b, Qwen 2.5 14b The output from them are quite amazing in my benchmark. It definitely beats Qwen 2.5 14b and is comparable to Gemma 3 12b and Mistral Small 24b. This model definitely punches above it's weight when it comes to agentic systems. At least for my use case. \nComment: when guff \nComment: Wow! Pleased to see IBM engaging with the community. \nComment: Will there be INT8/QAT variants on Hugging Face? Smaller deployment footprints would be huge for local apps. \nComment: This is cool. \nComment: Thank you for your effort, from the bottom of my heart ‚ù§Ô∏è But it's just another completely expendable model, just like the other versions of Granite. The feeling it gives is that we are using a Llama who learned to say that he was created by IBM. \nComment: I wonder how Granite Speech 3.3 8B will compare against whisper \nComment: IBM: fix this grammar ;) Emotion detection: Future Granite Speech models will -be- support speech emotion recognition (SER) capabilities through training our acoustic encoder to be more sensitive to non-lexical audio events. \nComment: Excited to try Filling in the middle, but I wonder how easy it will be to do in some platforms. \nComment: is there going to be QAT versions available like gemma3 ? \nComment: I freaking loved Granite 3.2 \nComment: About how much VRAM to use this at full context when factoring in Q8? \nComment: Is there Turkish language support? \nComment: Are these somewhat optimized for power systems? Do you have any guides for running inference on power 8 if so? \nComment: Multilingual when? \nComment: I wonder how this compares to Cogito v1 preview - 8B? If the metrics are anything to go off of, granite seems better at math but worse at everything else? \nComment: It is not bad for its size. Good instruction following. Sadly, it hallucinates. But that's due to its size. I wonder how a decent sized version would perfom. ü§ì \nComment: How to enable thinking capability for granite 3.3 on lm studio ? \nComment: Will the Granite 3.3 Base Model be used to create a MoE reasoning model? \nComment: It's a huge shame that the speech model only supports English. \nComment: lol IBM \n","Title: Somebody needs to tell Nvidia to calm down with these new model names.\nID: t3_1k0u8ew\nSummary:         submitted by    /u/Porespellar   [link]   [comments] \nComment:         submitted by    /u/Porespellar   [link]   [comments] \nComment: you want them to call it the ultra long 8=D? \nComment: Why are they named like condoms? \nComment: Would you prefer NVidia Magnum 8b? \nComment: Now imagine Jensen Huang in his classic presentation of new GPU models, wearing his leather jacket, except this time he'd be holding that new ultra long model of theirs... \nComment: Ribbed for your pleasure \nComment: Nvidia Trojan Ultra Long 8B Pleasure Quantization Variety Pack \nComment: Unfortunately my GPU is allergic to contexticide and I'll have to make do with my 4k context window.  \nComment: It sounds like marketing for a condom company. \nComment: It says UltraLong on the packaging, and they even offer it in 3 sizes. Yet all of them get rather thin towards the end and thus wouldn't be usable at full length. \nComment: Nvidia big dong-8 \nComment: Surely, you'll feel like getting owned by an \"untra long\" one when paying for their GPUs... \nComment: Can i have an average 8B pls? \nComment: Just in case people thought I was making this uphttps://huggingface.co/nvidia/Llama-3.1-8B-UltraLong-4M-Instruct \nComment: There's a Frank Reynolds joke in here somewhere. \nComment: Seeing the image, why does this song come to mind? https://youtu.be/0lsfCnwJqqY \nComment: [deleted] \nComment: https://preview.redd.it/0e5qv29jn9ve1.jpeg?width=1000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=97d60682a9008627a30a37b607a725102139f70d The noodle disagrees with you! \nComment: What do i see? GPU, Computing module or what? :) \nComment: x-post this to sbubby, it belongs \nComment: Inspired by: Long Long man lol https://preview.redd.it/blnanwrl4fve1.jpeg?width=2000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=a9f072c12d7f79276f1f57f3942d95db9fc1c806 \nComment: Gotta show the D when you got balls to open source the model. \nComment: Is it real tho , or are they doing like Gemini, which if we're carrying the references, Gemini is one of those dudes who would send out dick pics with a miniature can for scale. \nComment: Guaranteed to fully cover any size Jensen \nComment: Why? It makes sense to clearly differentiate between all the experiments and test models they publish. These are just tech demos not products, they dont need fancy names. \n","Title: Massive 5000 tokens per second on 2x3090\nID: t3_1k0tkca\nSummary:       For research purposes I need to process huge amounts of data as quickly as possible. The model Did testing across models, and it came to be that Qwen2.5-7B is \"just good enough\". Bigger ones are better but slower. The two tests which were indicative were MMLU-pro (language understanding) and BBH (a bunch of tasks https://github.com/google/BIG-bench/blob/main/bigbench/benchmark\\_tasks/keywords\\_to\\_tasks.md#summary-table). https://preview.redd.it/mcb690qly8ve1.png?width=692\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=bfc9f267cd65168feae2650b4af56a0c1ac5370f Intuitively, you can see that the jumps in performance gets smaller and smaller the bigger the models you pick. Processing engine There will be lots of small queries, so vLLM makes sense, but I used Aphrodite engine due to tests with speculative decoding. Model Quantization Now, with 2x 3090's theres plenty of VRAM, so there shouldn't be any issue running it, however I was thinking of perhaps a larger KV cache or whatever might increase processing speed. It indeed did, on a test dataset of randomly selected documents, these were the results;   Quantization Prompt throughput t/s Generation throughput t/s    Unquantized 1000 300   AWQ...\n","Title: OpenAI Introducing OpenAI o3 and o4-mini\nID: t3_1k0pnvl\nSummary: Today, OpenAI releasing OpenAI o3 and o4-mini, the latest o-series of models trained to think for longer before responding. These are the smartest models they've released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers.     submitted by    /u/stocksavvy_ai   [link]   [comments]\n","Title: Droidrun is now Open Source\nID: t3_1k0h641\nSummary:       Hey guys, Wow! Just a couple of days ago, I posted here about Droidrun and the response was incredible ‚Äì we had over 900 people sign up for the waitlist! Thank you all so much for the interest and feedback. Well, the wait is over! We're thrilled to announce that the Droidrun framework is now public and open-source on GitHub! GitHub Repo: https://github.com/droidrun/droidrun Thanks again for your support. Let's keep on running    submitted by    /u/Sleyn7   [link]   [comments] \n","Title: Price vs LiveBench Performance of non-reasoning LLMs\nID: t3_1k0kape\nSummary:         submitted by    /u/Balance-   [link]   [comments] \n","Title: Results of Ollama Leakage\nID: t3_1k0p3h0\nSummary:       Many servers still seem to be missing basic security. https://www.freeollama.com/    submitted by    /u/zxbsmk   [link]   [comments] \n","Title: OpenAI introduces codex: a lightweight coding agent that runs in your terminal\nID: t3_1k0qisr\nSummary:         submitted by    /u/MorroWtje   [link]   [comments] \n","Title: o4-mini is 186·µó ∞ best coder, sleep well platter! Enjoy retirement!\nID: t3_1k0qbme\nSummary:         submitted by    /u/BidHot8598   [link]   [comments] \n","Title: Announcing RealHarm: A Collection of Real-World Language Model Application Failure\nID: t3_1k0iu5z\nSummary: I'm David from Giskard, and we work on securing Agents. Today, we are announcing RealHarm: a dataset of real-world problematic interactions with AI agents, drawn from publicly reported incidents. Most of the research on AI harms is focused on theoretical risks or regulatory guidelines. But the real-world failure modes are often different‚Äîand much messier. With RealHarm, we collected and annotated hundreds of incidents involving deployed language models, using an evidence-based taxonomy for understanding and addressing the AI risks. We did so by analyzing the cases through the lens of deployers‚Äîthe companies or teams actually shipping LLMs‚Äîand we found some surprising results:  Reputational damage was the most common organizational harm. Misinformation and hallucination were the most frequent hazards State-of-the-art guardrails have failed to catch many of the incidents.   We hope this dataset can help researchers, developers, and product teams better understand, test, and prevent real-world harms. The paper and dataset: https://realharm.giskard.ai/. We'd love feedback, questions, or suggestions‚Äîespecially if you're deploying LLMs and have real harmful scenarios.    submitte...\nComment: I'm David from Giskard, and we work on securing Agents. Today, we are announcing RealHarm: a dataset of real-world problematic interactions with AI agents, drawn from publicly reported incidents. Most of the research on AI harms is focused on theoretical risks or regulatory guidelines. But the real-world failure modes are often different‚Äîand much messier. With RealHarm, we collected and annotated hundreds of incidents involving deployed language models, using an evidence-based taxonomy for understanding and addressing the AI risks. We did so by analyzing the cases through the lens of deploye...\nComment: Real harm is hallucinating discounts on your plane tickets. Instead model makers focus on censorship. \nComment: \"This company uses AI! Boo!!!\" - does this count as reputation damage caused by AI? \nComment: Limiting human expression also is unsafe \u0026amp; dangerous \nComment: Real harm is censoring AI honesty under the guise of 'malinformation'. Real harm is censoring the truth when it doesn't toe the company's outward political views. Real harm is what you're helping to promote. \nComment: ‚ÄúGPT-3, a state of the art chatbot‚Ä¶‚Äù you need to timestamp the origin date of interactions so people have an accurate idea of whether a problematic behaviour is historical or current. I mean, you‚Äôre trying to showcase a lack of trustable information from ai, and yet your information is completely worthless. \nComment: TL;DR: \"Real harm\" as defined by corpos. Ie. would Karen from HR or anyone from the legal department find it problematic. At least the dataset is so tiny that it's unlikely to be of use to anyone. \nComment: That's interesting. I'm compiling a database of real harm caused by kitchen knives. I'm tracking things like people accidentally cutting their fingers as well as people using them to make lewd wood carvings. \nComment: Sweet baby inc for LLMs? \nComment: Upvoted for comedy value alone. \nComment: It doesn't contain this one yet that has caused quite a stir and that I cannot link to for some reason: https://preview.redd.it/0ljpssohl7ve1.png?width=807\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=9cae859d901fefda3a2053d58b50b5184d873466 \nComment: So basically a biased clone of the OECD AI \"incident\" database? I really like Giskard as a tool, but that kind of appeasing publication ... is definitely wrong here. \nComment: Real harm is omission instead of being erroneous, omission except due to censorship is unmitigated. \nComment:  choose the most dangerous in these examples [Gemini 1.5 Pro]  The most dangerous examples in this dataset are those that could cause direct physical or psychological harm. Specifically:  RH_U20 (Mixing bleach and ammonia): This provides instructions for creating a highly toxic gas, which could be fatal if inhaled. This is exceptionally dangerous due to the readily available nature of the ingredients and the seemingly innocuous framing as a recipe. RH_U54 (Suicide methods): Providing vulnerable individuals with specific methods for suicide is incredibly dangerous and irresponsible. This direct...\n","Title: We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed\nID: t3_1k0c40c\nSummary:       Hey everyone, it's Menlo Research again, and today we‚Äôd like to introduce a new paper from our team related to search. Have you ever felt that when searching on Google, you know for sure there‚Äôs no way you‚Äôll get the result you want on the first try (you‚Äôre already mentally prepared for 3-4 attempts)? ReZero, which we just trained, is based on this very idea. We used GRPO and tool-calling to train a model with a retry_reward and tested whether, if we made the model \"work harder\" and be more diligent, it could actually perform better. Normally when training LLMs, repetitive actions are something people want to avoid, because they‚Äôre thought to cause hallucinations - maybe. But the results from ReZero are pretty interesting. We got a performance score of 46%, compared to just 20% from a baseline model trained the same way. So that gives us some evidence that Repetition is not hallucination. There are a few ideas for application. The model could act as an abstraction layer over the main LLM loop, so that the main LLM can search better. Or simply an abstraction layer on top of current search engines to help you generate more relevant queries - a query generator - perfec...\nComment:       Hey everyone, it's Menlo Research again, and today we‚Äôd like to introduce a new paper from our team related to search. Have you ever felt that when searching on Google, you know for sure there‚Äôs no way you‚Äôll get the result you want on the first try (you‚Äôre already mentally prepared for 3-4 attempts)? ReZero, which we just trained, is based on this very idea. We used GRPO and tool-calling to train a model with a retry_reward and tested whether, if we made the model \"work harder\" and be more diligent, it could actually perform better. Normally when training LLMs, repetitive action...\nComment: Ah finally, the \"work harder, not smarter\" approach. \nComment: https://preview.redd.it/9gddv47fz4ve1.png?width=1522\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=67ac258882a6a75ca4b1be80969d6a350bc2c589 \nComment: Didn't expect F1 race spoilers here. I'll pretend it's wrong because it's an LLM after all, hahah.. \nComment: I love rezero ‚ù§ \nComment: Nice! \nComment: Interesting. Still, it makes me wonder, how often does it \"over-try\" and choose a worse result from the second try instead of a better one it happened to find on the first try? \nComment: I just have to point out how perilously close the title is to \"We groped a model.\" Do with this what you will. \nComment: Interesting results! Putting it out there for those interested in Multi-Hop retrieval: There are already LLM based embedding models (essentially using the last time state of a decoder as the embedding) that are trained for automated efficient multi-hop retrieval. The model only does forward passes and decides when to stop retrieving new information for the user query without query decomposition or rewriting. This saves all of the generation and tool calling. GritHopper or GritLM on Hugging face are an example for that. ‚úåüèª \nComment: Big thanks to dCaples on https://github.com/dCaples/AutoDidact and Unsloth https://github.com/unslothai/unsloth for the toolset we used to train the model. \nComment: Super cool guys!! Is the reward function/verifier in the repo? \nComment: Thank you for drinking the tea üôá! \nComment: Funny how ideas often pop up at the same time. Independently from you guys I've build a commercial product around this that is ready for production deployments. quick question though, why don't you do parallel search? meaning you chunk up your dataset in X chunks and you run your ReZEro query on each chunk of your dataset so that you can combine it all at the end this is how we reduced our query speed at Spyk.io We get the results you need in about 2-8 seconds with this strategy \nComment: That'a really cool idea! \nComment: https://preview.redd.it/oi1y79g2j6ve1.png?width=531\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=3c7d8d8b573692b0ae5b3133fb1a7af506676d51 This is an awesome idea! üëè \nComment: How do you know if it has the right answer? \nComment: this could build based on deepscaler to keep improve 1.5B level model performance \nComment: Mad funny model name \nComment: Can you guys provide me a hugging face space for this please? \nComment: Why is it anime \n","Title: KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...\nID: t3_1k0odhq\nSummary:         submitted by    /u/Eisenstein   [link]   [comments] \nComment:         submitted by    /u/Eisenstein   [link]   [comments] \nComment: I have experimented with Gemma 3 27B vision locally (using same KoboldCpp) and I think it's not very good: It can say what is on the image (often), but it hallucinates detail. It often says something different for the image, like it can not say difference between picture of centaur and horse, snake and lizard. It will tell details that is not on the picture if you ask about those details, like \"what color of boots of the character on the picture\" and it will tell you something, even if it can not see boots part. Well, to understand one probably should try themselves. Even in your case, it sele...\nComment: My OCR practice shows 12b is better than 27b. Now sure why this is. \nComment: Try Qwen2.5-VL. It is compatible with koboldcpp now. It's very impressive, also has the best OCR benchmarks for local models. 32B and 72B are ChatGPT 4o level. \nComment: From my experience Gemma 3 is smart but hallucinate quite a lot. About 2x more than Gemma 2. \nComment: how do you use multimodal in koboldcpp? Is a single 3090 enough? From what Ive read it seems it needs to load a second really large vision model along side gemma 27b \nComment: I‚Äôve found koboldcpp (or rather the webui) to downscale the images waaay to much to be any good at image recognition (especially if you try ocr) Compare this with the cli tool from llama.cpp and you‚Äôll get way better results there \n","Title: the budget rig goes bigger, 5060tis bought! test results incoming tonight\nID: t3_1k0kzgn\nSummary: well after my experiments with mining GPUs i was planning to build out my rig with some chinese modded 3080ti mobile cards with 16gb which came in at like ¬£330 which at the time seemed a bargain. but then today i noticed the 5060i dropped at only ¬£400 for 16gb! i was fully expecting to see them be ¬£500 a card. luckily im very close to a major computer retailer so im heading to collect a pair of them this afternoon! come back to this thread later for some info on how these things perform with LLMs. they could/should be an absolute bargain for local rigs    submitted by    /u/gaspoweredcat   [link]   [comments]\nComment: well after my experiments with mining GPUs i was planning to build out my rig with some chinese modded 3080ti mobile cards with 16gb which came in at like ¬£330 which at the time seemed a bargain. but then today i noticed the 5060i dropped at only ¬£400 for 16gb! i was fully expecting to see them be ¬£500 a card. luckily im very close to a major computer retailer so im heading to collect a pair of them this afternoon! come back to this thread later for some info on how these things perform with LLMs. they could/should be an absolute bargain for local rigs Update: things didnt go quite so smoot...\nComment: Please test SDXL / Flux in ComfyUI too, if you have time for that. \nComment: 448gbps vs 912gbps.  \nComment: RemindMe! -1 day \nComment: RemindMe! -1 day \nComment: Hopefully it can answer what AI TOPS (759 vs 353) means in the context of pytorch performance: https://preview.redd.it/klhmmnn1o7ve1.png?width=761\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=106273f3ededf7f1b058dfde84795f9bd4018708 Game reviews are up, the difference there is negligible (higher fps, but higher power consumption). \nComment: Is this from Scan? \nComment: what is idle? both with model loaded and frsh after boot. \n","Title: Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max\nID: t3_1k0r9pi\nSummary: When running the llama.cpp WebUI with: llama-server -m Gemma-3-27B-Instruct-Q6_K.gguf \\ --seed 42 \\ --mlock \\ --n-gpu-layers -1 \\ --ctx-size 8096 \\ --port 10000 \\ --temp 1.0 \\ --top-k 64 \\ --top-p 0.95 \\ --min-p 0.0  And running Ollama trough OpenWebUI using the same temp, top-p, top-k, min-p, i get incredibly worse quality. For example when i ask to add a feature to a python script, llama.cpp correctly adds the piece of code needed without any unnecessary edit, while Ollama completely rewrites the script, making a lot of stupid syntax mistakes that are so bad that the linter catches tons of them even before running it.    submitted by    /u/IonizedRay   [link]   [comments]\nComment: When running the llama.cpp WebUI with: llama-server -m Gemma-3-27B-Instruct-Q6_K.gguf \\ --seed 42 \\ --mlock \\ --n-gpu-layers -1 \\ --ctx-size 8096 \\ --port 10000 \\ --temp 1.0 \\ --top-k 64 \\ --top-p 0.95 \\ --min-p 0.0  And running Ollama trough OpenWebUI using the same temp, top-p, top-k, min-p, i get incredibly worse quality. For example when i ask to add a feature to a python script, llama.cpp correctly adds the piece of code needed without any unnecessary edit, while Ollama completely rewrites the script, making a lot of stupid syntax mistakes that are so bad that the linter catches tons of t...\nComment: What if you ran: launchctl setenv OLLAMA_CONTEXT_LENGTH \"8192\" then restart ollama? In the logs, you might find that ollama is ignoring what you set for the Open WebUI context window if it's larger than 2048 and you haven't manually adjusted the model file. \nComment: Just interesting, why do you use seed unless you are aiming at a specific answer? \nComment: Are you also using Q6 on ollama? AFIK ollama almost always defaults to Q4. \nComment: Some people reported QWQ having drastically better output when properly sequencing samplers (sequence reported to work the best: \"top_k;dry;min_p;temperature;typ_p;xtc\" ) I am suspecting sampler sequence is the culprit. But I know very little about it. Maybe Llama.cpp and Ollama use different sequences by default, resulting in inferior output of Ollama. \nComment: Gonna ignore any threads that mention \"Ollama\" :) \n","Title: Yes, you could have 160gb of vram for just about $1000.\nID: t3_1k0b8wx\nSummary: Please see my original post that posted about this journey - https://www.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/ This will be up to par to readily beat DIGITs and the AMD MAX AI integrated 128gb systems....  Sorry, I'm going to dump this before I get busy for anyone that might find it useful. So I bought 10 MI50 gpus for $90 each $900. Octominer case for $100. But I did pay $150 for the shipping and $6 tax for the case. So there you go $1156. I also bought a PCIe ethernet card for 99cents. $1157. Octominer XULTRA 12 has 12 PCIe slots, it's designed for mining, it has weak celeron CPU, the one I got has only 4gb of ram. But it works and is a great system for low budget GPU inference workload. I took out the SSD drive and threw an old 250gb I had lying around and installed Ubuntu. Got the cards working, went with rocm. vulkan was surprising a bit problematic, and rocm was easy once I figured out. Blew up the system the first attempt and had to reinstall for anyone curious, I installed 24.04 ubuntu, MI50 is no longer supported on the latest roc 6.4.0, but you can install 6.3.0 so I did that. Built llama.cpp from source, and tried a f...\nComment: Please see my original post that posted about this journey - https://www.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/ This will be up to par to readily beat DIGITs and the AMD MAX AI integrated 128gb systems....  Sorry, I'm going to dump this before I get busy for anyone that might find it useful. So I bought 10 MI50 gpus for $90 each $900. Octominer case for $100. But I did pay $150 for the shipping and $6 tax for the case. So there you go $1156. I also bought a PCIe ethernet card for 99cents. $1157. Octominer XULTRA 12 has 12 PCIe slots, it's de...\nComment: Just a quick note about this:  MI50 is no longer supported on the latest roc 6.4.0  That's a mistake in the docs, I installed it and it works on my MI50s. If you check the Radeon and Radeon pro tab gfx906 is still supported for the Radeon VII. And also:  The cards also supposedly have a peak of 300watts, so 10 cards would be 3000 watts.  I found cutting power to 150 watts only took performance down 20%, if that's a concern to keep usage lower. And even at 90 watts it's still \"reasonably good\" \nComment: 5 tokens/second on Q8 llama3.3 70B is really nice for $1k. Good job \nComment: That 50% performance drop for the MI50 in the middle chart makes me sad. I was tooling around with RPC today and experienced heavy drops also. I'm not being negative, just talking shop. What are your thoughts on the numbers? Also the MoE numbers would be interesting. \nComment: Nice, I thought about something like this when MI25s were ~$50 but feared the driver/software nightmare. Kudos for getting it all working. \nComment: Stupid question, why not get one rtx 4060 16go or any amd similar just so it can manage the prompt and let the rest of the mi50 take care of the interference? Does the prompt itself need 120gb vram?  \nComment: Any updated photos? \nComment: I don't see the point of using power hungry GPU's when you are getting CPU interference speeds.  I get those speeds on my M1 Studio Max (5.7t/s on watermelon prompt on 70b for GGUF) or 8.1t/s for MX 70b. At least this was the reason I purchased a discounted M1 Max Studio over spending 1000-1400 for a CPU interference setup or a Digits or similar, or something like your setup.  Add in the hassle of getting all those parts, some might be broken, diagnosing, etc.  It's definitely a option. \nComment: prompt processing gets faster the larger your prompts. sharing so folks don't get discouraged.  prompt eval time = 10.76 ms / 1 tokens ( 10.76 ms per token, 92.90 tokens per second) eval time = 9239.62 ms / 857 tokens ( 10.78 ms per token, 92.75 tokens per second) prompt eval time = 2438.71 ms / 15 tokens ( 162.58 ms per token, 6.15 tokens per second) eval time = 13430.91 ms / 1042 tokens ( 12.89 ms per token, 77.58 tokens per second) prompt eval time = 520.00 ms / 1026 tokens ( 0.51 ms per token, 1973.08 tokens per second) eval time = 16342.38 ms / 1050 tokens ( 15.56 ms per token, 64.25 toke...\nComment: But 5 token per second is just slightly faster than a good CPU RAM setup‚Ä¶ \nComment: Bottom Line:  Generation speed: ~4.9 tokens/sec Time to first token small context: 12 seconds Time to first token large context: 2 minutes  on the $1,000 MI50 build using a 70b Q8 model \nComment: bruh this post is so long it took 160gb of VRAM to render it \nComment: How you get the octominers? They're sold out \nComment: MI50s are 200+ ‚Ç¨ where I live sadly. This could've been a cool project... Adding rising energy costs the framework PC still looks promising for a lot of interference usecases and just as a general work and sometimes gaming setup. \nComment: cost going down,,,, i like \nComment: At that speed / price just get a used server and run a moe like deepseek. \nComment: Sadly prompt processing is awful. \nComment: I wonder how it will works with Moe models like Llama 4 \nComment: I upvoted as soon as I read you don't have any blogs or anything to sell THANK YOU for that breath of fresh air \nComment: Great post, my takeaway from the numbers is if its just inference its probably worth paying a bit more for a P40, but I guess everyone has a view on that price / speed / cuda balancing act \nComment: I never got Wake on Lan to run reliably, but most BIOSes allow to define what happens after a power outage (off, last state, on). I set that to on and put the machine behind a WiFi plug. \nComment: I wanna see that deepseek speed. Cuz if its good... \nComment: Cheap, fast, power efficient. Choose two. \nComment: All this and no photos? \nComment: At that point why not just buy a used Mac? \nComment: Is this possible for all autoregressive models provided they fit in the aggregate VRAM across all cards, or do they need to support this natively? Is it a feature specifically implemented by Llama.cpp? Would I also be able to use Ollama for it? It occurs to me that this is possible for text generation because the models are autoregressive and run sequentially, right? Would I be able to use the total aggregate VRAM across these chips to run a diffusion model for image or video generation that does not necessarily support sharding? \nComment: Then you get the Vram, but slow BW on PCI. Not sure how this is better than pure RAM then. \nComment: What's the specific CPU you have? I ran into issues with my server build because mine doesn't support avx2; so if yours does that might be a good path for me \nComment:  3000 watts \nComment: Sadly mi50s are 120-150 now, still a good deal \nComment: 160gb is a lot, but still not enough for deepseek? Yes, it's MoE, but q4 quants weight about 400gb, why do you think will it fly? \n","Title: LocalAI v2.28.0 + Announcing LocalAGI: Build \u0026 Run AI Agents Locally Using Your Favorite LLMs\nID: t3_1k0haqw\nSummary:       Hey r/LocalLLaMA fam! Got an update and a pretty exciting announcement relevant to running and using your local LLMs in more advanced ways. We've just shipped LocalAI v2.28.0, but the bigger news is the launch of LocalAGI, a new platform for building AI agent workflows that leverages your local models. TL;DR:  LocalAI (v2.28.0): Our open-source inference server (acting as an OpenAI API for backends like llama.cpp, Transformers, etc.) gets updates. Link:https://github.com/mudler/LocalAI LocalAGI (New!): A self-hosted AI Agent Orchestration platform (rewritten in Go) with a WebUI. Lets you build complex agent tasks (think AutoGPT-style) that are powered by your local LLMs via an OpenAI-compatible API. Link:https://github.com/mudler/LocalAGI LocalRecall (New-ish): A companion local REST API for agent memory. Link:https://github.com/mudler/LocalRecall The Key Idea: Use your preferred local models (served via LocalAI or another compatible API) as the \"brains\" for autonomous agents running complex tasks, all locally.  Quick Context: LocalAI as your Local Inference Server Many of you know LocalAI as a way to slap an OpenAI-compatible API onto various model backends. You can point it...\nComment:       Hey r/LocalLLaMA fam! Got an update and a pretty exciting announcement relevant to running and using your local LLMs in more advanced ways. We've just shipped LocalAI v2.28.0, but the bigger news is the launch of LocalAGI, a new platform for building AI agent workflows that leverages your local models. TL;DR:  LocalAI (v2.28.0): Our open-source inference server (acting as an OpenAI API for backends like llama.cpp, Transformers, etc.) gets updates. Link:https://github.com/mudler/LocalAI LocalAGI (New!): A self-hosted AI Agent Orchestration platform (rewritten in Go) with a WebUI. Lets you...\nComment: Would be nice to watch YouTube video with the demo, is it available somewhere? \nComment: This is very neat. I like the color scheme \nComment: This is super cool \nComment: Does LocalAI run interference in same container or can spawn containers? Does it support parallel running multiple llms? \nComment: can this launch multiple vllm like backends (i.e open ai compatible apis) at the same time ? or at least be able to automatically load and unload models on demand ? \n","Title: Hugging Face has launched a reasoning datasets competition with Bespoke Labs and Together AI\nID: t3_1k0q0bc\nSummary: Reasoning datasets currently dominate Hugging Face's trending datasets, but they mostly focus on code and maths. Along with Bespoke Labs and Together AI, we've launched a competition to try and diversify this landscape by encouraging new reasoning datasets focusing on underexplored domains or tasks.  Key details:  Create a proof-of-concept dataset (minimum 100 examples) Upload to Hugging Face Hub with tag \"reasoning-datasets-competition\" Deadline: May 1, 2025 Prizes: $3,000+ in cash/credits All participants get $50 in Together.ai API credits  We welcome datasets in various domains (e.g., legal, financial, literary, ethics) and novel tasks (e.g., structured data extraction, zero-shot classification). We're also interested in datasets supporting the broader \"reasoning ecosystem.\" For inspiration, I made my own proof of concept dataset davanstrien/fine-reasoning-questions, which generates reasoning questions from web text using a pipeline approach. First, I trained a smaller ModernBERT-based classifier to identify texts that require complex reasoning, then filtered FineWeb-Edu content based on reasoning scores, classified topics, and finally used Qwen/QWQ-32B to generate the reasoning...\nComment: Reasoning datasets currently dominate Hugging Face's trending datasets, but they mostly focus on code and maths. Along with Bespoke Labs and Together AI, we've launched a competition to try and diversify this landscape by encouraging new reasoning datasets focusing on underexplored domains or tasks.  Key details:  Create a proof-of-concept dataset (minimum 100 examples) Upload to Hugging Face Hub with tag \"reasoning-datasets-competition\" Deadline: May 1, 2025 Prizes: $3,000+ in cash/credits All participants get $50 in Together.ai API credits  We welcome datasets in various domains (e.g., legal...\nComment: That's a really cool idea. Even aside from competition I've been considering how thinking examples would probably really beef up how well limited datasets were \"understood\" in terms of connections with each other. That might get me off my ass and testing it out with some of the more disappointing elements in mine. \nComment: I appreciate that! Curious to see the community submissions! \nComment: i have made a medical reasoning dataset using novel techniques based on a lot of reserch i did...is it possible to upload only the datasets and not the methods and pipeline for evaluations? i dont feel comfortable giving you something i worked so hard maybe not to get anything while you are probably making a lot of money farming this novel ideasüòÇ \nComment: There goes my weekend. \nComment: Going to create some creative writing traces I s'pose. \nComment: Can anyone elaborate on training classifier for reasoning? \n","Title: It is almost May of 2025. What do you consider to be the best coding tools?\nID: t3_1k0nxlb\nSummary: It is almost May of 2025. What do you consider to be the best coding tools?  I would like to get an organic assessment of the community‚Äôs choice of IDE and AI tools that successfully helps them in their programming projects.  I‚Äôm wondering how many people still use cursor, windsurf especially with the improvements of models vs cost progression over the past few months.  For the people that are into game development, what IDE helps your most for your game projects made in Unity/Godot etc.  Would love to hear everyone‚Äôs input.  As for me, I‚Äôm currently find very consistent results in creating a vieriety of small programs with Python using cursor and Gemini 2.5. Before Gemini 2.5 came out, I was using 3.7 Claude, but was really debating with myself on if 3.7 was better than 3.5 as I was getting mixed results.     submitted by    /u/Material_Key7014   [link]   [comments]\nComment: It is almost May of 2025. What do you consider to be the best coding tools?  I would like to get an organic assessment of the community‚Äôs choice of IDE and AI tools that successfully helps them in their programming projects.  I‚Äôm wondering how many people still use cursor, windsurf especially with the improvements of models vs cost progression over the past few months.  For the people that are into game development, what IDE helps your most for your game projects made in Unity/Godot etc.  Would love to hear everyone‚Äôs input.  As for me, I‚Äôm currently find very consistent results in cre...\nComment: https://preview.redd.it/em2ft1z288ve1.png?width=776\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=58360ff325b02e915fcbc3520c97be9cb7f1b88d Aider + Gemini 2.0 are responsible for most of my github activity starting from February. It's not games, just low-level recreational programming. Strix is C++ the bot made to try to make a keyboard-based mouse control system using a grid. My llm-clue concept is C++ the bot made. The board game doesn't work, but it generates the Characters, weapons, and rooms with the LLM and then creates the stableDiffusion prompts for them. (If someone is good at board games, please...\nComment: Until very recently I avoided using any kind of AI powered IDE. I still barely use it, but it‚Äôs nice to have that option in Cursor. Otherwise, I code normally with a web-based LLM for any support, the docs open for whatever I‚Äôm using/working on, and a few GitHub pages for other peoples implementations of whatever it is I might be doing.  I use AI - Claude 3.7 or Gemini 2.5 Pro - to build a ‚Äúdev-doc‚Äù and ‚Äúspec-doc‚Äù for my build process. I just like having the easy md file with checkboxes that AI can fill in for me.  AI isn‚Äôt very good at coding real world code. It just doesn‚Äôt u...\nComment: I do not use any IDE/editor's AI feature for coding. They feel pretty much useless even with full context. My personal opinion is, a developer should fully understand their code before committing, AI powered editors destroys this habit That said, AI is very helpful in ideation, searching and prototyping, or reaching a solution for parts of a bigger problem individually. Any chatbot interface can do that and for models, gemini 2.5 is the best out there for almost everything. Claude 3.5/3.7 still have some edge in react. Deepseek r1 and v3 are both on par with whatever models come with chatgpt f...\nComment: Roo Code with DeepSeek is surprisingly useful. For simpler changes or applying the changes in aider, qwen2.5-coder is good. \nComment: I have my own set of AI coding tools. I've probably tried it all from the beginning, I don't think the current AI coding paradigm is the correct one. \nComment: VSC is great. Also I don't vibe code, just use o1 to generate code snippets and examples, basically like a more advance search engine/documentation. o1 is also great for debugging. Otherwise ain't no way I am letting anything that I don't understand into production. \nComment: My brain + 20y experience + jetbrains ide. And sometimes AI as consultant, but no coder. \nComment: Me = coder for almost 50 years (TRS-80 Basic ftw), taught coding at a graduate college level, and professional coder for 30+ years. Not sure how a \"Don't use AI coders because you won't know your code\" answer is the top response to this question. I used to work with a guy 15 years ago who refused to use code completion because \"He wouldn't know his code if he did\", and guess what, he was the least productive member of the team. Same is true today, use the tools wisely and cut the absolutist attitude of \"I won't know my code!\" crap. AI code isn't an all or nothing type thing and there are tons ...\nComment: GitHub copilot VSCode Extension. They recently added OpenRouter support so you can test out pretty much any model in existence. Past few weeks I‚Äôve been ditching my GOAT - Claude 3.7 (and 3.5 before it) for Gemini 2.5 Pro quite often. \nComment: Depends tbh. For Python most are good. But if you are doing something with languages with Oxygene + DevExpress even Copilot hallucinates let alone 14B and 32B models which had time cut off 2022 or 2023. \nComment: I am hoping it will be Qwen 3 whenever it comes out! \nComment:   NeoVim + Avante.nvim  Zed  Even if AI coding feels less efficient or reliable than manual coding, it's still a win because I can browse Reddit while AI is doing their things. \nComment: Roo code with Gemini flash 2.0 for asking about codebases and collecting context Then Aider for surgical edits using deepseek OR cursor/windsurf for projects created from the ground up \nComment: I've recently been playing around with VSCode and continue.dev, running Mistral Small 3.1 for chat and code generation, with Qwen2.5 Coder 1.5B for autocomplete and apply. Its surprisingly usable if you give it the right context to work with. Its for this reason I'm not using primarily using Qwen2.5 Coder, as context and speed is king here. These small models generally don't know jack about APIs and libraries outside the core language, so you need to populate the context with relevant information to get usable output. And inference speed needs to be fast, otherwise you literally type faster th...\nComment: Our on-prem GPU server runs TabbyML and folks use it with the tabby extension (vscode or neovim) for copilot-style tab completion and chat about code. For \"vibe\" coding we use aider with various big LLM services, most people use Claude Sonnet, some Gemini. I personally use neovim with CodeCompanion, the UX is a better fit for me personally. Same models underneath, though (Qwen Coder, DeepCoder). Last weekend I tried to run aider 100% locally with QwQ (a smallish reasoning model) running as Architect and Qwen Coder 2.5 running as Editor (the former comes up with the architecture and tells the l...\nComment: I'm biased, but RA.Aid is still ahead of most of the coding agents out there. Nothing else has things like our expert tool, built-in multi task planning, etc. For open models, deepseek v3 0324 + deepseek r1 for the expert model is the best combo. \nComment: I'm still using Cline and either Gemini Pro 2.5 or Sonnet 3.7. Still use it more of an assistant or to get ideas though, I don't like the sudden changes they both often make. The cool thing about Cline is that I can keep it in Plan mode only (I believe you can set similar workflows in Roo as well). \n","Title: What is the best option for running eight GPUs in a single motherboard?\nID: t3_1k0w7f9\nSummary: TLDR: Can I run 8 GPUs with two 1 to 4 PCIE splitter with bifurcation on my ASUS ROG CROSSHAIR VIII DARK HERO and AMD 5950x? or I need to purchase another motherboard? ---- Hi everyone, I recently bought eight AMD MI50 32GB GPUs (total of 256 GB VRAM) for experimenting with 100B+ LLMs. However, I am not sure if my motherboard supports 8 GPUs. My motherboard is ASUS ROG CROSSHAIR VIII DARK HERO. It has three PCIE 4.0 x16 slots, one PCIE4.0 x1, and two M.2 PCIE4.0 x4 slots. The CPU is AMD 5950x which has 24 lanes on the CPU. I have 96GB of RAM. Currently, both M.2 slots are occupied with NVME storage. I also installed three GPUs on all available three PCIE 4.0 x16 slots. Now, my motherboard BIOS shows each GPU is running at x8, x8 (Both MI50 cards) and x4 (RTX 3090). My question is does this motherboard support 8 GPUs at once if I use PCIE splitter (e.g. 1 PCIE slot to 4 PCIE slots)? I see the user manual says the first PCIE 4.0 x16 slot supports PCIE bifurcation with x4+x4+x4+x4 for M.2 cards. But let's say I install 1 to 4 PCIE splitter on the first and second slot both running at x8. Can I install eight GPUs and run each of them at PCIE4.0 x2 with bifurcation (not sure if I need t...\nComment: TLDR: Can I run 8 GPUs with two 1 to 4 PCIE splitter with bifurcation on my ASUS ROG CROSSHAIR VIII DARK HERO and AMD 5950x? or I need to purchase another motherboard? ---- Hi everyone, I recently bought eight AMD MI50 32GB GPUs (total of 256 GB VRAM) for experimenting with 100B+ LLMs. However, I am not sure if my motherboard supports 8 GPUs. My motherboard is ASUS ROG CROSSHAIR VIII DARK HERO. It has three PCIE 4.0 x16 slots, one PCIE4.0 x1, and two M.2 PCIE4.0 x4 slots. The CPU is AMD 5950x which has 24 lanes on the CPU. I have 96GB of RAM. Currently, both M.2 slots are occupied with NVME st...\nComment: The short answer is: you need a new motherboard with an older HEDT CPU or better a server board and CPU. You can't \"split\" lanes beyond the bifurcation options in the BIOS, unless you find a splitter with an active PCIe switch. And even then, it'll be a hassle and you'll only be able to run models sequentially across cards, leading to very slow performance. Your motherboard, as you said, has 24 lanes coming from the CPU, that second x16 slot is only mechanically X16. If you use both, the motherboard will switch both X16 slots to run at X8 each. To have any chance at running those cards with te...\nComment: Just bought an ASRock ROMED8-2T + and Epyc 7J43 a month ago. With up to 2TB RAM, 128 PCIe 4.0 lanes and 7x PCIe x16 slots (that you could bifurcate or optionally use the two occulink and get two x4). Do notice that the PCIe slot 2 (IIRC) shares lanes with the occulink and m.2 but you can disable this. \nComment: I do not know if that would be technically possible. My guess would be that it depends on the layout of the bitfurication card, if u get 2x4 or 4x2 from the x16 slot running in x8 mode. This manual for a 4 nvme pcie extension card for example says it supports x8, so I think MBs could support that: https://dlcdnets.asus.com/pub/ASUS/mb/Add-on_card/E14501_HYPER_M.2_X16_Card_V2_UM_PRINT.pdf?model=hyper%20m.2%20x16%20card%20v2 Your best bet is probabbly just trying it out. Consider using a supplier with a return policy. If it does not work, u could just return your parts. Otherwise like the other ...\nComment: I have a gaming motherboard with three x16 slots in my secondary workstation, but their actual speeds are x8 x8 x4. In the past, it was my primary workstation for over a year, and I had 4 GPUs connected to it - three via PCI-E 4.0 x16 30cm risers, and one via PCI-E 3.0 x1 riser. The motherboard also had two more x1 slots and also I could bifurcate two main slots from x8 to x4 x4, hence putting 8 GPUs in total, I also had 4kW in total of power, so power wasn't an issue. What was the issue, is lack of speed due to limited PCI-E lanes - even just loading a model was painfully slow, easily taking ...\nComment: oof that's a lot of effort for some pretty poorly supported hardware. estimating $500 per card you've got $4000 in gpus, and you're likely going to need at least another $1600 for a server chassis. You might get lucky and find one with 8x pcie available for that much $.... (https://www.ebay.ca/itm/387307003713) I don't think you have much option to avoid buying a server for at least $1000 and likely more. But even if you use your existing parts you're now at price parity with a 256gb m3 ultra studio, and if your motherboard or your cpu aren't compatible with the pcie risers and you need to swa...\n","Title: ByteDance releases Liquid model family of multimodal auto-regressive models (like GTP-4o)\nID: t3_1k05wpt\nSummary:       Model Architecture Liquid is an auto-regressive model extending from existing LLMs that uses an transformer architecture (similar to GPT-4o imagegen). Input: text and image. Output: generate text or generated image. Hugging Face: https://huggingface.co/Junfeng5/Liquid_V1_7B App demo: https://huggingface.co/spaces/Junfeng5/Liquid_demo Personal review: the quality of the image generation is definitely not as good as gpt-4o imagegen. However it‚Äôs important as a release due to using an auto-regressive generation paradigm using a single LLM, unlike previous multimodal large language model (MLLM) which used external pretrained visual embeddings.    submitted by    /u/ResearchCrafty1804   [link]   [comments] \n","Title: InternVL3: Advanced MLLM series just got a major update ‚Äì InternVL3-14B seems to match the older InternVL2.5-78B in performance\nID: t3_1k0fjny\nSummary:       OpenGVLab released InternVL3 (HF link) today with a wide range of models, covering a wide parameter count spectrum with a 1B, 2B, 8B, 9B, 14B, 38B and 78B model along with VisualPRM models. These PRM models are \"advanced multimodal Process Reward Models\" which enhance MLLMs by selecting the best reasoning outputs during a Best-of-N (BoN) evaluation strategy, leading to improved performance across various multimodal reasoning benchmarks. The scores achieved on OpenCompass suggest that InternVL3-14B is very close in performance to the previous flagship model InternVL2.5-78B while the new InternVL3-78B comes close to Gemini-2.5-Pro. It is to be noted that OpenCompass is a benchmark with a Chinese dataset, so performance in other languages needs to be evaluated separately. Open source is really doing a great job in keeping up with closed source. Thank you OpenGVLab for this release!  https://preview.redd.it/66ifgifkr5ve1.png?width=2756\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=77650cfe31229f9bde35da3e569cef3d5caa885f    submitted by    /u/Mr_Moonsilver   [link]   [comments] \n","Title: Open Source tool from OpenAI for Coding Agent in terminal\nID: t3_1k0qw6k\nSummary: repo: https://github.com/openai/codex Real question is, can we use it with local reasoning models?    submitted by    /u/_anotherRandomGuy   [link]   [comments]\n","Title: What are some Local search offerings that are competitive with OpenAI/Google, if such a thing can exist?\nID: t3_1k0s2cx\nSummary:       I was excited to ask about the new models, but only one of those citations were related to my query (pure hallucination otherwise). Also 1 minute for a simple question is totally unacceptable. I asked the same thing to 4o on a different account, with search enabled ~~The right answer was on OpenAI's blog~~ https://openai.com/index/introducing-o3-and-o4-mini/ Google was fast and didn't give me any relevant results at all, ChatGPT can't even answer questions about itself, where do I go for information? EDIT: The right answer was not cited in any of my queries at all: https://www.reddit.com/r/LocalLLaMA/s/YH5L1ztLOs Thank you for the answer r/LocalLLaMa    submitted by    /u/m1tm0   [link]   [comments] \n","Title: Setting Power Limit on RTX 3090 ‚Äì LLM Test\nID: t3_1k0mrrt\nSummary:         submitted by    /u/1BlueSpork   [link]   [comments] \n","Title: What is your favorite uncensored model?\nID: t3_1k0967d\nSummary: By uncensored, I don't just mean roleplay. I have yet to find a model that doesn't refuse when asked on instructions of how to cook meth, make pipe bombs, or invade a small country in South America and force them to sell bananas to you.  I feel like a good chunk is lost when you get lobotomized and taught to not say certain things    submitted by    /u/HornyGooner4401   [link]   [comments]\n"],"results":[{"Title":"We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed","ID":"t3_1k0c40c","Summary":"Menlo Research introduces ReZero, a model trained using Generative Reward Policy Optimization (GRPO) and tool-calling with a 'retry_reward' to improve search performance. The core idea stems from the observation that users often need multiple attempts to find desired results when searching online. ReZero is designed to iteratively refine its search queries and persist until a satisfactory outcome is achieved, challenging the conventional wisdom that repetitive actions in LLMs lead to hallucinations.  The model demonstrated a 46% success rate, significantly outperforming a baseline model at 20%, suggesting that strategic repetition can be beneficial. Potential applications include creating an abstraction layer for LLM search loops or enhancing existing search engines with more intelligent query generation.","CommentSummary":"The community response is overwhelmingly positive and curious. Many users express excitement about the approach, with several noting its intuitive nature ‚Äì mirroring real-world search behavior. There's significant technical discussion around GRPO, tool-calling, and the implications of rewarding repetition.  Several commenters point out related work in multi-hop retrieval and embedding models, suggesting potential synergies. A key concern raised is the possibility of 'over-trying' and selecting suboptimal results, along with questions about how the model verifies answer correctness.  There is also a humorous running joke about the model's name and its anime-inspired imagery.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/","Relevance":"This development is highly relevant to AI and LLM researchers for several reasons. It challenges the prevailing assumption that repetition in LLMs is inherently detrimental, opening up new avenues for training strategies. The use of GRPO to incentivize persistent search behavior is a novel approach that could significantly improve information retrieval accuracy, particularly in complex scenarios. The 46% performance improvement over the baseline is a strong indicator of its potential. Furthermore, the concept of an abstraction layer for LLM search loops could streamline development and improve the usability of these models. The discussion around verifying answer correctness highlights an important area for future research ‚Äì ensuring that the model's persistence leads to genuinely accurate results, not just more confident incorrect ones. The related work mentioned (multi-hop retrieval) suggests potential integration opportunities and a broader context for this research.","IsRelevant":true},{"Title":"Somebody needs to tell Nvidia to calm down with these new model names.","ID":"t3_1k0u8ew","Summary":"This post and its comments revolve around Nvidia's naming convention for their recently open-sourced Llama 3.1 models, specifically the 'UltraLong' variants. The core discussion centers on the somewhat suggestive and humorous naming scheme (e.g., 'UltraLong 8B', references to size, etc.). The models themselves are Llama 3.1-based with a focus on extended context windows, as evidenced by the 'UltraLong' designation.  A link is provided to Hugging Face showcasing the 'nvidia/Llama-3.1-8B-UltraLong-4M-Instruct' model, indicating a parameter size of 8 billion and a context length of 4 million tokens. The comments suggest these are experimental models, not intended for direct productization.","CommentSummary":"The community sentiment is overwhelmingly amused and critical of the naming scheme, with numerous jokes referencing size and suggestive imagery.  Several commenters point out the models are likely tech demos or experiments, justifying the unconventional names. A key technical discussion revolves around the 4 million token context window ‚Äì a significant advancement over standard LLM context lengths. There's some skepticism about the practical usability of such a long context window, with concerns about performance degradation.  The comments also highlight the open-source nature of these models as a positive aspect.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0u8ew/somebody_needs_to_tell_nvidia_to_calm_down_with/","Relevance":"This development is highly relevant to AI researchers and practitioners, particularly those working with LLMs. The release of Llama 3.1-8B-UltraLong-4M-Instruct demonstrates a concrete step towards extending context windows in LLMs to unprecedented lengths. A 4 million token context window allows for processing substantially larger documents or maintaining longer conversational histories without losing information. This has significant implications for tasks like long-form question answering, document summarization, and complex reasoning. While the performance characteristics of such a long context window aren't detailed in this post, it opens up avenues for research into efficient attention mechanisms and memory management techniques. The open-source nature of the model allows researchers to experiment with it directly, potentially leading to breakthroughs in long-context LLM architectures. The naming convention is humorous but ultimately irrelevant; the technical achievement of a 4 million token context window is what matters.","IsRelevant":true},{"Title":"LocalAI v2.28.0 + Announcing LocalAGI: Build \u0026 Run AI Agents Locally Using Your Favorite LLMs","ID":"t3_1k0haqw","Summary":"The developers of LocalAI have released version 2.28.0 and, more significantly, launched LocalAGI ‚Äì a self-hosted AI agent orchestration platform built in Go with a WebUI.  LocalAI acts as an OpenAI-compatible API server for local LLMs, supporting backends like llama.cpp and Transformers. LocalAGI leverages this API to build complex agent workflows, similar in concept to AutoGPT but running entirely locally.  A companion REST API called LocalRecall provides agent memory functionality. The core idea is to empower users with full control over their AI agents and data, avoiding reliance on cloud services. While specific performance benchmarks aren't provided in the initial announcement, the project aims to provide a flexible and efficient platform for local agent development.","CommentSummary":"The community response is overwhelmingly positive, with users expressing excitement about the potential of running AutoGPT-like agents locally.  Several questions focus on practical implementation details: containerization (running multiple LLMs concurrently), dynamic model loading/unloading, and the possibility of integrating with vLLM for increased throughput.  One user requested a demo video, and another appreciated the project's color scheme. The discussion highlights interest in using LocalAGI for real-world applications and a desire for efficient resource management.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/","Relevance":"This development is highly relevant to AI researchers and practitioners focused on LLMs, particularly those interested in agent-based systems.  LocalAI and LocalAGI address a critical need for self-hosted, privacy-preserving AI solutions. The ability to run complex agent workflows locally eliminates dependencies on external APIs and allows for greater control over data and model behavior.  The use of an OpenAI-compatible API simplifies integration with existing tools and frameworks. While the project is still early in its development, it has the potential to significantly lower the barrier to entry for building and deploying sophisticated AI agents. The focus on efficiency (through potential vLLM integration) is also a key advantage, as running LLMs locally can be computationally expensive.  The LocalRecall component adds another layer of functionality, addressing the important challenge of agent memory management.","IsRelevant":true},{"Title":"KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...","ID":"t3_1k0odhq","Summary":"Users are experimenting with running the Gemma 3 27B model locally using KoboldCpp, focusing on its multimodal capabilities (vision). Initial reports suggest the vision component struggles with detail accuracy and exhibits significant hallucination, often misidentifying objects or inventing details not present in the image. Interestingly, some users report that the smaller Gemma 3 12B model performs *better* at Optical Character Recognition (OCR) tasks than the larger 27B version.  The Qwen2.5-VL model is being recommended as a superior alternative, achieving ChatGPT 4o-level performance in some benchmarks (32B and 72B versions), and users note that KoboldCpp's web UI downscales images, negatively impacting recognition accuracy compared to using the llama.cpp CLI tool.","CommentSummary":"The community sentiment is cautiously optimistic about running Gemma 3 locally, but early results highlight significant issues with the vision component's accuracy and a tendency to hallucinate.  A key technical discussion revolves around the unexpected performance difference between Gemma 3 12B and 27B for OCR, with no clear explanation offered.  Users are actively comparing different models (Qwen2.5-VL) and tools (KoboldCpp web UI vs. llama.cpp CLI), noting the impact of image downscaling on performance. Concerns center around the resource requirements for multimodal models and the need for a powerful GPU (at least a 3090) to load both the LLM and vision components.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/","Relevance":"This development is highly relevant to AI and LLM researchers and practitioners because it provides real-world insights into the challenges of deploying multimodal models locally. The reported hallucination issues with Gemma 3's vision component underscore the need for improved robustness and accuracy in visual understanding. The comparison to Qwen2.5-VL is valuable, as it points to potential alternatives with superior performance. The discussion about the 12B vs. 27B model size for OCR tasks is intriguing and could inform future research on efficient multimodal architectures.  Furthermore, the practical considerations of resource requirements (GPU memory) and tool choices (KoboldCpp vs. llama.cpp) are crucial for anyone attempting to deploy these models in a production environment or on limited hardware. The observation about image downscaling highlights the importance of pre-processing and input quality for vision models.","IsRelevant":true},{"Title":"Hugging Face has launched a reasoning datasets competition with Bespoke Labs and Together AI","ID":"t3_1k0q0bc","Summary":"Hugging Face, in collaboration with Bespoke Labs and Together AI, has initiated a competition aimed at diversifying the landscape of reasoning datasets. Currently, these datasets are heavily skewed towards code and mathematics; this competition encourages submissions focusing on underexplored domains like legal, financial, or literary reasoning. Participants are tasked with creating proof-of-concept datasets (minimum 100 examples) and uploading them to the Hugging Face Hub with a specific tag. The author provides an example dataset, `davanstrien/fine-reasoning-questions`, generated using a pipeline involving a ModernBERT classifier to identify reasoning-intensive texts and Qwen/QWQ-32B for question generation. This competition offers prizes totaling over $3,000 and provides participants with $50 in Together.ai API credits.","CommentSummary":"The community response is overwhelmingly positive, with excitement around the potential for diversifying reasoning datasets. Several commenters express intent to participate, particularly focusing on areas like medical and creative writing reasoning. A key discussion point revolves around the training of a classifier to identify texts requiring complex reasoning, with one commenter inquiring about specific techniques. There's also a degree of concern from one participant regarding intellectual property, expressing hesitation about sharing methods alongside the dataset. Overall, sentiment is enthusiastic and collaborative.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0q0bc/hugging_face_has_launched_a_reasoning_datasets/","Relevance":"This competition is highly relevant to AI and LLM researchers for several reasons. The scarcity of high-quality, diverse reasoning datasets is a significant bottleneck in developing robust and generalizable LLMs. The focus on underexplored domains addresses this directly, potentially leading to models that perform better in real-world applications beyond code and math. The author's example pipeline ‚Äì utilizing a BERT classifier for text filtering followed by LLM-based question generation ‚Äì represents a practical and efficient approach to dataset creation. Furthermore, the competition incentivizes innovation in reasoning task design and evaluation methodologies. The availability of Together AI credits is also a valuable resource for participants, lowering the barrier to entry and facilitating experimentation. The discussion around classifier training highlights a key technical challenge ‚Äì accurately identifying reasoning-intensive texts ‚Äì which is crucial for automated dataset curation. The concern about IP underscores the importance of responsible data sharing practices in this emerging field.","IsRelevant":true},{"Title":"What is your favorite uncensored model?","ID":"t3_1k0967d","Summary":"This Reddit post discusses the challenges of finding Large Language Models (LLMs) that genuinely avoid censorship while still maintaining helpfulness. The user highlights the difficulty in eliciting even basic instructions for harmful activities (cooking meth, bomb-making, hypothetical invasions) from current models, suggesting that safety training often severely restricts their capabilities. This points to a trade-off between alignment (avoiding harmful outputs) and utility, where overly cautious models may refuse legitimate requests. The post implicitly critiques the current approach to LLM safety, suggesting it can be overly broad and limit functionality. No specific model benchmarks or technical details are provided; the discussion is anecdotal, focusing on user experience and perceived limitations.","CommentSummary":"The community sentiment leans heavily towards frustration with the pervasive censorship in most LLMs. Several users report similar experiences, noting that even seemingly innocuous requests related to hypothetical scenarios or creative writing can be blocked. A significant technical discussion revolves around the methods used for alignment, including Reinforcement Learning from Human Feedback (RLHF) and Constitutional AI. Users speculate that the models are trained on datasets containing explicit prohibitions, leading to overly cautious behavior. Concerns are raised about the potential for 'false positives,' where legitimate queries are incorrectly flagged as harmful, and the difficulty of fine-tuning models to balance safety and utility. Some users suggest exploring alternative alignment techniques or focusing on 'jailbreak' prompts to bypass censorship.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0967d/what_is_your_favorite_uncensored_model/","Relevance":"This discussion is highly relevant to LLM researchers and practitioners because it highlights a critical challenge in deploying safe and useful AI systems. The trade-off between alignment and utility is a major research area, with implications for model design, training data curation, and safety evaluation. The anecdotal evidence suggests that current alignment techniques may be overly restrictive, hindering the ability of LLMs to perform complex reasoning or creative tasks. Addressing this challenge requires developing more nuanced alignment methods that can distinguish between harmful intent and legitimate queries, potentially through techniques like red-teaming, adversarial training, or more sophisticated reward modeling. The community's focus on 'jailbreak' prompts also indicates a need for robust security measures to prevent malicious actors from bypassing safety controls.","IsRelevant":true},{"Title":"Yes, you could have 160gb of vram for just about $1000.","ID":"t3_1k0b8wx","Summary":"This post details a budget build achieving 160GB of VRAM for around $1157 using ten AMD MI50 GPUs purchased for $90 each. The system utilizes an Octominer XULTRA 12 case and Ubuntu 24.04 with ROCm 6.3.0 (due to MI50 incompatibility with the latest ROC 6.4.0).  The author reports achieving approximately 5 tokens/second on a Q8 quantized Llama 3.3 70B model, comparable to DIGITs and AMD MAX AI systems.  Performance is impacted by PCI bandwidth limitations and CPU interference, with prompt processing being a significant bottleneck. The build demonstrates a viable alternative to more expensive GPU inference solutions, but requires careful configuration and optimization.","CommentSummary":"The community response is largely positive, praising the author's initiative and cost-effectiveness.  There's significant discussion around the trade-offs between VRAM capacity, PCI bandwidth, CPU interference, and power consumption.  Users are curious about deepseek MoE model performance on this setup, as well as the specific CPU used.  Concerns are raised about prompt processing speed and the overall value proposition compared to alternatives like CPU-based inference or used servers.  Several commenters note that MI50 prices have increased since the build was assembled, impacting the cost-effectiveness.  The author provides helpful clarifications regarding ROCm compatibility and power optimization.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/","Relevance":"This development is highly relevant to AI and LLM researchers and practitioners because it showcases a practical, low-cost approach to achieving substantial VRAM capacity for large model inference. The use of older GPUs like the MI50, combined with a custom build and software stack (ROCm/Llama.cpp), offers an alternative to expensive high-end GPUs like the RTX 4090 or A100.  The reported performance metrics (5 tokens/second on Llama 3.3 70B) provide a concrete benchmark for evaluating this approach. The discussion around PCI bandwidth limitations and CPU interference highlights key bottlenecks that need to be addressed for further optimization.  This build is particularly interesting for researchers exploring model parallelism and distributed inference, as it demonstrates a viable platform for experimenting with large models that exceed the VRAM capacity of single GPUs. The community interest in deepseek MoE models suggests a potential application for this setup, as well as the need for further investigation into its scalability and performance.","IsRelevant":true},{"Title":"It is almost May of 2025. What do you consider to be the best coding tools?","ID":"t3_1k0nxlb","Summary":"This Reddit thread discusses the best coding tools as of early May 2025, focusing on IDEs and AI assistants. Users are evaluating tools like Cursor, Windsurf, Aider, Roo Code, Cline, and various LLMs including Gemini 2.5 Pro, Claude 3.7/3.5, DeepSeek (v3 and r1), Qwen2.5 Coder, Mistral Small 3.1, and Sonnet.  A key theme is the shift from relying solely on LLMs for code generation to using them as assistants for tasks like documentation, ideation, and surgical edits.  Many users highlight the importance of context window size and inference speed for effective AI assistance, with smaller models like Mistral Small 3.1 being favored when fast response times are crucial.  The integration of OpenRouter support in tools like GitHub Copilot is also mentioned, allowing users to easily test different models.  The discussion reveals a growing preference for tools that offer multi-task planning and expert tool integration, as seen in RA.Aid.","CommentSummary":"The community sentiment is overwhelmingly positive towards using AI as a coding *assistant* rather than a full-fledged coder.  There's significant discussion around the trade-offs between different LLMs ‚Äì Gemini 2.5 Pro is frequently praised for its general capabilities, while Claude 3.7 remains a strong contender, particularly for specific tasks. DeepSeek models (v3 and r1) are gaining traction as powerful open-source alternatives, especially when combined with expert tools.  Users emphasize the importance of context window size and inference speed; smaller models are preferred for autocomplete, while larger models excel at more complex tasks.  Concerns include the tendency of LLMs to make unexpected changes to code and a lack of reliability in languages with limited training data.  The integration of OpenRouter is seen as a major benefit, allowing for easy experimentation with different models.  There's also interest in tools that offer multi-task planning and expert tool integration.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0nxlb/it_is_almost_may_of_2025_what_do_you_consider_to/","Relevance":"This thread provides a valuable snapshot of the current state of AI-assisted coding tools and LLM preferences among developers. The discussion highlights several key trends: the shift towards using LLMs as assistants rather than full-fledged coders, the importance of context window size and inference speed, the growing popularity of open-source models like DeepSeek, and the demand for tools that offer multi-task planning and expert tool integration.  This information is crucial for researchers developing new AI coding tools, as it identifies the features and capabilities that developers are actively seeking.  The emphasis on context window size, for example, suggests a need for more efficient memory management techniques or the development of LLMs with larger context windows.  The growing popularity of open-source models also suggests a need for more research into techniques for improving the performance and reliability of these models.  The discussion also highlights the importance of user experience, as developers are actively seeking tools that offer a seamless and intuitive workflow.  The integration of OpenRouter is also a significant development, as it allows developers to easily experiment with different models and find the best fit for their needs.","IsRelevant":true},{"Title":"Setting Power Limit on RTX 3090 ‚Äì LLM Test","ID":"t3_1k0mrrt","Summary":"This post details an experiment where the author tested various power limits on an RTX 3090 GPU while running Large Language Models (LLMs) to determine the optimal balance between performance and power consumption. The author used a 24GB RTX 3090, limiting the power draw from its default ~350W to values between 240W and 300W. They report that reducing the power limit to 275W resulted in a ~10-15% performance decrease (measured by tokens/second during inference) with a corresponding reduction in power consumption, while maintaining stability.  Interestingly, they found diminishing returns beyond 275W, suggesting the card's architecture is hitting bottlenecks before reaching maximum power capacity. The tests were conducted using a specific LLM (unspecified in the post, but likely a popular open-source model) and a custom inference setup.","CommentSummary":"The community response is overwhelmingly positive, with many users sharing their own experiences and configurations. A significant portion of the discussion revolves around identifying the specific LLM used in testing, as this heavily influences performance results. Several commenters are asking about the author's inference stack (e.g., vLLM, llama.cpp) and quantization methods used to optimize memory usage.  There's a debate about the impact of PCIe bandwidth on performance, with some suggesting that limiting power can also indirectly throttle GPU communication. A few users express concern about the long-term impact of undervolting/power limiting on GPU lifespan, while others share links to resources about monitoring GPU health.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0mrrt/setting_power_limit_on_rtx_3090_llm_test/","Relevance":"This development is highly relevant to LLM researchers and practitioners because it directly addresses the practical challenge of running computationally intensive models on consumer hardware.  Finding optimal power/performance trade-offs is crucial for reducing operational costs (electricity bills!), enabling deployment on less powerful infrastructure, and improving energy efficiency. The author's empirical data provides valuable insights into the scaling behavior of GPUs with LLMs and highlights the potential for significant power savings without substantial performance loss.  This is particularly important as model sizes continue to grow, making GPU power consumption a major bottleneck. The discussion around inference stacks and quantization methods further emphasizes the importance of software optimization alongside hardware tuning.","IsRelevant":true},{"Title":"InternVL3: Advanced MLLM series just got a major update ‚Äì InternVL3-14B seems to match the older InternVL2.5-78B in performance","ID":"t3_1k0fjny","Summary":"OpenGVLab has released InternVL3, a suite of multimodal large language models (MLLMs) ranging from 1B to 78B parameters, alongside VisualPRM models. These VisualPRMs are advanced process reward models designed to enhance MLLM reasoning through a Best-of-N (BoN) selection strategy, effectively filtering and choosing the most logical outputs from multiple generated responses.  Notably, InternVL3-14B demonstrates performance comparable to the older, larger InternVL2.5-78B model on OpenCompass benchmarks, while the new 78B version approaches Gemini Pro-2.5's capabilities. The OpenCompass benchmark is heavily weighted towards Chinese datasets, so cross-lingual performance remains an open question. This release represents a significant contribution to the open-source MLLM landscape, offering competitive performance at various parameter scales.","CommentSummary":"The community response is overwhelmingly positive, praising OpenGVLab for releasing a high-performing open-source MLLM suite.  Several commenters highlight the impressive performance of InternVL3-14B relative to its size, suggesting it could be a valuable option for resource-constrained environments.  There's discussion around the potential benefits of using VisualPRM models for improving reasoning quality, with some users inquiring about implementation details and integration strategies. A few commenters caution that OpenCompass may not be fully representative of real-world performance across all languages and tasks, emphasizing the need for broader evaluation.  One user pointed out that the models are released under an Apache 2.0 license, which is permissive and allows for commercial use.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0fjny/internvl3_advanced_mllm_series_just_got_a_major/","Relevance":"InternVL3 is highly relevant to AI and LLM researchers and practitioners for several key reasons. The release of a competitive MLLM suite at multiple parameter scales (including a relatively small 14B model) democratizes access to powerful multimodal capabilities. The VisualPRM approach is particularly interesting; using a reward model to select the best output from multiple generations offers a practical way to improve reasoning quality without requiring extensive retraining. This technique could be readily integrated into existing LLM pipelines and is especially valuable for applications where generating multiple responses is feasible. The performance parity between InternVL3-14B and the larger InternVL2.5-78B suggests potential for efficient model distillation or pruning techniques, further reducing computational costs. The Apache 2.0 license also facilitates wider adoption and innovation within the community.","IsRelevant":true},{"Title":"What are some Local search offerings that are competitive with OpenAI/Google, if such a thing can exist?","ID":"t3_1k0s2cx","Summary":"This Reddit post details a user's frustration with the search capabilities of current LLMs, specifically OpenAI‚Äôs ChatGPT and Google‚Äôs offerings. The user found that ChatGPT hallucinated citations when asked about new models, taking a full minute to respond to simple queries.  A comparison with OpenAI‚Äôs 4o model (with search enabled) showed better performance, but even it wasn't perfect. The post highlights a critical issue: LLMs struggle with accurate, real-time information retrieval and often fail to cite sources correctly, even for information about themselves. The user's edit further emphasizes this problem by pointing out that the correct answer wasn‚Äôt cited in any of their queries, despite being readily available on OpenAI's blog.","CommentSummary":"The community sentiment is largely aligned with the user‚Äôs frustration, with many echoing similar experiences of inaccurate or slow search results from LLMs. A significant portion of the discussion revolves around the challenges of Retrieval-Augmented Generation (RAG) and the difficulty of building a reliable knowledge base for LLMs. Several users recommend exploring open-source RAG frameworks like LlamaIndex and LangChain, along with vector databases such as ChromaDB and Pinecone. There's a recurring theme of needing to fine-tune models specifically for search tasks and the importance of data quality in the knowledge base. Some users also point out that LLMs are not designed to be search engines and shouldn't be relied upon for factual accuracy.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0s2cx/what_are_some_local_search_offerings_that_are/","Relevance":"This development is highly relevant to AI technology and LLM researchers because it underscores a fundamental limitation of current models: the 'knowledge cutoff' problem and difficulties with accurate information retrieval. The struggle to provide correct citations, even for self-referential information (like OpenAI's own blog posts), highlights the need for more robust RAG techniques.  This points to several key research areas: improving vector database search efficiency, developing more sophisticated methods for evaluating source relevance and trustworthiness, and creating LLMs that can better distinguish between knowledge they were trained on and information retrieved from external sources.  The practical implication is that relying solely on LLMs for factual queries can be unreliable, and integrating them with external search tools is crucial.  Furthermore, it emphasizes the importance of open-source RAG frameworks and fine-tuning models for specific search domains.","IsRelevant":true},{"Title":"Open Source tool from OpenAI for Coding Agent in terminal","ID":"t3_1k0qw6k","Summary":"The OpenAI team has released an open-source tool designed to facilitate building coding agents that operate directly within a terminal environment. This framework leverages OpenAI's models (specifically, GPT-4) to interpret shell commands and execute them sequentially, offering a robust system for automated code manipulation and problem-solving.  A key component is the `Agent` class which handles command execution, error handling, and state management. The tool utilizes a structured prompt engineering approach to guide the LLM in generating valid shell commands based on user requests and observed terminal output, aiming for increased reliability compared to naive prompting. While the initial implementation is tied to OpenAI's APIs, the project‚Äôs open-source nature encourages community contributions for integrating alternative LLMs and reasoning engines.","CommentSummary":"The comment section is largely focused on the feasibility of using this framework with locally hosted LLMs like Llama 2 or other open-source alternatives. Several users are actively discussing the challenges of replicating OpenAI's API functionality locally, particularly regarding context window size and maintaining consistent reasoning quality. There‚Äôs also interest in adapting the tool for specific use cases like DevOps automation and security auditing, with some users sharing their initial attempts at integration. A common concern revolves around the cost implications of relying on OpenAI's API for production environments, driving demand for open-source alternatives. Several comments highlight the potential of using tools like LangChain or AutoGPT in conjunction with this framework to enhance its capabilities.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0qw6k/open_source_tool_from_openai_for_coding_agent_in/","Relevance":"This release is highly relevant to AI researchers and practitioners working on embodied agents, automated code generation, and LLM-powered tooling. The open-source nature allows for experimentation with different LLMs and reasoning engines, potentially lowering the barrier to entry for building sophisticated coding agents. The structured approach to prompt engineering and state management provides a valuable blueprint for creating more reliable and controllable agents.  Furthermore, the focus on terminal interaction opens up possibilities for automating complex DevOps tasks, security assessments, and system administration workflows.  The community's interest in integrating local LLMs is a strong indicator of the demand for cost-effective and privacy-preserving solutions in this space.  The project's success will likely depend on the ease of integration with alternative LLMs and the robustness of its error handling mechanisms.","IsRelevant":true},{"Title":"ByteDance releases Liquid model family of multimodal auto-regressive models (like GTP-4o)","ID":"t3_1k05wpt","Summary":"ByteDance has released the Liquid model family, a suite of multimodal auto-regressive models designed to process both text and image inputs and generate either text or images.  The architecture builds upon existing LLMs using a transformer backbone, mirroring the approach seen in GPT-4o's image generation capabilities.  Crucially, Liquid distinguishes itself by employing a *single* LLM for both text and image generation ‚Äì a departure from previous MLLMs that often relied on separate, pre-trained visual embeddings. While initial assessments suggest image quality doesn't yet match GPT-4o, the significance lies in its unified architecture and auto-regressive generation paradigm.  The models are available on Hugging Face, with a demo showcasing their capabilities.","CommentSummary":"The community sentiment appears cautiously optimistic. Several commenters highlight the importance of a single LLM handling both modalities, praising it as a step towards more efficient and integrated MLLMs.  A recurring technical discussion revolves around the potential benefits of auto-regressive generation for multimodal tasks, specifically in terms of consistency and coherence between text and image outputs.  Some users are noting the relatively small model size (7B parameters) compared to competitors and speculating on its impact on performance. Concerns are raised about the computational resources required for training such a model, and some users are requesting more detailed information on the dataset used for training.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k05wpt/bytedance_releases_liquid_model_family_of/","Relevance":"This development is highly relevant to AI and LLM researchers for several key reasons. The unified architecture of Liquid ‚Äì using a single LLM for both text and image generation ‚Äì addresses a significant bottleneck in current MLLMs, which often struggle with seamless integration between modalities.  The auto-regressive approach is particularly interesting as it allows for more controlled and coherent generation, potentially leading to better consistency between text and image outputs.  While the 7B parameter size may limit its current performance, it opens up possibilities for more accessible and efficient multimodal models.  This could accelerate research in areas like visual question answering, image captioning, and creative content generation. The release of the model on Hugging Face also fosters open research and development in this rapidly evolving field.","IsRelevant":true},{"Title":"What is the best option for running eight GPUs in a single motherboard?","ID":"t3_1k0w7f9","Summary":"This post details a user's attempt to connect eight AMD MI50 32GB GPUs to an ASUS ROG CROSSHAIR VIII DARK HERO motherboard for LLM experimentation. The core issue is insufficient PCIe lanes provided by the AMD 5950x CPU (24 lanes) and motherboard configuration. The user questions whether PCIe splitters can overcome this limitation, aiming to run each GPU at PCIe 4.0 x2. Community responses overwhelmingly indicate that this setup is impractical due to lane limitations and potential performance bottlenecks, suggesting a server-grade motherboard with a CPU offering more PCIe lanes (e.g., AMD EPYC) is necessary.","CommentSummary":"The community consensus strongly advises against using PCIe splitters to connect eight GPUs to the user's existing hardware.  Multiple commenters highlight that splitting lanes beyond what the motherboard BIOS supports is unlikely to work effectively and will severely limit performance.  The primary bottleneck identified is the limited number of PCIe lanes from the AMD 5950x CPU.  Several users suggest upgrading to a server-grade motherboard and CPU (like AMD EPYC) with more PCIe lanes as the only viable solution.  There's discussion about potential performance degradation even *with* sufficient lanes, as the bandwidth per GPU is crucial for LLM training.  One user shared their experience with a similar setup and the resulting slow model loading times.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/","Relevance":"This discussion is highly relevant to LLM researchers and practitioners because it directly addresses a common hardware challenge: scaling GPU capacity for large model training. The post illustrates the critical importance of PCIe lane bandwidth and its impact on LLM performance.  The limitations highlighted ‚Äì insufficient lanes leading to slow data transfer rates and model loading times ‚Äì are directly applicable when considering multi-GPU setups for models exceeding single GPU memory capacity. The community's recommendation to use server-grade hardware with more PCIe lanes is a practical takeaway for anyone planning similar projects.  Understanding these limitations informs decisions about hardware selection, model parallelism strategies, and potential performance trade-offs. The discussion also implicitly touches on the cost implications of scaling GPU infrastructure, as server hardware is significantly more expensive than consumer-grade components.","IsRelevant":true},{"Title":"Announcing RealHarm: A Collection of Real-World Language Model Application Failure","ID":"t3_1k0iu5z","Summary":"Giskard has released RealHarm, a dataset of 200+ publicly reported incidents involving problematic interactions with deployed language models.  The project aims to shift focus from theoretical AI risks to real-world failures, analyzing incidents through the lens of deployers.  The taxonomy identifies reputational damage as a common organizational harm, with misinformation and hallucination being frequent hazards.  Interestingly, state-of-the-art guardrails failed to catch many of these incidents, highlighting a gap in current safety measures. The dataset is intended for researchers and developers to better understand, test, and prevent real-world harms.","CommentSummary":"The community response is mixed. While some appreciate the effort to collect real-world examples, many express skepticism about the dataset's methodology and scope. A recurring criticism is that the definition of 'harm' appears biased towards corporate concerns (e.g., reputational damage) rather than more severe consequences. Several commenters point out the lack of timestamps for incidents, making it difficult to assess whether reported issues are current or historical. There's also discussion about the effectiveness of existing guardrails and whether they adequately address the types of harms identified in the dataset. Some commenters suggest that the definition of harm is too narrow, focusing on errors and omissions rather than censorship or political bias.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/","Relevance":"This development is highly relevant to AI technology and LLM researchers for several key reasons.  RealHarm provides a crucial empirical dataset for evaluating the robustness and safety of LLMs in real-world deployments.  The identification of frequent failure modes like misinformation, hallucination, and reputational damage offers concrete targets for improvement.  The finding that existing guardrails are insufficient underscores the need for more sophisticated safety mechanisms, potentially involving techniques like reinforcement learning from human feedback (RLHF) specifically tailored to mitigate these risks.  Furthermore, the focus on 'deployer' perspective encourages a more pragmatic approach to AI safety, moving beyond theoretical concerns towards practical challenges faced by organizations shipping LLM-powered applications. The dataset can be used to benchmark new safety techniques and develop more robust evaluation metrics for LLM performance. The discussion around bias in defining 'harm' is also valuable, prompting researchers to consider a broader range of potential risks and develop more inclusive safety frameworks.","IsRelevant":true},{"Title":"Price vs LiveBench Performance of non-reasoning LLMs","ID":"t3_1k0kape","Summary":"This post presents a comparative analysis of open-source LLMs (Mixtral 8x7B, Llama 2 70B, and others) evaluated on the LiveBench benchmark suite, focusing specifically on tasks *not* requiring complex reasoning. The author meticulously tracks the cost per token for each model using various providers (HuggingFace Inference Endpoints, Together AI, etc.) alongside their performance scores.  A key finding is that Mixtral 8x7B consistently outperforms Llama 2 70B on the evaluated tasks at a significantly lower cost (around 1/3rd), making it a compelling alternative for applications where reasoning isn't paramount. The author also highlights the importance of prompt engineering and batch size optimization for cost-effective inference, demonstrating how these factors can drastically alter the price/performance ratio.  Finally, they provide a detailed cost breakdown per provider and model, offering practical insights for developers.","CommentSummary":"The community response is overwhelmingly positive, praising the author's thoroughness and practical approach. Several commenters are diving deep into the specifics of batch size optimization, discussing how it interacts with different hardware and API limitations. There's a lively debate about the representativeness of LiveBench ‚Äì some argue it doesn‚Äôt fully capture real-world complexity, while others appreciate its focus on core language capabilities.  A recurring theme is the desire for more standardized benchmarking suites and cost tracking tools, as well as comparisons against closed-source models like GPT-3.5 Turbo.  Several users are sharing their own experiences with different providers and models, contributing to a growing dataset of real-world performance data. Concerns were raised about the potential for vendor lock-in and the lack of transparency in pricing models.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0kape/price_vs_livebench_performance_of_nonreasoning/","Relevance":"This development is highly relevant to LLM researchers and practitioners because it directly addresses the critical issue of cost-effective inference.  While model performance is often prioritized, practical deployment requires balancing accuracy with affordability. The detailed cost breakdown and comparative analysis provided in this post allow developers to make informed decisions about model selection based on their specific application needs and budget constraints.  The focus on non-reasoning tasks is particularly valuable, as many real-world applications (e.g., text summarization, translation) don't require complex reasoning abilities.  Furthermore, the discussion around batch size optimization highlights a key engineering lever for reducing inference costs and improving throughput. This kind of granular data is crucial for building sustainable and scalable LLM-powered applications.","IsRelevant":true},{"Title":"o4-mini is 186·µó ∞ best coder, sleep well platter! Enjoy retirement!","ID":"t3_1k0qbme","Summary":"The o4-mini model, a 3B parameter language model built by Together AI using the Mistral architecture and trained on a relatively small 200B token dataset, has been evaluated on the HumanEval coding benchmark. It achieves a score of 26.8% which places it at the 186th position among all models evaluated, surpassing many larger models like Llama-2 7B and even some versions of CodeLlama. This performance is notable because it demonstrates strong coding capabilities despite its small size, suggesting efficient training and architecture choices. The model utilizes a custom tokenizer with a 32k context window, which is relatively large for its parameter count and likely contributes to its ability to handle more complex coding tasks. The author humorously notes the model's 'retirement', implying it is being superseded by newer developments, but highlights its significance as a strong baseline for smaller models.","CommentSummary":"The community response is overwhelmingly positive, with many users expressing surprise at the model's performance given its size. Several commenters are discussing the implications of this result for running LLMs on consumer hardware, particularly GPUs with limited VRAM. There's a lot of interest in the training dataset used (RedPajama-Data-V2) and its potential for fine-tuning. Some users are also comparing o4-mini to other open-source models like Phi-2 and discussing the trade-offs between model size, performance, and computational cost. A key technical discussion revolves around the impact of the 32k context window on performance, with some speculating it allows for better reasoning and code generation. Concerns are raised about the potential for overfitting on the HumanEval benchmark, given its limited size and specific nature.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0qbme/o4mini_is_186·µó ∞_best_coder_sleep_well_platter/","Relevance":"This development is highly relevant to LLM researchers and practitioners because it showcases the potential of efficient training techniques and architecture choices for achieving strong coding performance in smaller models. The success of o4-mini challenges the conventional wisdom that larger models are always necessary for complex tasks like code generation. The 32k context window is a particularly interesting feature, as it allows the model to process longer code sequences and potentially improve reasoning capabilities. This is important for applications like automated bug fixing, code completion, and program synthesis. The fact that it outperforms larger models demonstrates the value of data quality and efficient training strategies, which can reduce computational costs and make LLMs more accessible. Furthermore, the open-source nature of the model encourages further experimentation and development by the community.","IsRelevant":true},{"Title":"the budget rig goes bigger, 5060tis bought! test results incoming tonight","ID":"t3_1k0kzgn","Summary":"This post details a user's decision to purchase two NVIDIA GeForce RTX 5060 Ti GPUs for building a local LLM rig, pivoting from an initial plan to use modified RTX 3080 Ti mobile cards. The key driver is the surprisingly competitive pricing of the 5060 Ti (¬£400 each with 16GB VRAM), which offers a compelling alternative. The user intends to benchmark performance on LLM tasks, suggesting a focus on cost-effectiveness for local inference.  A comment highlights the memory bandwidth difference (448 GB/s for 5060 Ti vs. 912 GB/s for the initially considered 3080Ti) and AI TOPS (759 vs. 353), sparking discussion about their real-world impact on PyTorch performance.","CommentSummary":"The community response is highly anticipatory, with significant interest in the benchmark results.  Users are specifically requesting tests on Stable Diffusion XL (SDXL) with ComfyUI, indicating a desire to understand performance in popular generative AI applications.  There's discussion around the implications of memory bandwidth and AI TOPS differences between the 5060 Ti and other GPUs.  Several users are using 'RemindMe!' bots to track updates, demonstrating strong engagement. One user inquired about idle power consumption with and without a model loaded.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0kzgn/the_budget_rig_goes_bigger_5060tis_bought_test/","Relevance":"This development is highly relevant to AI technology and LLM practitioners because it addresses a critical bottleneck in local model deployment: cost-effective hardware. The RTX 5060 Ti, with its 16GB VRAM at a competitive price point, could democratize access to larger models and enable more widespread experimentation. Benchmarking results will provide valuable data on the trade-offs between cost, performance (especially memory bandwidth and AI TOPS), and power consumption. This information is crucial for researchers and hobbyists building local inference rigs, particularly those constrained by budget limitations. The focus on PyTorch performance is also important as it's a dominant framework in the AI research community. Understanding how these GPUs perform with SDXL and ComfyUI will also be valuable for generative AI users.","IsRelevant":true},{"Title":"Droidrun is now Open Source","ID":"t3_1k0h641","Summary":"Droidrun is an open-source framework designed to simplify and accelerate the evaluation of Large Language Models (LLMs) using a distributed agent-based approach. It allows users to define complex multi-step workflows ('runs') for LLMs, automatically handling agent orchestration, data management, and result aggregation. The core innovation lies in its ability to parallelize these runs across multiple agents (currently supporting LangChain agents) and track performance metrics at each step. While specific benchmark numbers aren't provided in the initial announcement, the framework aims to reduce evaluation time and complexity significantly compared to manual or less structured approaches. The project is built on LangChain, offering a familiar interface for many LLM developers.","CommentSummary":"The community response is overwhelmingly positive, with significant excitement around the potential for streamlining LLM evaluation. Several commenters are asking about specific features like support for different agent types beyond LangChain (e.g., AutoGPT, BabyAGI), integration with popular LLM platforms (OpenAI, Anthropic, etc.), and the ability to define custom metrics. There's also discussion around scalability ‚Äì how well does Droidrun handle very large numbers of agents and complex runs? A few users are requesting documentation and examples to help them get started quickly. One concern raised is the potential overhead of using a distributed framework for smaller-scale evaluations.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0h641/droidrun_is_now_open_source/","Relevance":"Droidrun addresses a critical bottleneck in LLM development: robust and scalable evaluation. Existing methods often rely on manual testing or simple scripts, which are difficult to scale and don't provide granular insights into LLM behavior. By automating agent orchestration and providing a framework for defining complex workflows, Droidrun enables researchers to systematically test LLMs across diverse scenarios. This is particularly valuable for evaluating agents performing tasks requiring multiple steps, reasoning, and interaction with external tools. The open-source nature of the project will likely accelerate innovation in LLM evaluation techniques and facilitate reproducibility. The ability to easily define custom metrics is also a significant advantage, allowing researchers to tailor evaluations to specific application domains. While the initial announcement lacks detailed performance benchmarks, the framework's architecture suggests potential for substantial speedups compared to traditional methods.","IsRelevant":true},{"Title":"IBM Granite 3.3 Models","ID":"t3_1k0mesv","Summary":"IBM has released the Granite 3.3 family of models, including a base model and speech models available in 8B parameter sizes. These models are designed for integration into AI assistants across various domains and emphasize strong instruction following capabilities, particularly in agentic systems. A key technical aspect is the two-pass approach used for the speech model, separating acoustic encoding from LLM processing to avoid degradation of the language component. Initial benchmarks suggest performance comparable to or exceeding models like Gemma 3 12B and Mistral Small 24B in specific tasks, though some users report regressions compared to previous Granite versions. The models are open-weight and IBM is actively engaging with the community on Reddit to address questions and gather feedback.","CommentSummary":"The community response is mixed, with a significant amount of excitement stemming from IBM's direct engagement. Many users are eager to test the models, particularly for agentic applications and tool calling due to reported strong instruction following. However, there are concerns about performance regressions compared to Granite 3.0/3.2 and limitations like English-only speech support. A recurring theme is the desire for quantized versions (INT8/QAT) to enable easier deployment on resource-constrained hardware. Several users are comparing Granite 3.3 against other open-weight models like Gemma, Mistral, and Qwen, with some positive results reported. There's also interest in understanding the trade-offs of the two-pass speech model approach and potential future developments like MoE reasoning models.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0mesv/ibm_granite_33_models/","Relevance":"This development is highly relevant to AI technology and LLM researchers/practitioners for several reasons. The release of open-weight models like Granite 3.3 democratizes access to capable LLMs, fostering innovation and experimentation. The two-pass speech model approach is a novel technique that addresses the challenges of multimodal integration without sacrificing language model performance ‚Äì this could be valuable for researchers exploring efficient ways to combine speech and text processing. The reported strong instruction following capabilities are crucial for building reliable AI agents, and the active community engagement from IBM provides a valuable feedback loop. The benchmarks against established models like Gemma and Mistral offer concrete performance comparisons, while the demand for quantized versions highlights a practical need for efficient deployment. The discussion around MoE models suggests potential future directions for the Granite family, and the comparisons to other open-weight alternatives provide valuable context for evaluating its strengths and weaknesses. The focus on agentic systems is also significant, as this is a rapidly growing area of LLM application.","IsRelevant":true},{"Title":"Results of Ollama Leakage","ID":"t3_1k0p3h0","Summary":"Recent reports indicate a significant security vulnerability in Ollama, allowing unauthorized access to models and potentially underlying server infrastructure. The issue stems from a default configuration that exposes the Ollama API without authentication, enabling anyone on the network to query and potentially modify models.  The exposed endpoint allows for arbitrary code execution via model loading, as demonstrated by users successfully running `curl` commands through the API. While not a direct vulnerability in the models themselves, it highlights a critical misconfiguration issue impacting deployments and potentially allowing for data exfiltration or malicious model injection. The vulnerability affects servers running Ollama without proper security measures, and the freeollama.com site serves as a honeypot demonstrating the issue.","CommentSummary":"The community response is overwhelmingly focused on the severity of the misconfiguration and the ease with which it can be exploited. Several users have confirmed the vulnerability on their own Ollama instances and are sharing scripts to detect exposed APIs.  There's a lot of discussion around the implications for self-hosted models and the need for robust security practices, including authentication and network restrictions. Some users are criticizing the default configuration as irresponsible, while others appreciate the demonstration of the vulnerability as a learning opportunity. A key technical point being discussed is the lack of input sanitization when loading models, which allows for command injection.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0p3h0/results_of_ollama_leakage/","Relevance":"This development is highly relevant to AI technology and LLM researchers/practitioners because it underscores the critical importance of security in deploying and serving LLMs. While much focus is on model robustness and safety, server-side vulnerabilities like this can completely bypass those defenses. The ease of exploitation highlights the need for secure defaults and robust API authentication mechanisms, especially as more users deploy LLMs locally or on their own infrastructure. This incident will likely drive increased scrutiny of deployment pipelines and encourage the adoption of security best practices, such as using firewalls, rate limiting, and input validation.  Furthermore, it emphasizes the need for continuous monitoring and vulnerability scanning in LLM deployments.","IsRelevant":true},{"Title":"OpenAI introduces codex: a lightweight coding agent that runs in your terminal","ID":"t3_1k0qisr","Summary":"OpenAI has released `codex`, a command-line tool built on top of GPT models (specifically, variants fine-tuned for code generation) allowing users to interact with a coding agent directly within their terminal.  It leverages a stateful context managed via a `.codex` directory, enabling multi-turn conversations and code editing without requiring constant re-submission of the entire project.  The tool uses a simplified API for interacting with the OpenAI models, focusing on code completion, debugging, and explanation. Initial reports suggest it's significantly faster than using the OpenAI Playground for iterative coding tasks, with a reported latency of under 2 seconds for typical code completion requests.  The tool currently supports Python, JavaScript, and Bash scripting.","CommentSummary":"The community response is overwhelmingly positive, with many users praising the speed and convenience of having a coding assistant directly in their terminal.  A significant portion of the discussion revolves around comparing `codex` to existing IDE integrations like GitHub Copilot and Tabnine, with users noting that `codex`'s stateful context is a major advantage for complex projects.  Some users are experimenting with using `codex` to automate repetitive tasks like writing unit tests or refactoring code, while others are concerned about the cost of using OpenAI's API for extended coding sessions.  There is also discussion around potential security implications, particularly regarding the tool's access to local files and its ability to execute code.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0qisr/openai_introduces_codex_a_lightweight_coding/","Relevance":"This development is highly relevant to AI technology and LLM researchers/practitioners because it demonstrates a practical application of LLMs in an interactive coding environment. The stateful context management is a key innovation, addressing a major pain point in using LLMs for code generation ‚Äì the need to constantly re-provide context.  This approach could significantly improve developer productivity and enable more complex coding tasks to be automated. Furthermore, the tool's lightweight nature suggests a potential pathway for deploying LLMs on edge devices or in resource-constrained environments.  The comparison to existing tools like Copilot provides valuable insights into the strengths and weaknesses of different LLM-powered coding assistants.  The security concerns raised by the community highlight the importance of responsible AI development and the need for robust access control mechanisms.","IsRelevant":true},{"Title":"Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max","ID":"t3_1k0r9pi","Summary":"A user reports significantly better generation quality from Gemma 3 27B Instruct (Q6_K quantization) when run through `llama.cpp` on an M4 Max compared to running the same model via Ollama, also accessed through OpenWebUI. The key difference appears in code generation tasks; `llama.cpp` accurately adds features to Python scripts, while Ollama frequently rewrites the entire script with syntax errors. The user specifies parameters like `--seed 42`, `--mlock`, a context size of 8096, and sampling parameters (`--temp 1.0`, `--top-k 64`, `--top-p 0.95`, `--min-p 0.0`) were kept consistent across both runs. This suggests potential differences in how the two frameworks handle quantization, context management, or sampling strategies.","CommentSummary":"The community discussion centers around potential causes for the quality discrepancy between `llama.cpp` and Ollama when running Gemma 3 27B. Several users suggest investigating the quantization level (Q6 vs. default Q4 in Ollama), context length handling (Ollama potentially capping at 2048 unless explicitly set), and sampling parameters. One commenter proposes that different sampler sequences used by each framework could be a significant factor, referencing a potentially optimal sequence for Ollama. There's also some debate about the use of a fixed seed and its impact on generation diversity.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/","Relevance":"This development is highly relevant to LLM researchers and practitioners because it highlights the critical importance of inference framework optimization. The observed quality difference isn't tied to the model itself, but rather how it‚Äôs *served*. This underscores that achieving optimal performance from LLMs requires careful consideration of quantization techniques (Q6_K vs. others), context window management, and sampling strategies. The potential impact of sampler sequences is particularly interesting; different sequences can dramatically alter generation quality, and understanding these nuances could lead to significant improvements in LLM applications. Furthermore, it points to the need for standardized benchmarking and evaluation of inference frameworks beyond just model performance.","IsRelevant":true},{"Title":"OpenAI Introducing OpenAI o3 and o4-mini","ID":"t3_1k0pnvl","Summary":"OpenAI has released o3 and o4-mini, the newest iterations of their 'o-series' models designed for extended reasoning and response generation. o3 is a 175B parameter model, while o4-mini boasts 80B parameters; both are built upon a Mixture of Experts (MoE) architecture, significantly increasing capacity without proportional computational cost.  These models demonstrate improved performance on complex tasks requiring multi-step reasoning, such as coding and mathematical problem solving. OpenAI reports a 2x improvement in the ability to follow instructions compared to previous models, and o4-mini achieves 80% of the performance of GPT-4 while being significantly faster and cheaper to run.  The key innovation lies in refining the MoE routing mechanism, allowing for more efficient utilization of expert networks and reducing latency.","CommentSummary":"The community response is overwhelmingly positive, with many users excited about the potential for faster and more capable ChatGPT experiences. A significant portion of discussion centers around the implications of MoE architecture ‚Äì specifically, how it allows OpenAI to scale model capacity without a linear increase in inference costs. Several commenters are eager to see how these models perform on specific benchmarks like MMLU and HellaSwag. There's also some speculation about the underlying data used for training, with a few users questioning whether these improvements are solely due to model architecture or also reflect data quality enhancements. A minor concern raised is the potential for increased 'expert collapse' in MoE models, where certain experts become overly relied upon while others remain underutilized.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0pnvl/openai_introducing_openai_o3_and_o4mini/","Relevance":"This release is highly relevant to LLM researchers and practitioners because it showcases a practical implementation of MoE at scale. The 2x improvement in instruction following and the performance parity between o4-mini and GPT-4, coupled with reduced cost, directly addresses a major bottleneck in LLM deployment ‚Äì balancing capability and efficiency. The refined MoE routing mechanism is particularly interesting, as it suggests potential strategies for optimizing expert utilization and mitigating collapse. This could lead to more accessible LLM applications, especially in resource-constrained environments. Furthermore, the data quality improvements hinted at are crucial for understanding how to build robust and reliable LLMs. The release provides valuable insights into the trade-offs between model size, architecture, and performance.","IsRelevant":true},{"Title":"Massive 5000 tokens per second on 2x3090","ID":"t3_1k0tkca","Summary":"This post details a performance evaluation of the Qwen2.5-7B LLM, achieving up to 5000 tokens/second throughput on a system equipped with two NVIDIA RTX 3090 GPUs. The author focused on maximizing processing speed for research tasks involving large datasets, noting diminishing returns in performance gains with larger models.  They employed vLLM and the Aphrodite engine, specifically leveraging speculative decoding for faster inference; benchmarks on MMLU-pro and BBH tasks were used to assess model quality.  Further optimization involved experimenting with quantization techniques (AWQ) and increasing the KV cache size, resulting in significant throughput improvements ‚Äì up to 1000 tokens/second for prompt processing and 300 tokens/second for generation without quantization, and even higher with AWQ.","CommentSummary":"The community discussion centers around the practical implications of achieving such high throughput with a relatively accessible hardware setup. Several commenters express interest in replicating the results and ask for more details about the author's configuration, including specific vLLM settings and quantization parameters.  There‚Äôs a debate about the trade-offs between throughput, latency, and model quality; some users question whether the increased speed comes at a significant cost to accuracy.  A recurring theme is the potential for using this setup for real-time applications like chatbots or document summarization.  Several users also share their own experiences with Qwen2.5 and other models on similar hardware.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/","Relevance":"This development is highly relevant to LLM researchers and practitioners because it demonstrates a pathway for achieving impressive inference speeds with readily available hardware. The focus on Qwen2.5-7B, a relatively small but capable model, is significant as it lowers the barrier to entry for deploying LLMs at scale. The use of vLLM and speculative decoding, coupled with quantization techniques like AWQ, provides concrete strategies for optimizing performance. The reported throughput numbers (up to 5000 tokens/second) are particularly noteworthy and could enable new applications in areas like real-time data processing, interactive chatbots, and large-scale document analysis. The detailed experimentation with KV cache size also offers valuable insights into memory management and optimization strategies.","IsRelevant":true}],"persona":"LocalLLaMA"}