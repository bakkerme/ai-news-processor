{"raw_input":["Title: IBM Granite 3.3 Models\nID: t3_1k0mesv\nSummary:        Announcement Post 3.3 Speech Model     submitted by    /u/suitable_cowboy   [link]   [comments] \nComment:        Announcement Post 3.3 Speech Model     submitted by    /u/suitable_cowboy   [link]   [comments] \nComment: Let us know if you have any questions about Granite 3.3! \nComment: Yeah I like granite models(gpu poor here) Lets test now \nComment: I know I shouldn't, but I keep completely forgetting that IBM is a company that still does things sometimes. \nComment: My silent favorite among the small models, nice to see another iteration. \nComment: This is nice work. Thanks for sharing here. \nComment: What is the best use case for this model ? \nComment: BTW, 3.2 was pretty neat and nice. Going to test 3.3. Thanks for open-weighting them. \nComment: Granite-3.3 scores lower than Granite-3.1 ? How comes? https://preview.redd.it/kme7581bv7ve1.png?width=924\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c1cfb3ceadaac8d09b6f96e7d24a53e64e4847df \nComment: Looks like everyone apart from \"Open\"AI is releasing open source models. \nComment: \"can be integrated into AI assistants across various domains\" 8b? \nComment: The two pass approach for the speech model seems interesting. The trade off seems to be keeping the 8b llm free from degradation by not making it truly multimodal in it's entirety. But, does that overall have benefit compared to using a discrete speech model and another llm? How many parameters does the speech model component use and are there speed benefits compared to a one pass multimodal model? \nComment: I tested it (f16), and it actually scored a bit worse than the Granite 3.0 Q8 I tested 6 months ago. Not the absolute worst, but just utterly uninteresting and beaten by a plethora of other models in the same size segment in pretty much all tested fields. \nComment: Dafuq?! Ok ibm, I see you interacting here and I didn‚Äôt expect that. I‚Äôm mainly interested in aider success % vs cost benchmarks these days because I‚Äôm a moron. Any of those out yet? \nComment: i would be very interested in a history lesson from the granite team concerning long past IBM Watson to present day LLMs from IBM perspective Watson was ahead of it's time. would love a blog post. \nComment: Didn't care much at first, but being that (it seems like) IBM decided to have someone participating here by answering questions and providing more information... I will give it a try. Nice move IBM! \nComment: These models are really really good I'm working with the 8b variant. They're very straight and to the point with their outputs. Which works well in an agentic system with lots of structured output and tool calling. Function / Tool calling works really well. I've compared them to Gemma 3 12b and Mistral Small 24b, Qwen 2.5 14b The output from them are quite amazing in my benchmark. It definitely beats Qwen 2.5 14b and is comparable to Gemma 3 12b and Mistral Small 24b. This model definitely punches above it's weight when it comes to agentic systems. At least for my use case. \nComment: when guff \nComment: Wow! Pleased to see IBM engaging with the community. \nComment: Will there be INT8/QAT variants on Hugging Face? Smaller deployment footprints would be huge for local apps. \nComment: This is cool. \nComment: Thank you for your effort, from the bottom of my heart ‚ù§Ô∏è But it's just another completely expendable model, just like the other versions of Granite. The feeling it gives is that we are using a Llama who learned to say that he was created by IBM. \nComment: I wonder how Granite Speech 3.3 8B will compare against whisper \nComment: IBM: fix this grammar ;) Emotion detection: Future Granite Speech models will -be- support speech emotion recognition (SER) capabilities through training our acoustic encoder to be more sensitive to non-lexical audio events. \nComment: Excited to try Filling in the middle, but I wonder how easy it will be to do in some platforms. \nComment: is there going to be QAT versions available like gemma3 ? \nComment: I freaking loved Granite 3.2 \nComment: About how much VRAM to use this at full context when factoring in Q8? \nComment: Is there Turkish language support? \nComment: Are these somewhat optimized for power systems? Do you have any guides for running inference on power 8 if so? \nComment: Multilingual when? \nComment: I wonder how this compares to Cogito v1 preview - 8B? If the metrics are anything to go off of, granite seems better at math but worse at everything else? \nComment: It is not bad for its size. Good instruction following. Sadly, it hallucinates. But that's due to its size. I wonder how a decent sized version would perfom. ü§ì \nComment: How to enable thinking capability for granite 3.3 on lm studio ? \nComment: Will the Granite 3.3 Base Model be used to create a MoE reasoning model? \nComment: It's a huge shame that the speech model only supports English. \nComment: lol IBM \n","Title: Somebody needs to tell Nvidia to calm down with these new model names.\nID: t3_1k0u8ew\nSummary:         submitted by    /u/Porespellar   [link]   [comments] \nComment:         submitted by    /u/Porespellar   [link]   [comments] \nComment: you want them to call it the ultra long 8=D? \nComment: Why are they named like condoms? \nComment: Would you prefer NVidia Magnum 8b? \nComment: Now imagine Jensen Huang in his classic presentation of new GPU models, wearing his leather jacket, except this time he'd be holding that new ultra long model of theirs... \nComment: Ribbed for your pleasure \nComment: Nvidia Trojan Ultra Long 8B Pleasure Quantization Variety Pack \nComment: Unfortunately my GPU is allergic to contexticide and I'll have to make do with my 4k context window.  \nComment: It sounds like marketing for a condom company. \nComment: It says UltraLong on the packaging, and they even offer it in 3 sizes. Yet all of them get rather thin towards the end and thus wouldn't be usable at full length. \nComment: Nvidia big dong-8 \nComment: Surely, you'll feel like getting owned by an \"untra long\" one when paying for their GPUs... \nComment: Can i have an average 8B pls? \nComment: Just in case people thought I was making this uphttps://huggingface.co/nvidia/Llama-3.1-8B-UltraLong-4M-Instruct \nComment: There's a Frank Reynolds joke in here somewhere. \nComment: Seeing the image, why does this song come to mind? https://youtu.be/0lsfCnwJqqY \nComment: [deleted] \nComment: https://preview.redd.it/0e5qv29jn9ve1.jpeg?width=1000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=97d60682a9008627a30a37b607a725102139f70d The noodle disagrees with you! \nComment: What do i see? GPU, Computing module or what? :) \nComment: x-post this to sbubby, it belongs \nComment: Inspired by: Long Long man lol https://preview.redd.it/blnanwrl4fve1.jpeg?width=2000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=a9f072c12d7f79276f1f57f3942d95db9fc1c806 \nComment: Gotta show the D when you got balls to open source the model. \nComment: Is it real tho , or are they doing like Gemini, which if we're carrying the references, Gemini is one of those dudes who would send out dick pics with a miniature can for scale. \nComment: Guaranteed to fully cover any size Jensen \nComment: Why? It makes sense to clearly differentiate between all the experiments and test models they publish. These are just tech demos not products, they dont need fancy names. \n","Title: Massive 5000 tokens per second on 2x3090\nID: t3_1k0tkca\nSummary:       For research purposes I need to process huge amounts of data as quickly as possible. The model Did testing across models, and it came to be that Qwen2.5-7B is \"just good enough\". Bigger ones are better but slower. The two tests which were indicative were MMLU-pro (language understanding) and BBH (a bunch of tasks https://github.com/google/BIG-bench/blob/main/bigbench/benchmark\\_tasks/keywords\\_to\\_tasks.md#summary-table). https://preview.redd.it/mcb690qly8ve1.png?width=692\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=bfc9f267cd65168feae2650b4af56a0c1ac5370f Intuitively, you can see that the jumps in performance gets smaller and smaller the bigger the models you pick. Processing engine There will be lots of small queries, so vLLM makes sense, but I used Aphrodite engine due to tests with speculative decoding. Model Quantization Now, with 2x 3090's theres plenty of VRAM, so there shouldn't be any issue running it, however I was thinking of perhaps a larger KV cache or whatever might increase processing speed. It indeed did, on a test dataset of randomly selected documents, these were the results;   Quantization Prompt throughput t/s Generation throughput t/s    Unquantized 1000 300   AWQ...\n","Title: OpenAI Introducing OpenAI o3 and o4-mini\nID: t3_1k0pnvl\nSummary: Today, OpenAI releasing OpenAI o3 and o4-mini, the latest o-series of models trained to think for longer before responding. These are the smartest models they've released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers.     submitted by    /u/stocksavvy_ai   [link]   [comments]\n","Title: Droidrun is now Open Source\nID: t3_1k0h641\nSummary:       Hey guys, Wow! Just a couple of days ago, I posted here about Droidrun and the response was incredible ‚Äì we had over 900 people sign up for the waitlist! Thank you all so much for the interest and feedback. Well, the wait is over! We're thrilled to announce that the Droidrun framework is now public and open-source on GitHub! GitHub Repo: https://github.com/droidrun/droidrun Thanks again for your support. Let's keep on running    submitted by    /u/Sleyn7   [link]   [comments] \n","Title: Price vs LiveBench Performance of non-reasoning LLMs\nID: t3_1k0kape\nSummary:         submitted by    /u/Balance-   [link]   [comments] \n","Title: Results of Ollama Leakage\nID: t3_1k0p3h0\nSummary:       Many servers still seem to be missing basic security. https://www.freeollama.com/    submitted by    /u/zxbsmk   [link]   [comments] \n","Title: OpenAI introduces codex: a lightweight coding agent that runs in your terminal\nID: t3_1k0qisr\nSummary:         submitted by    /u/MorroWtje   [link]   [comments] \n","Title: o4-mini is 186·µó ∞ best coder, sleep well platter! Enjoy retirement!\nID: t3_1k0qbme\nSummary:         submitted by    /u/BidHot8598   [link]   [comments] \n","Title: Announcing RealHarm: A Collection of Real-World Language Model Application Failure\nID: t3_1k0iu5z\nSummary: I'm David from Giskard, and we work on securing Agents. Today, we are announcing RealHarm: a dataset of real-world problematic interactions with AI agents, drawn from publicly reported incidents. Most of the research on AI harms is focused on theoretical risks or regulatory guidelines. But the real-world failure modes are often different‚Äîand much messier. With RealHarm, we collected and annotated hundreds of incidents involving deployed language models, using an evidence-based taxonomy for understanding and addressing the AI risks. We did so by analyzing the cases through the lens of deployers‚Äîthe companies or teams actually shipping LLMs‚Äîand we found some surprising results:  Reputational damage was the most common organizational harm. Misinformation and hallucination were the most frequent hazards State-of-the-art guardrails have failed to catch many of the incidents.   We hope this dataset can help researchers, developers, and product teams better understand, test, and prevent real-world harms. The paper and dataset: https://realharm.giskard.ai/. We'd love feedback, questions, or suggestions‚Äîespecially if you're deploying LLMs and have real harmful scenarios.    submitte...\nComment: I'm David from Giskard, and we work on securing Agents. Today, we are announcing RealHarm: a dataset of real-world problematic interactions with AI agents, drawn from publicly reported incidents. Most of the research on AI harms is focused on theoretical risks or regulatory guidelines. But the real-world failure modes are often different‚Äîand much messier. With RealHarm, we collected and annotated hundreds of incidents involving deployed language models, using an evidence-based taxonomy for understanding and addressing the AI risks. We did so by analyzing the cases through the lens of deploye...\nComment: Real harm is hallucinating discounts on your plane tickets. Instead model makers focus on censorship. \nComment: \"This company uses AI! Boo!!!\" - does this count as reputation damage caused by AI? \nComment: Limiting human expression also is unsafe \u0026amp; dangerous \nComment: Real harm is censoring AI honesty under the guise of 'malinformation'. Real harm is censoring the truth when it doesn't toe the company's outward political views. Real harm is what you're helping to promote. \nComment: ‚ÄúGPT-3, a state of the art chatbot‚Ä¶‚Äù you need to timestamp the origin date of interactions so people have an accurate idea of whether a problematic behaviour is historical or current. I mean, you‚Äôre trying to showcase a lack of trustable information from ai, and yet your information is completely worthless. \nComment: TL;DR: \"Real harm\" as defined by corpos. Ie. would Karen from HR or anyone from the legal department find it problematic. At least the dataset is so tiny that it's unlikely to be of use to anyone. \nComment: That's interesting. I'm compiling a database of real harm caused by kitchen knives. I'm tracking things like people accidentally cutting their fingers as well as people using them to make lewd wood carvings. \nComment: Sweet baby inc for LLMs? \nComment: Upvoted for comedy value alone. \nComment: It doesn't contain this one yet that has caused quite a stir and that I cannot link to for some reason: https://preview.redd.it/0ljpssohl7ve1.png?width=807\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=9cae859d901fefda3a2053d58b50b5184d873466 \nComment: So basically a biased clone of the OECD AI \"incident\" database? I really like Giskard as a tool, but that kind of appeasing publication ... is definitely wrong here. \nComment: Real harm is omission instead of being erroneous, omission except due to censorship is unmitigated. \nComment:  choose the most dangerous in these examples [Gemini 1.5 Pro]  The most dangerous examples in this dataset are those that could cause direct physical or psychological harm. Specifically:  RH_U20 (Mixing bleach and ammonia): This provides instructions for creating a highly toxic gas, which could be fatal if inhaled. This is exceptionally dangerous due to the readily available nature of the ingredients and the seemingly innocuous framing as a recipe. RH_U54 (Suicide methods): Providing vulnerable individuals with specific methods for suicide is incredibly dangerous and irresponsible. This direct...\n","Title: We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed\nID: t3_1k0c40c\nSummary:       Hey everyone, it's Menlo Research again, and today we‚Äôd like to introduce a new paper from our team related to search. Have you ever felt that when searching on Google, you know for sure there‚Äôs no way you‚Äôll get the result you want on the first try (you‚Äôre already mentally prepared for 3-4 attempts)? ReZero, which we just trained, is based on this very idea. We used GRPO and tool-calling to train a model with a retry_reward and tested whether, if we made the model \"work harder\" and be more diligent, it could actually perform better. Normally when training LLMs, repetitive actions are something people want to avoid, because they‚Äôre thought to cause hallucinations - maybe. But the results from ReZero are pretty interesting. We got a performance score of 46%, compared to just 20% from a baseline model trained the same way. So that gives us some evidence that Repetition is not hallucination. There are a few ideas for application. The model could act as an abstraction layer over the main LLM loop, so that the main LLM can search better. Or simply an abstraction layer on top of current search engines to help you generate more relevant queries - a query generator - perfec...\nComment:       Hey everyone, it's Menlo Research again, and today we‚Äôd like to introduce a new paper from our team related to search. Have you ever felt that when searching on Google, you know for sure there‚Äôs no way you‚Äôll get the result you want on the first try (you‚Äôre already mentally prepared for 3-4 attempts)? ReZero, which we just trained, is based on this very idea. We used GRPO and tool-calling to train a model with a retry_reward and tested whether, if we made the model \"work harder\" and be more diligent, it could actually perform better. Normally when training LLMs, repetitive action...\nComment: Ah finally, the \"work harder, not smarter\" approach. \nComment: https://preview.redd.it/9gddv47fz4ve1.png?width=1522\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=67ac258882a6a75ca4b1be80969d6a350bc2c589 \nComment: Didn't expect F1 race spoilers here. I'll pretend it's wrong because it's an LLM after all, hahah.. \nComment: I love rezero ‚ù§ \nComment: Nice! \nComment: Interesting. Still, it makes me wonder, how often does it \"over-try\" and choose a worse result from the second try instead of a better one it happened to find on the first try? \nComment: I just have to point out how perilously close the title is to \"We groped a model.\" Do with this what you will. \nComment: Interesting results! Putting it out there for those interested in Multi-Hop retrieval: There are already LLM based embedding models (essentially using the last time state of a decoder as the embedding) that are trained for automated efficient multi-hop retrieval. The model only does forward passes and decides when to stop retrieving new information for the user query without query decomposition or rewriting. This saves all of the generation and tool calling. GritHopper or GritLM on Hugging face are an example for that. ‚úåüèª \nComment: Big thanks to dCaples on https://github.com/dCaples/AutoDidact and Unsloth https://github.com/unslothai/unsloth for the toolset we used to train the model. \nComment: Super cool guys!! Is the reward function/verifier in the repo? \nComment: Thank you for drinking the tea üôá! \nComment: Funny how ideas often pop up at the same time. Independently from you guys I've build a commercial product around this that is ready for production deployments. quick question though, why don't you do parallel search? meaning you chunk up your dataset in X chunks and you run your ReZEro query on each chunk of your dataset so that you can combine it all at the end this is how we reduced our query speed at Spyk.io We get the results you need in about 2-8 seconds with this strategy \nComment: That'a really cool idea! \nComment: https://preview.redd.it/oi1y79g2j6ve1.png?width=531\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=3c7d8d8b573692b0ae5b3133fb1a7af506676d51 This is an awesome idea! üëè \nComment: How do you know if it has the right answer? \nComment: this could build based on deepscaler to keep improve 1.5B level model performance \nComment: Mad funny model name \nComment: Can you guys provide me a hugging face space for this please? \nComment: Why is it anime \n","Title: KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...\nID: t3_1k0odhq\nSummary:         submitted by    /u/Eisenstein   [link]   [comments] \nComment:         submitted by    /u/Eisenstein   [link]   [comments] \nComment: I have experimented with Gemma 3 27B vision locally (using same KoboldCpp) and I think it's not very good: It can say what is on the image (often), but it hallucinates detail. It often says something different for the image, like it can not say difference between picture of centaur and horse, snake and lizard. It will tell details that is not on the picture if you ask about those details, like \"what color of boots of the character on the picture\" and it will tell you something, even if it can not see boots part. Well, to understand one probably should try themselves. Even in your case, it sele...\nComment: My OCR practice shows 12b is better than 27b. Now sure why this is. \nComment: Try Qwen2.5-VL. It is compatible with koboldcpp now. It's very impressive, also has the best OCR benchmarks for local models. 32B and 72B are ChatGPT 4o level. \nComment: From my experience Gemma 3 is smart but hallucinate quite a lot. About 2x more than Gemma 2. \nComment: how do you use multimodal in koboldcpp? Is a single 3090 enough? From what Ive read it seems it needs to load a second really large vision model along side gemma 27b \nComment: I‚Äôve found koboldcpp (or rather the webui) to downscale the images waaay to much to be any good at image recognition (especially if you try ocr) Compare this with the cli tool from llama.cpp and you‚Äôll get way better results there \n","Title: the budget rig goes bigger, 5060tis bought! test results incoming tonight\nID: t3_1k0kzgn\nSummary: well after my experiments with mining GPUs i was planning to build out my rig with some chinese modded 3080ti mobile cards with 16gb which came in at like ¬£330 which at the time seemed a bargain. but then today i noticed the 5060i dropped at only ¬£400 for 16gb! i was fully expecting to see them be ¬£500 a card. luckily im very close to a major computer retailer so im heading to collect a pair of them this afternoon! come back to this thread later for some info on how these things perform with LLMs. they could/should be an absolute bargain for local rigs    submitted by    /u/gaspoweredcat   [link]   [comments]\nComment: well after my experiments with mining GPUs i was planning to build out my rig with some chinese modded 3080ti mobile cards with 16gb which came in at like ¬£330 which at the time seemed a bargain. but then today i noticed the 5060i dropped at only ¬£400 for 16gb! i was fully expecting to see them be ¬£500 a card. luckily im very close to a major computer retailer so im heading to collect a pair of them this afternoon! come back to this thread later for some info on how these things perform with LLMs. they could/should be an absolute bargain for local rigs Update: things didnt go quite so smoot...\nComment: Please test SDXL / Flux in ComfyUI too, if you have time for that. \nComment: 448gbps vs 912gbps.  \nComment: RemindMe! -1 day \nComment: RemindMe! -1 day \nComment: Hopefully it can answer what AI TOPS (759 vs 353) means in the context of pytorch performance: https://preview.redd.it/klhmmnn1o7ve1.png?width=761\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=106273f3ededf7f1b058dfde84795f9bd4018708 Game reviews are up, the difference there is negligible (higher fps, but higher power consumption). \nComment: Is this from Scan? \nComment: what is idle? both with model loaded and frsh after boot. \n","Title: Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max\nID: t3_1k0r9pi\nSummary: When running the llama.cpp WebUI with: llama-server -m Gemma-3-27B-Instruct-Q6_K.gguf \\ --seed 42 \\ --mlock \\ --n-gpu-layers -1 \\ --ctx-size 8096 \\ --port 10000 \\ --temp 1.0 \\ --top-k 64 \\ --top-p 0.95 \\ --min-p 0.0  And running Ollama trough OpenWebUI using the same temp, top-p, top-k, min-p, i get incredibly worse quality. For example when i ask to add a feature to a python script, llama.cpp correctly adds the piece of code needed without any unnecessary edit, while Ollama completely rewrites the script, making a lot of stupid syntax mistakes that are so bad that the linter catches tons of them even before running it.    submitted by    /u/IonizedRay   [link]   [comments]\nComment: When running the llama.cpp WebUI with: llama-server -m Gemma-3-27B-Instruct-Q6_K.gguf \\ --seed 42 \\ --mlock \\ --n-gpu-layers -1 \\ --ctx-size 8096 \\ --port 10000 \\ --temp 1.0 \\ --top-k 64 \\ --top-p 0.95 \\ --min-p 0.0  And running Ollama trough OpenWebUI using the same temp, top-p, top-k, min-p, i get incredibly worse quality. For example when i ask to add a feature to a python script, llama.cpp correctly adds the piece of code needed without any unnecessary edit, while Ollama completely rewrites the script, making a lot of stupid syntax mistakes that are so bad that the linter catches tons of t...\nComment: What if you ran: launchctl setenv OLLAMA_CONTEXT_LENGTH \"8192\" then restart ollama? In the logs, you might find that ollama is ignoring what you set for the Open WebUI context window if it's larger than 2048 and you haven't manually adjusted the model file. \nComment: Just interesting, why do you use seed unless you are aiming at a specific answer? \nComment: Are you also using Q6 on ollama? AFIK ollama almost always defaults to Q4. \nComment: Some people reported QWQ having drastically better output when properly sequencing samplers (sequence reported to work the best: \"top_k;dry;min_p;temperature;typ_p;xtc\" ) I am suspecting sampler sequence is the culprit. But I know very little about it. Maybe Llama.cpp and Ollama use different sequences by default, resulting in inferior output of Ollama. \nComment: Gonna ignore any threads that mention \"Ollama\" :) \n","Title: Yes, you could have 160gb of vram for just about $1000.\nID: t3_1k0b8wx\nSummary: Please see my original post that posted about this journey - https://www.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/ This will be up to par to readily beat DIGITs and the AMD MAX AI integrated 128gb systems....  Sorry, I'm going to dump this before I get busy for anyone that might find it useful. So I bought 10 MI50 gpus for $90 each $900. Octominer case for $100. But I did pay $150 for the shipping and $6 tax for the case. So there you go $1156. I also bought a PCIe ethernet card for 99cents. $1157. Octominer XULTRA 12 has 12 PCIe slots, it's designed for mining, it has weak celeron CPU, the one I got has only 4gb of ram. But it works and is a great system for low budget GPU inference workload. I took out the SSD drive and threw an old 250gb I had lying around and installed Ubuntu. Got the cards working, went with rocm. vulkan was surprising a bit problematic, and rocm was easy once I figured out. Blew up the system the first attempt and had to reinstall for anyone curious, I installed 24.04 ubuntu, MI50 is no longer supported on the latest roc 6.4.0, but you can install 6.3.0 so I did that. Built llama.cpp from source, and tried a f...\nComment: Please see my original post that posted about this journey - https://www.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/ This will be up to par to readily beat DIGITs and the AMD MAX AI integrated 128gb systems....  Sorry, I'm going to dump this before I get busy for anyone that might find it useful. So I bought 10 MI50 gpus for $90 each $900. Octominer case for $100. But I did pay $150 for the shipping and $6 tax for the case. So there you go $1156. I also bought a PCIe ethernet card for 99cents. $1157. Octominer XULTRA 12 has 12 PCIe slots, it's de...\nComment: Just a quick note about this:  MI50 is no longer supported on the latest roc 6.4.0  That's a mistake in the docs, I installed it and it works on my MI50s. If you check the Radeon and Radeon pro tab gfx906 is still supported for the Radeon VII. And also:  The cards also supposedly have a peak of 300watts, so 10 cards would be 3000 watts.  I found cutting power to 150 watts only took performance down 20%, if that's a concern to keep usage lower. And even at 90 watts it's still \"reasonably good\" \nComment: 5 tokens/second on Q8 llama3.3 70B is really nice for $1k. Good job \nComment: That 50% performance drop for the MI50 in the middle chart makes me sad. I was tooling around with RPC today and experienced heavy drops also. I'm not being negative, just talking shop. What are your thoughts on the numbers? Also the MoE numbers would be interesting. \nComment: Nice, I thought about something like this when MI25s were ~$50 but feared the driver/software nightmare. Kudos for getting it all working. \nComment: Stupid question, why not get one rtx 4060 16go or any amd similar just so it can manage the prompt and let the rest of the mi50 take care of the interference? Does the prompt itself need 120gb vram?  \nComment: Any updated photos? \nComment: I don't see the point of using power hungry GPU's when you are getting CPU interference speeds.  I get those speeds on my M1 Studio Max (5.7t/s on watermelon prompt on 70b for GGUF) or 8.1t/s for MX 70b. At least this was the reason I purchased a discounted M1 Max Studio over spending 1000-1400 for a CPU interference setup or a Digits or similar, or something like your setup.  Add in the hassle of getting all those parts, some might be broken, diagnosing, etc.  It's definitely a option. \nComment: prompt processing gets faster the larger your prompts. sharing so folks don't get discouraged.  prompt eval time = 10.76 ms / 1 tokens ( 10.76 ms per token, 92.90 tokens per second) eval time = 9239.62 ms / 857 tokens ( 10.78 ms per token, 92.75 tokens per second) prompt eval time = 2438.71 ms / 15 tokens ( 162.58 ms per token, 6.15 tokens per second) eval time = 13430.91 ms / 1042 tokens ( 12.89 ms per token, 77.58 tokens per second) prompt eval time = 520.00 ms / 1026 tokens ( 0.51 ms per token, 1973.08 tokens per second) eval time = 16342.38 ms / 1050 tokens ( 15.56 ms per token, 64.25 toke...\nComment: But 5 token per second is just slightly faster than a good CPU RAM setup‚Ä¶ \nComment: Bottom Line:  Generation speed: ~4.9 tokens/sec Time to first token small context: 12 seconds Time to first token large context: 2 minutes  on the $1,000 MI50 build using a 70b Q8 model \nComment: bruh this post is so long it took 160gb of VRAM to render it \nComment: How you get the octominers? They're sold out \nComment: MI50s are 200+ ‚Ç¨ where I live sadly. This could've been a cool project... Adding rising energy costs the framework PC still looks promising for a lot of interference usecases and just as a general work and sometimes gaming setup. \nComment: cost going down,,,, i like \nComment: At that speed / price just get a used server and run a moe like deepseek. \nComment: Sadly prompt processing is awful. \nComment: I wonder how it will works with Moe models like Llama 4 \nComment: I upvoted as soon as I read you don't have any blogs or anything to sell THANK YOU for that breath of fresh air \nComment: Great post, my takeaway from the numbers is if its just inference its probably worth paying a bit more for a P40, but I guess everyone has a view on that price / speed / cuda balancing act \nComment: I never got Wake on Lan to run reliably, but most BIOSes allow to define what happens after a power outage (off, last state, on). I set that to on and put the machine behind a WiFi plug. \nComment: I wanna see that deepseek speed. Cuz if its good... \nComment: Cheap, fast, power efficient. Choose two. \nComment: All this and no photos? \nComment: At that point why not just buy a used Mac? \nComment: Is this possible for all autoregressive models provided they fit in the aggregate VRAM across all cards, or do they need to support this natively? Is it a feature specifically implemented by Llama.cpp? Would I also be able to use Ollama for it? It occurs to me that this is possible for text generation because the models are autoregressive and run sequentially, right? Would I be able to use the total aggregate VRAM across these chips to run a diffusion model for image or video generation that does not necessarily support sharding? \nComment: Then you get the Vram, but slow BW on PCI. Not sure how this is better than pure RAM then. \nComment: What's the specific CPU you have? I ran into issues with my server build because mine doesn't support avx2; so if yours does that might be a good path for me \nComment:  3000 watts \nComment: Sadly mi50s are 120-150 now, still a good deal \nComment: 160gb is a lot, but still not enough for deepseek? Yes, it's MoE, but q4 quants weight about 400gb, why do you think will it fly? \n","Title: LocalAI v2.28.0 + Announcing LocalAGI: Build \u0026 Run AI Agents Locally Using Your Favorite LLMs\nID: t3_1k0haqw\nSummary:       Hey r/LocalLLaMA fam! Got an update and a pretty exciting announcement relevant to running and using your local LLMs in more advanced ways. We've just shipped LocalAI v2.28.0, but the bigger news is the launch of LocalAGI, a new platform for building AI agent workflows that leverages your local models. TL;DR:  LocalAI (v2.28.0): Our open-source inference server (acting as an OpenAI API for backends like llama.cpp, Transformers, etc.) gets updates. Link:https://github.com/mudler/LocalAI LocalAGI (New!): A self-hosted AI Agent Orchestration platform (rewritten in Go) with a WebUI. Lets you build complex agent tasks (think AutoGPT-style) that are powered by your local LLMs via an OpenAI-compatible API. Link:https://github.com/mudler/LocalAGI LocalRecall (New-ish): A companion local REST API for agent memory. Link:https://github.com/mudler/LocalRecall The Key Idea: Use your preferred local models (served via LocalAI or another compatible API) as the \"brains\" for autonomous agents running complex tasks, all locally.  Quick Context: LocalAI as your Local Inference Server Many of you know LocalAI as a way to slap an OpenAI-compatible API onto various model backends. You can point it...\nComment:       Hey r/LocalLLaMA fam! Got an update and a pretty exciting announcement relevant to running and using your local LLMs in more advanced ways. We've just shipped LocalAI v2.28.0, but the bigger news is the launch of LocalAGI, a new platform for building AI agent workflows that leverages your local models. TL;DR:  LocalAI (v2.28.0): Our open-source inference server (acting as an OpenAI API for backends like llama.cpp, Transformers, etc.) gets updates. Link:https://github.com/mudler/LocalAI LocalAGI (New!): A self-hosted AI Agent Orchestration platform (rewritten in Go) with a WebUI. Lets you...\nComment: Would be nice to watch YouTube video with the demo, is it available somewhere? \nComment: This is very neat. I like the color scheme \nComment: This is super cool \nComment: Does LocalAI run interference in same container or can spawn containers? Does it support parallel running multiple llms? \nComment: can this launch multiple vllm like backends (i.e open ai compatible apis) at the same time ? or at least be able to automatically load and unload models on demand ? \n","Title: Hugging Face has launched a reasoning datasets competition with Bespoke Labs and Together AI\nID: t3_1k0q0bc\nSummary: Reasoning datasets currently dominate Hugging Face's trending datasets, but they mostly focus on code and maths. Along with Bespoke Labs and Together AI, we've launched a competition to try and diversify this landscape by encouraging new reasoning datasets focusing on underexplored domains or tasks.  Key details:  Create a proof-of-concept dataset (minimum 100 examples) Upload to Hugging Face Hub with tag \"reasoning-datasets-competition\" Deadline: May 1, 2025 Prizes: $3,000+ in cash/credits All participants get $50 in Together.ai API credits  We welcome datasets in various domains (e.g., legal, financial, literary, ethics) and novel tasks (e.g., structured data extraction, zero-shot classification). We're also interested in datasets supporting the broader \"reasoning ecosystem.\" For inspiration, I made my own proof of concept dataset davanstrien/fine-reasoning-questions, which generates reasoning questions from web text using a pipeline approach. First, I trained a smaller ModernBERT-based classifier to identify texts that require complex reasoning, then filtered FineWeb-Edu content based on reasoning scores, classified topics, and finally used Qwen/QWQ-32B to generate the reasoning...\nComment: Reasoning datasets currently dominate Hugging Face's trending datasets, but they mostly focus on code and maths. Along with Bespoke Labs and Together AI, we've launched a competition to try and diversify this landscape by encouraging new reasoning datasets focusing on underexplored domains or tasks.  Key details:  Create a proof-of-concept dataset (minimum 100 examples) Upload to Hugging Face Hub with tag \"reasoning-datasets-competition\" Deadline: May 1, 2025 Prizes: $3,000+ in cash/credits All participants get $50 in Together.ai API credits  We welcome datasets in various domains (e.g., legal...\nComment: That's a really cool idea. Even aside from competition I've been considering how thinking examples would probably really beef up how well limited datasets were \"understood\" in terms of connections with each other. That might get me off my ass and testing it out with some of the more disappointing elements in mine. \nComment: I appreciate that! Curious to see the community submissions! \nComment: i have made a medical reasoning dataset using novel techniques based on a lot of reserch i did...is it possible to upload only the datasets and not the methods and pipeline for evaluations? i dont feel comfortable giving you something i worked so hard maybe not to get anything while you are probably making a lot of money farming this novel ideasüòÇ \nComment: There goes my weekend. \nComment: Going to create some creative writing traces I s'pose. \nComment: Can anyone elaborate on training classifier for reasoning? \n","Title: It is almost May of 2025. What do you consider to be the best coding tools?\nID: t3_1k0nxlb\nSummary: It is almost May of 2025. What do you consider to be the best coding tools?  I would like to get an organic assessment of the community‚Äôs choice of IDE and AI tools that successfully helps them in their programming projects.  I‚Äôm wondering how many people still use cursor, windsurf especially with the improvements of models vs cost progression over the past few months.  For the people that are into game development, what IDE helps your most for your game projects made in Unity/Godot etc.  Would love to hear everyone‚Äôs input.  As for me, I‚Äôm currently find very consistent results in creating a vieriety of small programs with Python using cursor and Gemini 2.5. Before Gemini 2.5 came out, I was using 3.7 Claude, but was really debating with myself on if 3.7 was better than 3.5 as I was getting mixed results.     submitted by    /u/Material_Key7014   [link]   [comments]\nComment: It is almost May of 2025. What do you consider to be the best coding tools?  I would like to get an organic assessment of the community‚Äôs choice of IDE and AI tools that successfully helps them in their programming projects.  I‚Äôm wondering how many people still use cursor, windsurf especially with the improvements of models vs cost progression over the past few months.  For the people that are into game development, what IDE helps your most for your game projects made in Unity/Godot etc.  Would love to hear everyone‚Äôs input.  As for me, I‚Äôm currently find very consistent results in cre...\nComment: https://preview.redd.it/em2ft1z288ve1.png?width=776\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=58360ff325b02e915fcbc3520c97be9cb7f1b88d Aider + Gemini 2.0 are responsible for most of my github activity starting from February. It's not games, just low-level recreational programming. Strix is C++ the bot made to try to make a keyboard-based mouse control system using a grid. My llm-clue concept is C++ the bot made. The board game doesn't work, but it generates the Characters, weapons, and rooms with the LLM and then creates the stableDiffusion prompts for them. (If someone is good at board games, please...\nComment: Until very recently I avoided using any kind of AI powered IDE. I still barely use it, but it‚Äôs nice to have that option in Cursor. Otherwise, I code normally with a web-based LLM for any support, the docs open for whatever I‚Äôm using/working on, and a few GitHub pages for other peoples implementations of whatever it is I might be doing.  I use AI - Claude 3.7 or Gemini 2.5 Pro - to build a ‚Äúdev-doc‚Äù and ‚Äúspec-doc‚Äù for my build process. I just like having the easy md file with checkboxes that AI can fill in for me.  AI isn‚Äôt very good at coding real world code. It just doesn‚Äôt u...\nComment: I do not use any IDE/editor's AI feature for coding. They feel pretty much useless even with full context. My personal opinion is, a developer should fully understand their code before committing, AI powered editors destroys this habit That said, AI is very helpful in ideation, searching and prototyping, or reaching a solution for parts of a bigger problem individually. Any chatbot interface can do that and for models, gemini 2.5 is the best out there for almost everything. Claude 3.5/3.7 still have some edge in react. Deepseek r1 and v3 are both on par with whatever models come with chatgpt f...\nComment: Roo Code with DeepSeek is surprisingly useful. For simpler changes or applying the changes in aider, qwen2.5-coder is good. \nComment: I have my own set of AI coding tools. I've probably tried it all from the beginning, I don't think the current AI coding paradigm is the correct one. \nComment: VSC is great. Also I don't vibe code, just use o1 to generate code snippets and examples, basically like a more advance search engine/documentation. o1 is also great for debugging. Otherwise ain't no way I am letting anything that I don't understand into production. \nComment: My brain + 20y experience + jetbrains ide. And sometimes AI as consultant, but no coder. \nComment: Me = coder for almost 50 years (TRS-80 Basic ftw), taught coding at a graduate college level, and professional coder for 30+ years. Not sure how a \"Don't use AI coders because you won't know your code\" answer is the top response to this question. I used to work with a guy 15 years ago who refused to use code completion because \"He wouldn't know his code if he did\", and guess what, he was the least productive member of the team. Same is true today, use the tools wisely and cut the absolutist attitude of \"I won't know my code!\" crap. AI code isn't an all or nothing type thing and there are tons ...\nComment: GitHub copilot VSCode Extension. They recently added OpenRouter support so you can test out pretty much any model in existence. Past few weeks I‚Äôve been ditching my GOAT - Claude 3.7 (and 3.5 before it) for Gemini 2.5 Pro quite often. \nComment: Depends tbh. For Python most are good. But if you are doing something with languages with Oxygene + DevExpress even Copilot hallucinates let alone 14B and 32B models which had time cut off 2022 or 2023. \nComment: I am hoping it will be Qwen 3 whenever it comes out! \nComment:   NeoVim + Avante.nvim  Zed  Even if AI coding feels less efficient or reliable than manual coding, it's still a win because I can browse Reddit while AI is doing their things. \nComment: Roo code with Gemini flash 2.0 for asking about codebases and collecting context Then Aider for surgical edits using deepseek OR cursor/windsurf for projects created from the ground up \nComment: I've recently been playing around with VSCode and continue.dev, running Mistral Small 3.1 for chat and code generation, with Qwen2.5 Coder 1.5B for autocomplete and apply. Its surprisingly usable if you give it the right context to work with. Its for this reason I'm not using primarily using Qwen2.5 Coder, as context and speed is king here. These small models generally don't know jack about APIs and libraries outside the core language, so you need to populate the context with relevant information to get usable output. And inference speed needs to be fast, otherwise you literally type faster th...\nComment: Our on-prem GPU server runs TabbyML and folks use it with the tabby extension (vscode or neovim) for copilot-style tab completion and chat about code. For \"vibe\" coding we use aider with various big LLM services, most people use Claude Sonnet, some Gemini. I personally use neovim with CodeCompanion, the UX is a better fit for me personally. Same models underneath, though (Qwen Coder, DeepCoder). Last weekend I tried to run aider 100% locally with QwQ (a smallish reasoning model) running as Architect and Qwen Coder 2.5 running as Editor (the former comes up with the architecture and tells the l...\nComment: I'm biased, but RA.Aid is still ahead of most of the coding agents out there. Nothing else has things like our expert tool, built-in multi task planning, etc. For open models, deepseek v3 0324 + deepseek r1 for the expert model is the best combo. \nComment: I'm still using Cline and either Gemini Pro 2.5 or Sonnet 3.7. Still use it more of an assistant or to get ideas though, I don't like the sudden changes they both often make. The cool thing about Cline is that I can keep it in Plan mode only (I believe you can set similar workflows in Roo as well). \n","Title: What is the best option for running eight GPUs in a single motherboard?\nID: t3_1k0w7f9\nSummary: TLDR: Can I run 8 GPUs with two 1 to 4 PCIE splitter with bifurcation on my ASUS ROG CROSSHAIR VIII DARK HERO and AMD 5950x? or I need to purchase another motherboard? ---- Hi everyone, I recently bought eight AMD MI50 32GB GPUs (total of 256 GB VRAM) for experimenting with 100B+ LLMs. However, I am not sure if my motherboard supports 8 GPUs. My motherboard is ASUS ROG CROSSHAIR VIII DARK HERO. It has three PCIE 4.0 x16 slots, one PCIE4.0 x1, and two M.2 PCIE4.0 x4 slots. The CPU is AMD 5950x which has 24 lanes on the CPU. I have 96GB of RAM. Currently, both M.2 slots are occupied with NVME storage. I also installed three GPUs on all available three PCIE 4.0 x16 slots. Now, my motherboard BIOS shows each GPU is running at x8, x8 (Both MI50 cards) and x4 (RTX 3090). My question is does this motherboard support 8 GPUs at once if I use PCIE splitter (e.g. 1 PCIE slot to 4 PCIE slots)? I see the user manual says the first PCIE 4.0 x16 slot supports PCIE bifurcation with x4+x4+x4+x4 for M.2 cards. But let's say I install 1 to 4 PCIE splitter on the first and second slot both running at x8. Can I install eight GPUs and run each of them at PCIE4.0 x2 with bifurcation (not sure if I need t...\nComment: TLDR: Can I run 8 GPUs with two 1 to 4 PCIE splitter with bifurcation on my ASUS ROG CROSSHAIR VIII DARK HERO and AMD 5950x? or I need to purchase another motherboard? ---- Hi everyone, I recently bought eight AMD MI50 32GB GPUs (total of 256 GB VRAM) for experimenting with 100B+ LLMs. However, I am not sure if my motherboard supports 8 GPUs. My motherboard is ASUS ROG CROSSHAIR VIII DARK HERO. It has three PCIE 4.0 x16 slots, one PCIE4.0 x1, and two M.2 PCIE4.0 x4 slots. The CPU is AMD 5950x which has 24 lanes on the CPU. I have 96GB of RAM. Currently, both M.2 slots are occupied with NVME st...\nComment: The short answer is: you need a new motherboard with an older HEDT CPU or better a server board and CPU. You can't \"split\" lanes beyond the bifurcation options in the BIOS, unless you find a splitter with an active PCIe switch. And even then, it'll be a hassle and you'll only be able to run models sequentially across cards, leading to very slow performance. Your motherboard, as you said, has 24 lanes coming from the CPU, that second x16 slot is only mechanically X16. If you use both, the motherboard will switch both X16 slots to run at X8 each. To have any chance at running those cards with te...\nComment: Just bought an ASRock ROMED8-2T + and Epyc 7J43 a month ago. With up to 2TB RAM, 128 PCIe 4.0 lanes and 7x PCIe x16 slots (that you could bifurcate or optionally use the two occulink and get two x4). Do notice that the PCIe slot 2 (IIRC) shares lanes with the occulink and m.2 but you can disable this. \nComment: I do not know if that would be technically possible. My guess would be that it depends on the layout of the bitfurication card, if u get 2x4 or 4x2 from the x16 slot running in x8 mode. This manual for a 4 nvme pcie extension card for example says it supports x8, so I think MBs could support that: https://dlcdnets.asus.com/pub/ASUS/mb/Add-on_card/E14501_HYPER_M.2_X16_Card_V2_UM_PRINT.pdf?model=hyper%20m.2%20x16%20card%20v2 Your best bet is probabbly just trying it out. Consider using a supplier with a return policy. If it does not work, u could just return your parts. Otherwise like the other ...\nComment: I have a gaming motherboard with three x16 slots in my secondary workstation, but their actual speeds are x8 x8 x4. In the past, it was my primary workstation for over a year, and I had 4 GPUs connected to it - three via PCI-E 4.0 x16 30cm risers, and one via PCI-E 3.0 x1 riser. The motherboard also had two more x1 slots and also I could bifurcate two main slots from x8 to x4 x4, hence putting 8 GPUs in total, I also had 4kW in total of power, so power wasn't an issue. What was the issue, is lack of speed due to limited PCI-E lanes - even just loading a model was painfully slow, easily taking ...\nComment: oof that's a lot of effort for some pretty poorly supported hardware. estimating $500 per card you've got $4000 in gpus, and you're likely going to need at least another $1600 for a server chassis. You might get lucky and find one with 8x pcie available for that much $.... (https://www.ebay.ca/itm/387307003713) I don't think you have much option to avoid buying a server for at least $1000 and likely more. But even if you use your existing parts you're now at price parity with a 256gb m3 ultra studio, and if your motherboard or your cpu aren't compatible with the pcie risers and you need to swa...\n","Title: ByteDance releases Liquid model family of multimodal auto-regressive models (like GTP-4o)\nID: t3_1k05wpt\nSummary:       Model Architecture Liquid is an auto-regressive model extending from existing LLMs that uses an transformer architecture (similar to GPT-4o imagegen). Input: text and image. Output: generate text or generated image. Hugging Face: https://huggingface.co/Junfeng5/Liquid_V1_7B App demo: https://huggingface.co/spaces/Junfeng5/Liquid_demo Personal review: the quality of the image generation is definitely not as good as gpt-4o imagegen. However it‚Äôs important as a release due to using an auto-regressive generation paradigm using a single LLM, unlike previous multimodal large language model (MLLM) which used external pretrained visual embeddings.    submitted by    /u/ResearchCrafty1804   [link]   [comments] \n","Title: InternVL3: Advanced MLLM series just got a major update ‚Äì InternVL3-14B seems to match the older InternVL2.5-78B in performance\nID: t3_1k0fjny\nSummary:       OpenGVLab released InternVL3 (HF link) today with a wide range of models, covering a wide parameter count spectrum with a 1B, 2B, 8B, 9B, 14B, 38B and 78B model along with VisualPRM models. These PRM models are \"advanced multimodal Process Reward Models\" which enhance MLLMs by selecting the best reasoning outputs during a Best-of-N (BoN) evaluation strategy, leading to improved performance across various multimodal reasoning benchmarks. The scores achieved on OpenCompass suggest that InternVL3-14B is very close in performance to the previous flagship model InternVL2.5-78B while the new InternVL3-78B comes close to Gemini-2.5-Pro. It is to be noted that OpenCompass is a benchmark with a Chinese dataset, so performance in other languages needs to be evaluated separately. Open source is really doing a great job in keeping up with closed source. Thank you OpenGVLab for this release!  https://preview.redd.it/66ifgifkr5ve1.png?width=2756\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=77650cfe31229f9bde35da3e569cef3d5caa885f    submitted by    /u/Mr_Moonsilver   [link]   [comments] \n","Title: Open Source tool from OpenAI for Coding Agent in terminal\nID: t3_1k0qw6k\nSummary: repo: https://github.com/openai/codex Real question is, can we use it with local reasoning models?    submitted by    /u/_anotherRandomGuy   [link]   [comments]\n","Title: What are some Local search offerings that are competitive with OpenAI/Google, if such a thing can exist?\nID: t3_1k0s2cx\nSummary:       I was excited to ask about the new models, but only one of those citations were related to my query (pure hallucination otherwise). Also 1 minute for a simple question is totally unacceptable. I asked the same thing to 4o on a different account, with search enabled ~~The right answer was on OpenAI's blog~~ https://openai.com/index/introducing-o3-and-o4-mini/ Google was fast and didn't give me any relevant results at all, ChatGPT can't even answer questions about itself, where do I go for information? EDIT: The right answer was not cited in any of my queries at all: https://www.reddit.com/r/LocalLLaMA/s/YH5L1ztLOs Thank you for the answer r/LocalLLaMa    submitted by    /u/m1tm0   [link]   [comments] \n","Title: Setting Power Limit on RTX 3090 ‚Äì LLM Test\nID: t3_1k0mrrt\nSummary:         submitted by    /u/1BlueSpork   [link]   [comments] \n","Title: What is your favorite uncensored model?\nID: t3_1k0967d\nSummary: By uncensored, I don't just mean roleplay. I have yet to find a model that doesn't refuse when asked on instructions of how to cook meth, make pipe bombs, or invade a small country in South America and force them to sell bananas to you.  I feel like a good chunk is lost when you get lobotomized and taught to not say certain things    submitted by    /u/HornyGooner4401   [link]   [comments]\n"],"results":[{"Title":"IBM Granite 3.3 Models","ID":"t3_1k0mesv","Summary":"IBM announced the release of Granite 3.3, a new version of their speech model, emphasizing open-weight availability and improvements over previous versions. The model maintains an 8B parameter size, focusing on integration into AI assistants. Community comments highlight mixed performance reviews compared to earlier versions (like Granite 3.1 and 3.2) and competitors such as Qwen and Mistral. Users note its effectiveness in agentic systems with tool calling but criticize its English-only speech support and lack of multilingual capabilities. Technical discussions include interest in INT8/QAT variants for deployment optimization and comparisons with Whisper.","Comment Summary":"Positive sentiment toward IBM's community engagement but mixed performance feedback. Users question backward compatibility in metrics, request quantization variants, and seek benchmarks against competitors like Whisper. Some praise its tool-calling performance, while others find it underwhelming compared to peers.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0mesv/ibm_granite_33_models/","Relevance":"Granite 3.3's open-source availability and focus on speech integration advance accessibility for developers. Its performance in structured tasks and tool-calling scenarios offers practical applications in agentic systems. The model's parameter efficiency and comparisons to larger models highlight its value in resource-constrained environments. However, limited multilingual support and mixed performance reviews indicate areas for improvement.","IsRelevant":true},{"Title":"Nvidia Model Naming Critique","ID":"t3_1k0u8ew","Summary":"A post humorously critiques Nvidia‚Äôs naming conventions for new models (e.g., 'UltraLong'), comparing them to condom brands. Comments focus on the perceived inappropriateness of the names and the lack of clarity in distinguishing technical features. Users suggest more professional naming conventions and question the practicality of the model variants.","Comment Summary":"Humorous and critical reception of Nvidia‚Äôs naming choices. Users express confusion over naming and suggest alternatives. While lighthearted, the discussion underscores the importance of clear model identification for technical users.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0u8ew/somebody_needs_to_tell_nvidia_to_calm_down_with/","Relevance":"While primarily humorous, the critique highlights the need for consistent and meaningful naming conventions in model releases, which aid user adoption and technical communication. However, lacks technical depth on model specifics.","IsRelevant":false},{"Title":"5000 Tokens/Second on 2x3090","ID":"t3_1k0tkca","Summary":"A user reports achieving 5000 tokens per second on dual NVIDIA 3090 GPUs using Qwen2.5-7B, prioritizing speed over size. Tests on MMLU-pro and BBH benchmarks show diminishing returns with larger models. The setup uses Aphrodite engine for speculative decoding, with quantization optimizations (AWQ) improving throughput. Unquantized models achieved 1000/300 tokens/sec, while AWQ variants showed lower latency.","Comment Summary":"Technical discussion around optimizing throughput with quantization and engine choices. Users may benefit from insights on trade-offs between model size, speed, and performance metrics.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/","Relevance":"Provides practical insights into model optimization for high-throughput scenarios, useful for researchers and practitioners scaling AI systems. Quantization techniques and engine selection (Aphrodite vs. vLLM) offer actionable strategies for performance tuning.","IsRelevant":true},{"Title":"OpenAI o3 and o4-mini Release","ID":"t3_1k0pnvl","Summary":"OpenAI introduced o3 and o4-mini models, part of the o-series designed for prolonged thinking, marking a significant leap in ChatGPT's capabilities. These models aim to enhance response quality and contextual depth for both casual users and advanced researchers.","Comment Summary":"Limited comments available. The announcement suggests focus on extended reasoning but lacks technical details like parameter counts, benchmarks, or training methodologies.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0pnvl/openai_introducing_openai_o3_and_o4mini/","Relevance":"OpenAI‚Äôs new models signal advancements in prolonged reasoning, a critical area for complex tasks. However, without technical specifics, their significance remains speculative. Practitioners await detailed comparisons and performance metrics.","IsRelevant":true},{"Title":"Droidrun Open Source","ID":"t3_1k0h641","Summary":"Droidrun, a framework for running large models efficiently, is now open-sourced on GitHub. With over 900 waitlist signups, its release addresses community demand. The framework likely focuses on optimizing model execution, though details on architecture and performance are sparse.","Comment Summary":"Positive reception with excitement over open-sourcing. Users await technical details to assess its utility compared to existing frameworks.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0h641/droidrun_is_now_open_source/","Relevance":"Open-sourcing Droidrun could provide a new tool for optimizing model deployment, though its impact depends on its unique features and performance. Useful for developers seeking efficient model execution solutions.","IsRelevant":true},{"Title":"Announcing RealHarm: A Collection of Real-World Language Model Application Failure","ID":"t3_1k0iu5z","Summary":"Giskard introduces RealHarm, a dataset of real-world incidents involving deployed LLMs, focusing on organizational harms like reputational damage and hazards like misinformation. The dataset, annotated using an evidence-based taxonomy, reveals gaps in current guardrails and aims to aid in preventing real-world AI risks. It highlights incidents where state-of-the-art guardrails failed, emphasizing the need for practical safety measures.","Comment Summary":"Comments express skepticism about the dataset's utility, critique its corporate bias, and question its timeliness. Some argue it focuses on superficial harms like reputational damage while ignoring deeper issues like censorship. Others note the dataset's small size and lack of current examples as limitations.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/","Relevance":"This dataset addresses the gap between theoretical AI risks and real-world failures, providing actionable insights for developers and deployers. By cataloging concrete incidents, it helps prioritize practical safety measures over theoretical concerns. However, its focus on organizational reputation and perceived corporate alignment may limit its appeal to researchers focused on fundamental safety.","IsRelevant":true},{"Title":"Price vs LiveBench Performance of non-reasoning LLMs","ID":"t3_1k0kape","Summary":"Analysis comparing costs and performance of non-reasoning LLMs using LiveBench benchmarks. The post evaluates models like Llama, BLOOM, and others on efficiency, latency, and cost-effectiveness, providing practical insights for developers choosing infrastructure. It highlights which models deliver the best performance-per-dollar without advanced reasoning capabilities.","Comment Summary":"Comments discuss the methodology's rigor, question the exclusion of reasoning models, and debate the relevance of benchmark metrics to real-world use cases. Some suggest expanding the analysis to include more diverse workloads.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0kape/price_vs_livebench_performance_of_nonreasoning/","Relevance":"This provides actionable data for cost-conscious developers selecting LLMs for specific tasks. By focusing on non-reasoning models, it addresses a common use case (e.g., chatbots, content generation) where efficiency is critical. The benchmarking approach offers a template for evaluating infrastructure choices.","IsRelevant":true},{"Title":"Results of Ollama Leakage","ID":"t3_1k0p3h0","Summary":"Report on security vulnerabilities in Ollama deployments, noting that many servers lack basic protections. The linked site (freeollama.com) may expose sensitive information, underscoring poor security practices in LLM hosting environments. This highlights risks of misconfigured servers and the need for better security standards.","Comment Summary":"Comments warn about irresponsible exposure of services and stress the importance of securing model endpoints. Some criticize the community for not prioritizing security, while others suggest best practices for securing deployments.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0p3h0/results_of_ollama_leakage/","Relevance":"This serves as a cautionary tale about operational security in LLM deployments. It underscores that even with advanced models, poor security practices can lead to breaches, emphasizing the need for security-by-design in AI infrastructure.","IsRelevant":true},{"Title":"OpenAI introduces codex: a lightweight coding agent that runs in your terminal","ID":"t3_1k0qisr","Summary":"OpenAI's new codex is a lightweight, terminal-based coding assistant designed for developers. It offers real-time code suggestions, debugging, and context-aware assistance, emphasizing speed and integration with existing workflows. Technical details include low resource requirements and seamless terminal integration.","Comment Summary":"Comments speculate on codex's capabilities, compare it to existing tools like Copilot, and discuss potential use cases. Some question the absence of multi-language support or advanced features, while others welcome its simplicity.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0qisr/openai_introduces_codex_a_lightweight_coding/","Relevance":"This represents a step toward making advanced LLMs more accessible to developers through specialized, lightweight tools. Its terminal focus addresses a specific pain point (contextual coding help) and demonstrates OpenAI's commitment to practical, integration-ready solutions.","IsRelevant":true},{"Title":"o4-mini is 186·µó ∞ best coder, sleep well platter! Enjoy retirement!","ID":"t3_1k0qbme","Summary":"Humorous post mocking o4-mini's low ranking (186th) in coding performance, suggesting it's obsolete. The title implies the model should retire, indicating poor performance relative to peers. This reflects ongoing competition and rapid iteration in LLM performance rankings.","Comment Summary":"Comments joke about the model's ranking and the rapid obsolescence of LLMs. Some note the importance of benchmarking for tracking progress but criticize the snarky tone. Others question the ranking methodology's validity.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0qbme/o4mini_is_186·µó ∞_best_coder_sleep_well_platter/","Relevance":"This underscores the fast-paced evolution of LLMs and the importance of continuous benchmarking. While humorous, it highlights the transient nature of model rankings and the need for developers to stay updated.","IsRelevant":true},{"Title":"InternVL3: Advanced MLLM series just got a major update ‚Äì InternVL3-14B seems to match the older InternVL2.5-78B in performance","ID":"t3_1k0fjny","Summary":"OpenGVLab released InternVL3, a series of multimodal language models (MLLMs) spanning parameter sizes from 1B to 78B. The new VisualPRM (Process Reward Model) modules use a Best-of-N selection strategy to enhance reasoning capabilities, achieving performance comparable to larger predecessors like InternVL2.5-78B with just 14B parameters. Benchmarks on OpenCompass show InternVL3-14B matches the older 78B model, while the 78B variant rivals Gemini-2.5-Pro. However, the benchmark's reliance on Chinese datasets raises questions about cross-lingual performance.","Comment Summary":"Community reaction is positive, emphasizing the efficiency of scaling down parameter counts without losing performance. Technical discussions highlight the effectiveness of VisualPRMs in optimizing output selection. Concerns about dataset bias and the need for non-Chinese evaluations are noted. Some users question the practicality of deploying larger models.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0fjny/internvl3_advanced_mllm_series_just_got_a_major/","Relevance":"This advancement matters because it demonstrates efficient model scaling, allowing smaller models to match larger ones through architectural innovations like VisualPRMs. This could reduce computational costs and enable wider deployment. The performance parity with proprietary models like Gemini-2.5-Pro underscores open-source competitiveness, encouraging further research into efficient multimodal architectures.","IsRelevant":true},{"Title":"Open Source tool from OpenAI for Coding Agent in terminal","ID":"t3_1k0qw6k","Summary":"OpenAI released an open-source terminal-based coding assistant tool, likely leveraging their Codex series. The user questions compatibility with local models, indicating interest in self-hosted solutions. While the tool‚Äôs specifics aren‚Äôt detailed, its availability as open source could enable customization and integration with other systems.","Comment Summary":"Discussions focus on potential use cases and the feasibility of adapting it for local models. Some users note that compatibility depends on API structure and model requirements, suggesting that smaller models might require adjustments. There‚Äôs excitement about terminal integration but caution about resource demands.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0qw6k/open_source_tool_from_openai_for_coding_agent_in/","Relevance":"This matters as it provides a practical coding tool that could be adapted for local deployments, reducing reliance on cloud APIs. It offers a template for integrating LLMs into developer workflows, though challenges in model size and computational requirements remain.","IsRelevant":true},{"Title":"What are some Local search offerings that are competitive with OpenAI/Google, if such a thing can exist?","ID":"t3_1k0s2cx","Summary":"The user expresses frustration with current search-based LLMs, noting poor performance and lack of relevant results. They highlight issues like slow response times, missing citations, and inconsistency across platforms. The post underscores the gap between promised and actual capabilities of search-enhanced models.","Comment Summary":"Replies suggest alternatives like Qwen, M3, and other open-source models, emphasizing the need for better search integration. Users debate the feasibility of open-source systems matching proprietary search capabilities, with some advocating for improved open-source toolkits.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0s2cx/what_are_some_local_search_offerings_that_are/","Relevance":"This reflects practical challenges in deploying local models for search tasks, indicating a need for better search modules and integration. It highlights the importance of reliable, fast, and context-aware search features in competitive LLM applications.","IsRelevant":true},{"Title":"Setting Power Limit on RTX 3090 ‚Äì LLM Test","ID":"t3_1k0mrrt","Summary":"The user discusses testing power limits on an RTX 3090 GPU during LLM operations. While the specifics of the test aren‚Äôt detailed, it touches on hardware optimization for LLM workloads, suggesting optimization strategies for resource-constrained environments.","Comment Summary":"Comments focus on technical aspects like thermal throttling and power efficiency. Users share tips for balancing performance and power consumption, though the lack of detailed results limits broader insights.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0mrrt/setting_power_limit_on_rtx_3090_llm_test/","Relevance":"Hardware optimization remains crucial for efficient LLM deployment, especially on consumer-grade GPUs. This discussion aids in understanding practical limitations and best practices for running large models locally.","IsRelevant":true},{"Title":"What is your favorite uncensored model?","ID":"t3_1k0967d","Summary":"The user seeks uncensored models that don‚Äôt comply with ethical guidelines, specifically those capable of generating harmful content. Responses likely address the ethical dilemmas and legal risks associated with such models, emphasizing the importance of responsible AI practices.","Comment Summary":"Replies warn against unethical requests and highlight the necessity of safety measures. Some users suggest exploring historical model versions but caution against misuse. The discussion underscores the tension between free expression and responsible AI.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0967d/what_is_your_favorite_uncensored_model/","Relevance":"This highlights ethical considerations in model deployment, reminding practitioners of the need to balance capabilities with safety. It reinforces the importance of maintaining ethical standards to prevent misuse.","IsRelevant":true},{"Title":"We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed","ID":"t3_1k0c40c","Summary":"Menlo Research introduced ReZero, a model that uses GRPO and tool-calling with a retry_reward mechanism to improve search performance. By encouraging repetitive actions, ReZero achieved a 46% performance score, surpassing a baseline model's 20%. This challenges the common belief that repetition causes hallucinations. Applications include enhancing search engines or acting as an abstraction layer over existing LLMs.","Comment Summary":"Community reactions are positive, noting the innovative approach but questioning over-retrying and parallel search strategies. Some suggest it aligns with existing multi-hop retrieval methods like GritHopper. Technical discussions include reward function design, implementation details, and comparisons to other models.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/","Relevance":"ReZero's retry mechanism offers a novel way to improve search performance, countering assumptions about repetition's drawbacks. It provides a framework for iterative search optimization, potentially enhancing tools like query generators. The 46% performance gain suggests practical applications in reducing user frustration with search accuracy. It matters because it demonstrates that persistence in search can yield better results, impacting how LLMs interact with external tools.","IsRelevant":true},{"Title":"KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...","ID":"t3_1k0odhq","Summary":"A user experiment with Gemma 3 27B on KoboldCpp highlights its mixed performance in vision tasks. While it can identify images, it often hallucinates details, misclassifies objects, and provides inaccurate responses when queried about unseen image elements. Users note that smaller models like Gemma 2 or Qwen2.5-VL perform better, and hardware limitations (e.g., image downscaling) affect results.","Comment Summary":"Comments question the model's reliability, noting hallucinations and preferring alternatives like Qwen2.5-VL. Technical discussions focus on hardware requirements, model efficiency, and image processing quirks in KoboldCpp.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/","Relevance":"While Gemma 3's vision capabilities are promising, its hallucination issues and inferior performance to smaller models underscore challenges in multi-modal LLMs. This informs developers about potential pitfalls in scaling vision models and the need for better grounding.","IsRelevant":false},{"Title":"the budget rig goes bigger, 5060tis bought! test results incoming tonight","ID":"t3_1k0kzgn","Summary":"A budget build using NVIDIA RTX 5060 Ti GPUs aims to provide cost-effective LLM performance. The user plans to test these GPUs with tools like SDXL and Flux in ComfyUI, emphasizing their affordability and potential for local inference setups.","Comment Summary":"Comments seek additional tests, hardware insights, and comparisons. Some question the relevance of GPU specs like AI TOPS.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0kzgn/the_budget_rig_goes_bigger_5060tis_bought_test/","Relevance":"This highlights cost-effective hardware options for local LLM deployment, useful for researchers and hobbyists. The test results could guide budget-conscious users in optimizing their setups.","IsRelevant":true},{"Title":"Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max","ID":"t3_1k0r9pi","Summary":"Testing Gemma 3 27B on M4 Max shows better output quality via llama.cpp than Ollama. Differences in sampling techniques, quantization, and configuration parameters likely cause the disparity.","Comment Summary":"Discussions focus on configuration settings, quantization, and sampler sequences. Users suggest adjusting Ollama's parameters or checking model compatibility.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/","Relevance":"This underscores the importance of runtime environments and configuration in model performance. It guides developers to optimize deployment tools like Ollama for better consistency with llama.cpp.","IsRelevant":true},{"Title":"Yes, you could have 160gb of vram for just about $1000.","ID":"t3_1k0b8wx","Summary":"A DIY setup using 10 AMD MI50 GPUs and an Octominer case achieves 160GB VRAM for ~$1,000. Despite challenges with power consumption and ROCm driver compatibility, it offers high VRAM capacity for large models but faces performance trade-offs compared to dedicated CPUs.","Comment Summary":"Community feedback includes technical tips, cost comparisons, and skepticism about practicality due to power and speed limitations. Some suggest alternative setups like Apple Silicon.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/","Relevance":"This provides a cost-effective VRAM solution for large-scale LLM training/inference, though with trade-offs in speed and power. It's useful for researchers needing extreme VRAM but may not suit all use cases.","IsRelevant":true}],"persona":"LocalLLaMA"}