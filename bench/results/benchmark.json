{"raw_input":["Title: What do I test out / run first?\nID: t3_1kexdgy\nSummary:       Just got her in the mail. Haven't had a chance to put her in yet.    submitted by    /u/Recurrents   [link]   [comments] \nImageDescription: \nComment:       Just got her in the mail. Haven't had a chance to put her in yet.    submitted by    /u/Recurrents   [link]   [comments] \nComment: First run home. Preferably safely. \nComment: llama 3.2 1b \nComment: Bro is loaded. How many kidneys did you sell for that?! \nComment: LLAMA 405B Q.000016 \nComment: all the new qwen 3 models \nComment: sexy ass card \nComment: Old School Runescape \nComment: Are they selling those already? \nComment: Download cuda and make sure your pytorch is the cuda version \nComment: Can it run Crysis? \nComment: You bought it just to benchmark it, didn't you? \nComment: Hello Kitty Island Adventures, butters would be proud of you. \nComment: Would you mind sharing or DMing retailer info? I don‚Äôt have a preferred vendor and am curious on your experience. \nComment: Llama 3.3 70b at 8-bit. Would be interesting to see how many tokens per second gives. \nComment: Try Super Mario Bros ü•∏ \nComment: you don't need it. gimme that. \nComment: Rtx pro 6000 is 96Gb it is beast. Without pro is 48gb. I really want to know how many FOPS it is. Or the t/s for a deepseek 70B or largest model it can fit. \nComment: Get some silly concurrency going on qwen 3 32b awq and run the aider benchmark. \nComment: Flux to generate pics of your dream Audi. Find out your use case and try some models that fit. I was first impressed by GLM 4 in one shot coding, but it fails to use other tools. Mistral small is my daily driver currently. It's even fluent in most languages. \nComment: That‚Äôs some expensive computer hardware. Congratulations. \nComment: That‚Äôs our serial number now \nComment: Your power connectors. \nComment: i cant imagine spending that much money on a gpu with that power connector \nComment: Houston we have lift off  https://preview.redd.it/v3z4prno2wye1.png?width=780\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=6a6156b3fc0818b93b0459a14c86a0e0dd1d70d7 \nComment: https://preview.redd.it/5bnvabxayvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=9516acddbdda888267887c823c70c25db1ba8c6e New card installed! \nComment: Is it better then a h100 performance wise? i know the vram is slightly bigger. \nComment: Quake I \nComment: Everything. In all seriousness, I would reaaally like to see the benchmarks on that thing \nComment: Old school runescape \nComment: Cancer research. \nComment: OT, but run 3Dmark and confirm if it really is faster in games than the 5090 (for once in the history of workstation cards). \nComment: dude you are so lucky congrats!! run every qwen 3 model and make videos! i hear you stream, how about a live stream using llama.cpp and testing out models, or lm studio. this card is so awesome üòç \nComment: Something like Gemma 3 27B/Mistral small-3/Qwen 3 32B with maximum context size? \nComment: Qwen3 and don‚Äôt look back \nComment: Qwen 30B A3B q8 has something around 30 GB file size. Should run very fast and have plent of room for context. \nComment: Bios \nComment: Where did you buy it from? \nComment: About $12,000 to $16,000 for the 48gb vram editions here .. not sure we can get the 96gb \nComment: What CPU are you pairing with? Linux? \nComment: https://preview.redd.it/glj9rjmk9vye1.jpeg?width=1280\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=64d6eac13d0a6aaed4b500953bfd300dcea46322 \nComment: ü•∫ü•πüò≠ \nComment: Wan 2.1 fp 32 model \nComment: Crysis. \nComment: Haha I thought it had a plaid pattern printed on it üòÖ \nComment: Dude so cheesed, could've even wait to get home \nComment: Hey, I was looking to buy one as well, how much did you pay and how long did it take to arrive. They are releasing so many cards these days I get confused. \nComment: How much \nComment: what version is it? Max‚ÄìQ? Workstation edition? Etc‚Ä¶ \nComment: GTA V \nComment: Grounding strap. \nComment: Crysis \nComment: Plex Media Server. But make sure to hack your drivers. \nComment: Mate, share some benchmarks!  I‚Äôm about ready to pull the trigger on one too, but the price gouging here is insane. They‚Äôre still selling Ampere A6000s for 6‚Äì7K AUD, and the Ada version is going for as much as 12K.  Instead of dropping prices on the older cards, they‚Äôre just marking up the new Blackwell ones way above MSRP. The server variant of this exact card is already sitting at 17K AUD (~11K USD)‚Äîabsolute piss take tbh. \nComment: Image and clip generation \nComment: I think I'll stream getting some LLMs and comfyui up tomorrow and the next few days. give a follow if you want to be notified https://twitch.tv/faustcircuits \nComment: Get that unsloth 235B Qwen3 model at Q2K_XL. It should fit. Q2 is the most efficient size when it comes to benchmark score to size ratio according to unsloths documentation. It should be fast AF too since only 22B active parameters.  \nComment: Nice! Still waiting for mine. Can you let me know if you are able to disable ECC or not? \nComment: what Audi is that? S4? \nComment: Nice. Run stuff and share stats! Would be cool to see. \nComment: https://preview.redd.it/ems9w2z6yvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=76b13f186be7cb783727c000bda533c92c1e8c56 here is the old card lol \n","Title: Visa is looking for vibe coders - thoughts?\nID: t3_1keolh9\nSummary:         submitted by    /u/eastwindtoday   [link]   [comments] \nImageDescription: ## Image Analysis: Visa \"Vibe Coder\" Job Requirements\n\nThe image presents a snippet of text outlining requirements for a position at Visa, seemingly related to \"Vibe Coding\" as indicated by the post title. Here's a detailed breakdown:\n\n**What is shown:**\n\n*   **Textual List:** The core content is a bullet-point list of desired skills and experience.\n*   **Logo:** A small, orange \"technical\" logo is present in the bottom right corner. This suggests the requirements are for a technical role, likely within an engineering or data science team.\n*   **Repetitive Text:** A large block of seemingly random characters (likely placeholder or filler text) is present below the skill list. This appears to be a deliberate obfuscation, possibly for visual effect or to prevent easy automated scraping of the requirements.\n\n**Technical Details:**\n\n*   **Vector Databases:** Explicit mention of experience with vector databases ‚Äì specifically **ChromaDB and Conecone**. This is a key indicator. Vector databases are used for similarity search on embeddings, crucial in modern AI applications like recommendation systems, semantic search, and RAG (Retrieval-Augmented Generation).\n*   **Embeddings:** The requirement for experience with embedding models directly ties into the vector database need.  These models (likely transformer-based) convert data into numerical vectors representing semantic meaning.\n*   **Containerization:**  Experience with **Docker** is requested, indicating a need for deploying and managing applications in containerized environments.\n*   **Kubernetes:**  The inclusion of **Kubernetes** suggests a need for orchestration and scaling of containerized applications, likely in a production environment.\n*   **Frontend Design Systems:**  Experience with **Lovable and V0** (likely referring to V0.dev) indicates a need for building user interfaces, potentially around AI-powered features or tools. V0 is a platform specifically focused on building and showcasing open-source AI projects.\n*   **Data Science Background:**  A \"big plus\" is given to candidates with a background in Data Science.\n*   **Problem-Solving Skills:**  Emphasis on strong problem-solving skills, typical for any engineering role.\n\n**Relation to Post Title (\"Visa is looking for vibe coders - thoughts?\"):**\n\nThe term \"vibe coding\" appears to be a marketing term used by Visa. Based on the requirements, it likely refers to engineers who can rapidly prototype and build AI-powered features with a focus on user experience. The emphasis on frontend design systems (Lovable, V0) and the rapid prototyping nature of AI projects suggests a focus on quickly iterating based on user feedback. The vector database and embedding experience points to building AI features that understand semantic meaning, which is often a core component of \"vibe\" or intuitive user interactions.\n\n**Key Insights:**\n\n*   **AI Focus:** Visa is actively investing in AI capabilities, specifically around semantic understanding and user-facing applications.\n*   **Rapid Prototyping:** The role likely involves building and iterating on AI features quickly, potentially using open-source tools.\n*   **Full Stack Potential:** The combination of backend (vector databases, embeddings) and frontend (design systems) requirements suggests a full-stack role.\n*   **Modern Tech Stack:** The use of technologies like ChromaDB, Kubernetes and V0 indicates a modern engineering environment.\n*   **RAG likely in play:** The combination of vector databases and embeddings strongly suggests that the team is working on Retrieval-Augmented Generation (RAG) applications.\n\n\n\n\nComment:         submitted by    /u/eastwindtoday   [link]   [comments] \nComment: The way it's written with the other requirements makes me think that they want a programmer able to write vibe coding tools, not specifically hiring a vibe coder for programming. \nComment: \"Background in Data Science is a big plus\" - Be prepared to be payed less than a proper data scientist. \nComment: Dumb. Vibe coding is a bit ill-defined but what I see is a niche where people like product managers can use AI to build tiny tools that would be hard to get into a product team's priority list. In particular what I saw was using Zapier which is a user-friendly automation tool that lets you run snippets of Python or js inside workflows and now lets you generate them with AI. Great way to build little utilities without a developer but also very narrow, low stakes applications that aren't critical path. This job req wants familiarity with vector DBs and containers which means they want an actual ...\nComment: Vibe coding 2025 is same as slapping an untested, ill designed, 700-lines-a-function Python script on anything and calling it a day in the 2015s Edit: You scrolled down on some requirements tho https://www.visa.ca/fr_ca/jobs/REF061638W \u0026gt; Strong proficiency with Python, FastAPI and PostgreSQL for backend development. \nComment: These days, stupid HRs are stuffing the job requirements sections with as many tech buzzwords as possible, regardless of whether these buzzwords actually reflect the technologies used in the company. I feel like \"vibe coding\" may be one of those buzzwords. \nComment: now exactly nobody will know why credit card applications failed \nComment: Maybe we should all \"vibe-pay\" our Visa card bills. \nComment: Click click, sold all visa stocks instantly \nComment: I for one am happy my CC transactions are handled by vibed up systems. \nComment: they are starting up an ml intership for students next summer in poland too \nComment: I don't even know what to think. My general reaction of most job posts is to assume the companily already has someone they're gonna hire, and the position is only being posted as a formality and to make it appear that the company has fair hiring practices (even though requirements weed out people that are perfectly capable of being trained). \nComment: I know some folks working with such teams. They have ambitions to build vibe coding tools (like cursor) but without the dependency on letting data go out of the org (from a financial reg pov). Visa‚Äôs CTO is one of the highest paid CTOs in the US, and they are building agents replicating (gamma.ai, OpenAI deep research, Agentic Commerce) to start with. \nComment: They are even Vibe-using Caps. That's so Vibe. It gives me Bad Vibes. \nComment: That job posting is clear as mud. Title is an obvious mismatch for the skillset. \nComment: well I'm glad none of my credit cards are visa...  \nComment: what could possibly go wrong? \nComment: If they are listing the tech stacks that should be enough. \nComment: At least they're not requiring minimum 15 years of experience \nComment: Lmao \nComment: AI generated job posting ? :p \nComment: Vibe coding doesn't work. It takes me 10-20 prompts to generate anything useful with gemini 2.5.  Sometimes 30+ prompts even with explicit instructions. \nComment: And I thought it was a big deal 15 years ago when I talked NASA into letting me put ‚Äúauto-generated‚Äù guidance and control code into a satellite. That particular NASA center had never done such a thing, and I had to pop their cherry on it (as a contractor). Partly because a colleague of mine was doing the same thing on a bigger project, and he really wanted to be able to say his project wasn‚Äôt the first. The ‚Äúauto-generation‚Äù step is just compiling MatLab down into C code, and from there is just standard C compilation on the target environment. But the MatLab becomes the source code, ...\nComment: As someone already posted, these are not the full requirements, but another thing also:  Vibe-coding is expensive. Requests can quickly become $1-2 a piece. Over the course of a month, you could easily rack up $500+ if you're using it liberally. Now consider that you're already paying a regular salary, put this on top, and it becomes kind of unsustainable.  The money has to come from somewhere, and it's most probably the base pay. \nComment: I don't know much about developers as vibe coders. But, a lot of project managers \u0026amp; UI UX folks and product managers are starting to learn these tools (lovable, cursor, etc.,) and will roll out the basic screens for evaluation.  Also, this will start getting back to actual developers on justifying their timelines, complexity justifications, etc., This is mostly as nobody gives a fuck about code maintainability, reusability, tech debt, security, etc,.  Other side to this coin is 90% of code written by \u0026lt;5 years experienced devs are not following these standards anyways ü§∑‚Äç‚ôÇÔ∏è 90% o...\n","Title: UI-Tars-1.5 reasoning never fails to entertain me.\nID: t3_1keo3te\nSummary:       7B parameter computer use agent.    submitted by    /u/Impressive_Half_2819   [link]   [comments] \nImageDescription: ## Image Analysis: UI-Tars-1.5 Reasoning \u0026 LLM Interaction\n\nThe image depicts a screenshot of a user interface (UI) seemingly built to interact with a Large Language Model (LLM). The UI is styled to resemble a macOS application window. Here's a detailed breakdown:\n\n**1. UI Elements \u0026 Layout:**\n\n*   **LLM Provider and Model Selection:** A prominent section allows the user to choose an LLM provider (not visible in detail) and a specific model.  A dropdown menu is present, currently displaying \"Custom Model...\" as the selected option. Below this is a text input field labeled ‚ÄúCustom Model Name‚Äù, populated with \"ByteDance-Seed/UI-Tars-1.5-7B\". This strongly suggests the user is interacting with a locally hosted or privately accessible LLM.\n*   **Chat Interface:** A large text area labeled \"Textbox\" is present, indicating a conversational input field. The prompt below it reads ‚ÄúAsk me to perform tasks in a virtual macOS environment‚Äù.\n*   **LLM Thoughts Display:** A section titled \"UI-Tars Thoughts\" displays the LLM's internal reasoning process. The text reveals a fascinating behavior: the model is attempting to explain its thought process *before* executing a task. It states it wants to search for a repository, then acknowledges its inability to do so directly (\"I'm not rid of this pop-up\"). This suggests a self-awareness or meta-cognitive loop.\n*   **Provider Base URL:** A long, seemingly random string of characters is displayed below the model name. This likely represents a URL pointing to the API endpoint or hosting location of the LLM, potentially used for direct communication.\n\n**2. Technical Details \u0026 Implications:**\n\n*   **Model Name: ByteDance-Seed/UI-Tars-1.5-7B:** This is a crucial piece of information. \"ByteDance\" indicates the model was developed or fine-tuned by ByteDance (the company behind TikTok). \"UI-Tars\" likely refers to a project name or specific architecture focused on UI interaction. The \"1.5-7B\" signifies version 1.5 and a model size of approximately 7 billion parameters.  This is a relatively large model, capable of complex reasoning and generation tasks.\n*   **Virtual macOS Environment:** The prompt requesting a task within a virtual macOS environment is significant. This suggests the LLM has been trained to understand and potentially interact with a simulated macOS environment, possibly through code generation or API calls.\n*   **\"Pop-up\" Issue:** The LLM's struggle with a \"pop-up\" suggests it is attempting to perform actions within the virtual environment (like searching for a repository) but is constrained by its access or capabilities. This could be due to security restrictions, lack of necessary tools within the environment, or limitations in its API access.\n*   **Reasoning Display:** The explicit display of the LLM's \"thoughts\" is a key feature. This allows for debugging, understanding its decision-making process, and potentially guiding it towards more accurate or desired outcomes. This is a common technique in research LLMs to improve interpretability and control.\n\n**3. Relation to Post Title (\"UI-Tars-1.5 reasoning never fails to entertain me\"):**\n\nThe post title directly references the model name (\"UI-Tars-1.5\"). The image showcases *why* it's entertaining: the LLM is exhibiting a surprisingly human-like behavior of explaining its thought process, acknowledging limitations (\"I'm not rid of this pop-up\"), and attempting to solve a problem before executing it. The \"never fails\" suggests this behavior is consistent and amusing to the user.\n\n**4. Key Insights:**\n\n*   The image demonstrates a sophisticated LLM designed for UI interaction, potentially capable of automating tasks within a virtual environment.\n*   The explicit reasoning display is valuable for understanding and debugging the model's behavior.\n*   The LLM exhibits a degree of self-awareness and meta-cognition, acknowledging its limitations.\n*   The \"pop-up\" issue highlights the challenges of integrating LLMs with real-world environments and tools.\n*   The model is likely a research project focused on improving LLM interpretability, control, and UI automation capabilities.\n\n\n\n\nComment:       7B parameter computer use agent.    submitted by    /u/Impressive_Half_2819   [link]   [comments] \nComment: What's more important here is the model used - ByteDance-Seed/UI-TARS-1.5-7B the model which it is meant to be used with, so how did you make it work? Because last time I checked I haven't seen that model being converted to GGUF format, nor having vision support added into llama.cpp for it. \nComment: I guess: https://github.com/trycua/cua \nComment: When you train a model to use computers for humans and do the tiresome ToS reading, but it can't be bothered to do it either \nComment: Most probably trained on Gen-Z data. \nComment: https://preview.redd.it/4ignwtxrhuye1.jpeg?width=1079\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=e5d30270b51854c061aebdd502448897c63fed18 \nComment: On one hand, I guess I'd like the language model to read language on my behalf - on the other hand I wouldn't want the model to decide the cookies policy warrants user review or some other distraction so maybe skipping it is for the best after all. It does seem reading the pop-up falls within the scope of accessing the site to search for a repository \nComment: Try out yourself using cu/a! \nComment: I mean, fair \nComment: tiktok ai getting lazy \nComment: TARS, would you set your attention span setting to 8 for me? \nComment: It‚Äôs the defaut personality? \n","Title: Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)\nID: t3_1kepuli\nSummary:       Hey LocalLlama! We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon. We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device). Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist. We figured we'd kickstart this since we already maintain this benchmarking infra/tooling at RunLocal for our enterprise customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and TFLite to date, so a few things are still WIP for GGUF support.  Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill Tokens=512, Num Generation Tokens=128). GGUFs are from Unsloth üêê Qwen3 GGUF benchmarks on laptops Qwen3 GGUF benchmarks on phones You can see more of the benchmark data for Qwen3 here. We realize there are so many variables (devices, backends, etc.) that interpreting the data is currently h...\nImageDescription: ## Image Analysis: Qwen3 Performance Benchmarks\n\nThe image displays a screenshot of what appears to be a user interface (UI) for running and benchmarking the Qwen3 large language model (LLM) across a diverse set of devices. The UI is dark-themed with a card-based layout, likely from an application or web service designed to facilitate LLM inference and performance evaluation.\n\n**Key UI Elements \u0026 Technical Details:**\n\n* **Model Selection:** Several Qwen3 model variants are listed, including:\n    * `DepthAnythingV2`: Suggests integration with a depth estimation model alongside Qwen3, potentially for multimodal applications.\n    * `Llama-3_8B-Instruct-0R_8_gpu`:  Clearly indicates benchmarking of the Llama 3 8B Instruct model, likely as a comparative baseline. The `_gpu` suffix suggests GPU acceleration is being used for this model.\n    * `Kokoro-82M`: A smaller model variant, likely for faster inference on resource-constrained devices.\n* **Benchmark Metrics:** Each card displays several key metrics:\n    * `toks/s`: Tokens per second ‚Äì a primary measure of inference speed.  This is the core benchmark reported in the post title.\n    * `RAM utilization`: Indicates memory usage during inference, crucial for understanding device compatibility and performance bottlenecks.\n    * `Text Generation`:  A likely indicator of the model's ability to generate coherent text.\n    * `Depth Estimation`:  Present for DepthAnythingV2, indicating performance metrics related to depth map creation.\n* **Device Information (Implicit):** While not explicitly shown, the post title (\"~50 devices\") implies that each card represents benchmark results from a specific device (iOS, Android, Mac, Windows). The varying metrics suggest performance differences across these platforms.\n* **Timestamp:** Each card includes a timestamp (e.g., \"8d ago\") indicating when the benchmark was run, allowing for tracking performance changes over time.\n* **\"View Benchmarks\" Button:**  Suggests detailed benchmark reports are available for each model/device combination.\n\n**Relation to Post Title:**\n\nThe image directly illustrates the core content of the post title: performance benchmarks for Qwen3 (and comparative models like Llama 3) across a wide range of devices. The `toks/s` and RAM utilization metrics are prominently displayed, confirming the focus of the post.\n\n**Key Insights:**\n\n* **Cross-Platform Evaluation:** The UI is designed for comprehensive benchmarking across diverse hardware and operating systems.\n* **Performance Variability:**  The varying metrics (even for the same model) suggest significant performance differences based on device capabilities.\n* **Resource Optimization:** The inclusion of smaller models like Kokoro-82M indicates an effort to optimize LLM inference for resource-constrained devices (e.g., mobile phones).\n* **Comparative Analysis:** Benchmarking against Llama 3 provides a valuable point of reference for evaluating Qwen3's performance.\n* **Real-World Testing:** The timestamps suggest these are results from actual device runs, providing practical insights into LLM performance in real-world scenarios.\n\n\n\n\nComment:       Hey LocalLlama! We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon. We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device). Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist. We figured we'd kickstart this since we alr...\nComment: Iphone 16's Metal performance is pretty impressive for 1.6b-q8. But I do wonder why q8's performance is faster than q4 in that particular setup. \nComment: It‚Äôs interesting to see that performance in m4 is pretty similar in both cpu and gpu \nComment: There's one edge factor you missed - on Metal backend when you get OOM you get completely wrong results. For example on Qwen3 8B Q4 your results are like this: - MacBook Pro M1, 8GB = 99232.83tok/s prefill, 2133.70tok/s generation - MacBook Pro M3, 8GB = 90508.66tok/s prefill, 2507.50tok/s generation If you wouldn't get OOM the correct results for that model should be around ~100-150tok/s prefill and ~10tok/s generation. Additionally, all results for RAM usage on Apple silicon \u0026amp; Metal are not correct. In terms of your UX/UI there's tons of stuff that should be improved. but to not make thi...\nComment: How to run on metal on iphone 16 pro? I have pocketpal app and how to switch from cpu to metal? \nComment: if i'm reading this correctly the load time on cpu is better than gpu/metal for macbook pro but the gpu/metal is less memory intensive? also metal perf on iphone 16 is pretty impressive. \nComment: How do I run this on Android? Rn it just crashes \nComment: Why is Q8 faster than Q4??? \nComment: For laptops, is vulkan using the igpu ? \nComment: according to this data on iphone 16 you have 24 t/s on Q8 and 22 t/s on Q4 why so tiny models? \nComment: The iPhone 16e is listed to have the A18 Pro SoC but it actually has the A18.  https://preview.redd.it/h1dx2hgphvye1.png?width=623\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c2f8794bf27c6a9042b81040ebefa09873eae989 \n","Title: Qwen 30B A3B performance degradation with KV quantization\nID: t3_1kewkno\nSummary: I came across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4 that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization. I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens. I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens without finding any solution. Then I saw that sunpazed didn't used KV quantization and tried the same: boom! First time right. It worked with Q5_K_S and also with Q4_K_XL For who wants more details I leave here a gist https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef Do you have any report of performance degradation with long generations on Qwen3 30B A3B and KV quantization?    submitted by    /u/fakezeta   [link]   [comments]\nImageDescription: \nComment: I came across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4 that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization. I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens. I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens wit...\nComment: What KV quant level were you using? IMO on llama.cpp you shouldn‚Äôt push it past Q8_0. Q4_0 cache quant tanks quality in any model and especially models that heavily leverage GQA.  \nComment: I have one rule: I always test ALL new models without flash attention and with full 16bit KV cache. \nComment: Which KV quantization are you using? Don't have time to run this test right now, but I usually use -ctk q8_0 -ctv q5_1 (requires -DGGML_CUDA_FA_ALL_QUANTS=on) \nComment: Ive only tried KV quantization once and saw that any amount of it makes models super dumb. Not sure why anybody uses it tbh \nComment: Which quantization did you use initially? \nComment: Interested here since I'm running a q6 \nComment: Use these parameters: Thinking Mode Settings: Temperature = 0.6 Min_P = 0.0 Top_P = 0.95 TopK = 20 Non-Thinking Mode Settings: Temperature = 0.7  Min_P = 0.0 Top_P = 0.8 TopK = 20 \nComment: Could you please tell us how to disable KV cache quantisation? I'd also like to check the difference. What is the difference in the amount of memory used with KV running at fp16 in comparison with regular q4? \nComment: I‚Äôm confused. Isn‚Äôt K_M KV quantization? And yet you said Qwen 30b solved the rest with Q4 K_M? \nComment: Of course  Cache should always be fp16 even Q8 has degradation. Only flash attention is ok...ish ( as is fp16 ) \n","Title: QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.\nID: t3_1kenk4f\nSummary:       All models are from Bartowski - q4km version Test only HTML frontend. My assessment lauout quality from 0 to 10 Prompt \"Generate a beautiful website for Steve's pc repair using a single html script.\" QwQ 32b - 3/10 - poor layout but ..works , very basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b Qwen 3 32b - 6/10 - much better looks but still not too complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b272399ba8140785feb429a196ecc5173 GLM-4-32b 9/10 - looks insanely good , quality layout like sonnet 3.7 easily - 1500+ code lines https://preview.redd.it/3zj2lr2ahsye1.png?width=2469\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964 GLM-4-32b is insanely good for html code frontend. I say that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most. Other coding language like python , c , c++ or any other quality of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on the seme level but for html and JavaScript ... is GREAT.    submitte...\nImageDescription: \nComment:       All models are from Bartowski - q4km version Test only HTML frontend. My assessment lauout quality from 0 to 10 Prompt \"Generate a beautiful website for Steve's pc repair using a single html script.\" QwQ 32b - 3/10 - poor layout but ..works , very basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b Qwen 3 32b - 6/10 - much better looks but still not too complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b2...\nComment: Yeah, I have also tried to generate webpages with a couple of models, like GLM-4, Qwen3, Phi-4 Reasoning, etc. GLM-4 is so far the clear winner at these tasks. It's a gem in my model collection. \nComment: I've created a list of one-shot generated HTML pages using different models, big and small. https://blog.kekepower.com/ai/ \nComment: Ironically, so far userscript (javascript): Qwen 3 32B \u0026gt; GLM-4-0414. Don't get me wrong. I love GLM-4-0414, but it feels like it lacked the required understanding for my particular requests that Qwen 3 32B understood well. \nComment: Whats the temp? Did you rerun multiple times? \nComment: Why not try a slightly more complex task? E.g. a mini-game?  Create a single-HTML-page game using Babylon.js where the player controls a ship and moves about the open sea exploring islands to find treasure. A single, small map with 3 islands of which only one has the treasure is enough  In the first reply, the camera works, but WASD didn't. I copied-pasted the errors from the console a couple times and WASD works now. It looks terrible, but I guess that's expected without external assets. https://preview.redd.it/lf5foomq1tye1.png?width=1288\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b02ed407b3fbef31bf...\nComment: GLM falls flat on its face when I try to continue developing after the first prompt. It feels like a model trained (very well) for one-shots \nComment: Try UIGEN-T2 for html generation! There's also a react model.  https://huggingface.co/Tesslate/UIGEN-T2 \nComment: Would be interesting to see how these results compare to the recent Tesslate/UIGEN-T2-7B. It's a tuned version of Qwen 2.5 Coder 7B specifically for UI generation. \nComment: What quants what context window size? Ollama default size will kill QWQ reasoning if you don‚Äôt know how to set it up properly. \nComment: Still kind of freaked out that smaller Qwen 3 models are probably as good at website development as I was as a teenager. And a damn sight quicker too. \nComment: Thanks for posting the eval. Would be curious to see the prompt used as well. \nComment: Interesting. I ran the same input on QwQ with these settings: Temp: 0.6 Top p: 0.95 Min p: 0.0 Top k: 40 And quite a bit different output. Output: https://pastebin.com/Ntc8QQfH \nComment: GPT 4.1 is clearly the winner here in my opinion as well as claude sonnet 3.7 \n","Title: LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!\nID: t3_1keoint\nSummary:       You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B) mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert) tl;dr; I highly recommend doing a git pull and re-building your ik_llama.cpp or llama.cpp repo to take advantage of recent major performance improvements just released. The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving r/LocalLLaMA community! If you have enough VRAM to fully offload and already have an existing \"normal\" quant of Qwen3 MoE then you'll get a little more speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork! Details I spent yesterday compiling and running benhmarks on the newest versions of both ik_llama.cpp and mainline llama.cpp. For those that don't know, ikawrakow was an early contributor to mainline llama.cpp working on important features that have since trickled down into ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my understanding) the ik_llama.cpp fork was built and has a number of ...\nImageDescription: \nComment:       You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B) mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert) tl;dr; I highly recommend doing a git pull and re-building your ik_llama.cpp or llama.cpp repo to take advantage of recent major performance improvements just released. The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving r/LocalLLaMA community! If you have enough VRAM to fully offload and already have an existing \"normal\" quant of Qwen3 MoE then you'll get ...\nComment: I'm currently running ik_llama.cpp with Qwen3-235B-A22 on a Xeon E5-2680v4, that's a 10 year old CPU with 128GB ddr4 memory, and a single RTX3090. I'm getting 7 tok/s generation, very usable if you don't use reasoning. BTW the server is multi-GPU but ik_llama.cpp just crash trying to use multiple-gpus, but I don't think it would improve speed a lot, as the CPU is always the bottleneck. \nComment: Could you explain how to read your pictures? I see orange plot below red plot, so ik_llama.cpp is slower than llama.cpp? \nComment: Can you post some of the commands you use for the benchmarks? I want to tinker to see what is best for my use case \nComment: Oh, just updated. My rig is busy for running deepseek \u0026amp; ik_llama (1 week jobs). I will update after that :) \nComment: Maybe GGUF will now give same speed as MLX on Mac devices \nComment: I have a 3090. Doesn't this say it's slower, not faster? \nComment: https://preview.redd.it/0zroyhg1qsye1.png?width=3404\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b3e55128b1aac3f6d2ddfbd22597b9cd6d7dd02c In my limited testing you probably want to go with ik_llama.cpp for fully offloaded non-MoE models like the recent GLM-4 which is crazy efficient on kv-cache VRAM usage due to its GQA design. \nComment: I just pulled and rebuilt and I'm now actually going about 15 tps slower. My previous build was from about a week ago, and I was getting an eval time of about 54 tps. Now I'm only getting 39 tokens per second, so pretty significant drop. I just downloaded the latest unsloth model I'm running on 2 3090s, using this command: ``` .\\bin\\Release\\llama-server.exe -m C:\\shared-drive\\llm_models\\unsloth-2-Qwen3-30B-A3B-128K-Q8_0.gguf --host 0.0.0.0 --ctx-size 50000 --n-predict 10000 --jinja --tensor-split 14,14 --top_k 20 --min_p 0.0 --top_p 0.8 --flash-attn --n-gpu-layers 9999 --threads 24 ``` Prompt:...\nComment: How close is llamacpp to vLLM and exllama now? \nComment: Seems like it is related to CUDA only, so I guess only for people with Nvidia cards and not folks on Apple Silicon and others. \n","Title: Apparently shipping AI platforms is a thing now as per this post from the Qwen X account\nID: t3_1kebauw\nSummary:         submitted by    /u/MushroomGecko   [link]   [comments] \nImageDescription: ## Image Analysis: Qwen AI Platform Shipping Announcement\n\nThe image is a promotional graphic, likely intended for social media (given the cartoon style and celebratory elements), announcing the shipping of a Qwen AI platform. It features two anthropomorphic animal characters: a sloth and a bear, both appearing to be mascots. The bear is presenting a flower to the sloth.\n\n**Key Visual Elements \u0026 Technical Implications:**\n\n* **Mascots:** The use of mascots suggests a focus on accessibility and user-friendliness.  This is common in platforms aiming for broader adoption, particularly those targeting developers or non-technical users.\n* **\"Qwen\" Branding:** While not explicitly stated, the post title strongly implies these characters are associated with the Qwen AI platform.  Qwen is known for its open-source large language models (LLMs) developed by Alibaba.\n* **Celebratory Tone:** The flower presentation and overall aesthetic indicate a positive announcement ‚Äì likely the release or availability of a shipping product.  This could refer to:\n    * **Model Weights:** Availability for download and deployment (e.g., Qwen-1.5).\n    * **API Access:** Launch of a commercial API for accessing Qwen models.\n    * **Software Package/SDK:** Release of tools to facilitate integration and deployment (e.g., a Docker image, Python library).\n    * **Pre-built Deployment:** A fully packaged solution for easier deployment on cloud platforms.\n\n**Relating to the Post Title (\"Apparently shipping AI platforms is a thing now...\")**\n\nThe image directly supports the post's claim.  Traditionally, LLMs were primarily research projects or required significant engineering effort to deploy. The image suggests Qwen is moving beyond this by offering a readily \"shippable\" platform ‚Äì implying reduced complexity for users.  The cartoon style further emphasizes this ease of use and accessibility.\n\n**Key Insights:**\n\n* **Productization of LLMs:**  Qwen is actively working to make its models more accessible through a packaged platform. This aligns with the broader trend of LLM productization, where models are no longer just research outputs but deployable services.\n* **Focus on User Experience:** The use of mascots and a friendly aesthetic suggests Qwen is prioritizing ease-of-use and developer experience.\n* **Potential for Wider Adoption:**  A \"shippable\" platform lowers the barrier to entry, potentially leading to increased adoption of Qwen models in various applications.\n\n\n\nWithout further context (e.g., accompanying text, website links), it's difficult to pinpoint the exact nature of the \"shipping\" ‚Äì but the image clearly indicates a significant step towards making Qwen models more readily available to developers and users.\nComment:         submitted by    /u/MushroomGecko   [link]   [comments] \nComment: Ah yes, the AI romcom. \nComment: theres lore now? \nComment: -Ok, lets see, i want to make love to you-. -But...wait. no no \nComment: What do you mean by \"shipping AI platforms\"? \nComment: Can't wait for the competitions to see which model writes the best fan fiction \nComment: Should AI companies also create their own VTuber characters that use their models? \nComment: Qwen guy looks mean. His expression is like he's never felt love. \nComment: I ship it! \nComment: And those two ship often \nComment: Hell yeah \nComment: Has anyone noticed better performance for similar quants of the same model by different creators? \nComment: Oh no \nComment: Love the contribution to the OSS these guys make!! Pure awesomeness \nComment: ü§¶üèª‚Äç‚ôÇÔ∏è \nComment: Anyone feeding into this just entirely lacks a social life, why are some of you like this? \n","Title: IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models\nID: t3_1kedu0d\nSummary:         submitted by    /u/ab2377   [link]   [comments] \nImageDescription: \nComment:   \n                \n            submitted by   \n            /u/ab2377   [link]\n              [comments]\n            \nComment: Hope they\n            can release a larger one like 30b-a3b  \nComment: so a new\n            architecture, more moe goodness  \"Whereas prior generations\n            of Granite LLMs utilized a conventional transformer architecture, all models in the\n            Granite 4.0 family utilize a new hybrid Mamba-2/Transformer architecture, marrying the\n            speed and efficiency of Mamba with the precision of transformer-based self-attention.\n            Granite 4.0 Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts\n            (MoE) model, with 7B total parameters and only 1B active parameters at inference\n            time. Many of the innovat...\nComment: Please\n            look here: https://huggingface.co/ibm-granite/granite-4.0-tiny-preview/discussions/2\n            gabegoodhart IBM\n            Granite org 1\n            day ago Since this model is hot-off-the-press, we\n            don't have inference support in llama.cpp yet.\n            I'm actively working on it, but since this is one of the first major models\n            using a hybrid-recurrent architecture, there are a number of in-flight architectural\n            changes in the codebase that need to all meet up to get this supported. We'll\n            keep you posted! gabegoodhart IBM...\nComment: i hope we\n            can see some larger models too! I really want them to scale those more experimental\n            architectures and see where it leads. I think there is huge potential in combining\n            attention with hidden state models. attention to understand context, hidden state to\n            think ahead, remember key information etc. \nComment: Read the\n            full thing. It‚Äôs worth it. \nComment: Holy,\n            this actually looks really good. IBM might actually be able to catch up with Alibaba\n            with this one. \nComment: Neat but\n            unless folks really start working to help add support for mamba architectures to\n            llama.cpp it'll be dead on arrival. It would be great to see\n            the folks at /u/IBM step up and help out\n            llama.cpp to support things like this. \nComment: \n            The Granite 4.0 architecture uses no positional encoding (NoPE). Our testing\n            demonstrates convincingly that this has had no adverse effect on long-context\n            performance.   This is interesting. Are there any\n            papers that explain why this still works? \nComment: Looking\n            very promising... \nComment: Is IBM\n            going to be the silent winner? It‚Äôs impressive that their tiny model is 8b MOE and\n            likely to perform at the same level as their previous dense 8b: Granite 4.0\n            Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts (MoE) model,\n            with 7B total parameters and only 1B active parameters at inference time.\n            I hope their efforts attempt to improve in https://fiction.live/stories/Fiction-liveBench-April-6-2025/oQdzQvKHw8JyXbN87\n            and not just passkey testing. \nComment: I'm just a dreamer without much background in\n            ML stuff. Can anyone with sense comment on how likely it is we'll ever see\n            something that might be so efficient it'll run well on CPU? I mean, this model\n            already sounds pretty exciting from an efficiency perspective. Wondering if\n            we've exhausted architectural changes that would e.g. reduce memory bandwidth\n            requirements \nComment: ‚ÄúWe‚Äôre\n            excited to continue pre-training Granite 4.0 Tiny, given such promising results so early\n            in the process. We‚Äôre also excited to apply our learnings from post-training Granite\n            3.3, particularly with regard to reasoning capabilities and complex instruction\n            following, to the new models. Like its predecessors in Granite 3.2 and Granite 3.3,\n            Granite 4.0 Tiny Preview offers toggleablethinking on andthinking off functionality\n            (though its reasoning-focused post-training is very much incomplete).‚Äù\n            I hope s...\nComment: Now if\n            only we could get IBM to sell a version of their AI card to the public\n            \nComment: ibm doing\n            better work than meta theyre surprisingly becoming a big player in open source (for\n            small models) \nComment: I wonder\n            what is prompt processing speed for semi-recurrent stuff compared to transformers.\n            Transformers have fantastic prompt processing speed like 1000t/s easy even on crap like\n            3060, but slow down during token generation as context grows. This seems to be the other\n            way around, slow PP but fast TG. I might be completely\n            wrong. \nComment: Large\n            datasets: all of Harry Potter series asking questions like, what would have to change in\n            the series for Harry to end up with Hermione or for Voldemort to win. It‚Äôs a series\n            everyone knows fairly well and requires details in the story and the story\n            whole. \nComment: I\n            remember seeing this model a few days ago. There's no gguf so i cant try it out.\n            I guess there's not a lot of interest in this moe or it's not currently\n            possibly to make ggufs for it at the moment.  Webui stopped working\n            for me last year after i updated it and I've never been able to get it working\n            right since then, so been using lm studio appimages. That program runs everything good\n            for me but only runs ggufs. \nComment: Yall need\n            to learn Transformers and stop hating on llama.cpp \n"],"results":[{"Title":"UI-Tars-1.5 reasoning never fails to entertain me.","ID":"t3_1keo3te","Summary":"This post showcases a custom UI interface for interacting with the ByteDance-Seed/UI-Tars-1.5-7B model, a 7-billion-parameter language model developed by ByteDance. The UI allows users to execute tasks in a virtual macOS environment while displaying the model's internal reasoning process. The image reveals the model's attempt to explain its actions (e.g., 'I want to search for a repository') and acknowledge limitations ('I'm not rid of this pop-up'), suggesting meta-cognitive capabilities. The model's architecture appears optimized for UI automation, with explicit 'thought' displays to improve interpretability. Technical details include a custom model name field and a provider base URL, indicating potential local deployment or private API access.","CommentSummary":"The community focused on technical implementation challenges, with users questioning how the model was adapted for llama.cpp (e.g., GGUF format compatibility) and noting its apparent lack of vision capabilities. Some comments humorously critiqued the model's inability to handle 'pop-ups' or cookie policies, while others speculated about its training data ('Gen-Z data') and potential applications. Discussions highlighted both fascination with the model's reasoning display and skepticism about its practical limitations in real-world environments. A link to a GitHub repository (cua) was shared, suggesting community-driven experimentation with similar tools.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/","Relevance":"","IsRelevant":true},{"Title":"Visa is looking for vibe coders - thoughts?","ID":"t3_1keolh9","Summary":"This Reddit post analyzes a Visa job posting for 'Vibe Coders,' which appears to seek engineers with expertise in AI-driven development. The requirements include vector databases (ChromaDB, Conecone), embeddings, Docker, Kubernetes, and frontend systems like Lovable and V0.dev. The role suggests a focus on rapid prototyping of AI features, potentially leveraging Retrieval-Augmented Generation (RAG) for semantic understanding. The post highlights Visa's investment in modern AI infrastructure, blending backend (vector databases, Python) and frontend (design systems) skills. However, the term 'vibe coding' remains ambiguous, with some interpreting it as a marketing buzzword for AI-powered tooling rather than a formal role.","CommentSummary":"The community is divided on the 'vibe coding' term, with some mocking it as vague marketing jargon. Comments criticize the use of buzzwords in job postings and question whether the role is a formality. Others note the practical tech stack (e.g., Kubernetes, Python) but express skepticism about work-life balance or pay equity. A few users speculate that 'vibe coding' might relate to low-stakes AI tools for internal use, while others highlight concerns about over-reliance on untested AI-generated code. The discussion also touches on broader frustrations with corporate job descriptions and the sustainability of AI-driven workflows.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/","Relevance":"","IsRelevant":true},{"Title":"Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)","ID":"t3_1kepuli","Summary":"This post details an open-source initiative to benchmark Qwen3's performance across 50+ devices, including iOS, Android, Mac, and Windows. The benchmarks measure tokens per second (toks/s), RAM utilization, and other metrics for model formats like Core ML, ONNX, TFLite, and GGUF. The image shows a dark-themed UI with cards displaying results for Qwen3 variants (e.g., DepthAnythingV2, Llama-3_8B-Instruct) and comparative models. Key metrics include prefill/generation speeds (e.g., 99,232.83 tok/s prefill on M1 Mac) and RAM usage, with notes on device-specific variability. The project aims to create a standardized hub for evaluating model viability in real-world applications, though GGUF support is still WIP.","CommentSummary":"The community appreciates the initiative but raises technical concerns. A user highlights a critical issue: Metal backend OOM errors on Apple devices produce misleading results (e.g., inflated tok/s metrics for Qwen3 8B Q4). Others discuss puzzling performance differences (e.g., Q8 vs. Q4 faster on iPhone 16) and usability questions (e.g., switching to Metal backend on iPhone 16 Pro). Comments also note discrepancies in RAM usage reporting on Apple silicon and request clarification on Vulkan's IGPU usage. While some praise the iPhone 16's Metal performance, others report Android crashes, indicating platform-specific challenges. The discussion underscores the complexity of cross-device LLM optimization.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/","Relevance":"","IsRelevant":true},{"Title":"QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison","ID":"t3_1kenk4f","Summary":"This post compares the HTML generation capabilities of three large language models (QwQ 32b, Qwen 3 32b, and GLM-4-32B) using a standardized prompt to create a 'beautiful website for Steve's pc repair' in a single HTML script. The evaluation includes quantitative metrics like line counts (250-1500+ lines) and qualitative layout scores (3/10 to 9/10). GLM-4-32B receives the highest rating (9/10) for producing visually appealing, complex layouts resembling Sonnet 3.7's quality, while QwQ 32b scores poorly (3/10) with minimalistic code. The analysis notes GLM-4-32B's exceptional performance in HTML/JavaScript but suggests its code quality declines for other languages like Python. Technical details include model quantization (q4km) and specific image links showcasing generated outputs.","CommentSummary":"The community highlights GLM-4-32B's dominance in HTML generation but notes limitations in other domains. Users compare it to models like Qwen 3, Phi-4, and Tesslate/UIGEN-T2, with some preferring Qwen 3 for specific tasks. Discussions about quantization settings (e.g., Ollama's context window) and prompt engineering emerge, while others test models on complex tasks like Babylon.js games. Critics mention GLM-4's difficulty in continuing development after initial prompts and note that smaller Qwen models match teenage-level coding ability. The post sparks interest in specialized UI-generation models and prompts requests for additional benchmarks.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/","Relevance":"","IsRelevant":true},{"Title":"IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models","ID":"t3_1kedu0d","Summary":"IBM has unveiled the Granite 4.0 Tiny Preview, a hybrid Mamba-2/Transformer model with 7B total parameters and only 1B active parameters at inference time, leveraging a fine-grained mixture-of-experts (MoE) architecture. The model replaces traditional positional encoding with a 'NoPE' approach, demonstrating strong long-context performance without sacrificing efficiency. It combines Mamba's sequential processing speed with transformer-based self-attention, enabling potential improvements in reasoning and context handling. Early benchmarks suggest it may match or exceed previous dense 8B models while maintaining lower inference costs.","CommentSummary":"The community is excited about the hybrid architecture's potential but concerned about limited tooling support. Users highlighted the need for llama.cpp compatibility to enable broader adoption, noting that current inference frameworks lag behind the model's innovation. Discussions focused on the significance of NoPE, with requests for academic papers explaining its effectiveness. Some expressed hope for larger models and real-world performance comparisons, while others speculated about CPU-friendly efficiency. A recurring theme was admiration for IBM's open-source progress, though skepticism remained about practical deployment challenges.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/","Relevance":"","IsRelevant":true},{"Title":"Qwen 30B A3B performance degradation with KV quantization","ID":"t3_1kewkno","Summary":"A Reddit user reported inconsistent performance issues with Qwen 30B when using KV quantization, particularly during the OpenAI cypher test. The user found that Qwen 30B solved the test with Q4_K_M quantization but encountered repetition loops or incorrect conclusions when using Unsloth's Q4_K_XL and Bartowski's Q5_K_S. Disabling KV quantization resolved the issues, suggesting a critical dependency on KV cache precision. The post includes detailed testing logs and comparisons across quantization levels, highlighting how different configurations affect model behavior. The user also shared a gist with parameters and asks if others have observed similar degradation in long-generation tasks.","CommentSummary":"The community debated the impact of KV quantization on model quality, with several users warning against pushing quantization beyond Q8_0. Comments emphasized that Q4_K_M and Q4_0 cache quantization significantly degrade performance, especially for models using GQA (Grouped Query Attention). Some users recommended testing without flash attention and using FP16 KV caches for reliability. A few questioned why KV quantization is used at all, given its negative effects. Discussions also touched on parameter tuning (e.g., temperature, top_p) and memory trade-offs between FP16 and Q4 quantization. The thread highlighted a lack of clear documentation on KV quantization's risks for large models like Qwen 30B.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/","Relevance":"","IsRelevant":true},{"Title":"LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!","ID":"t3_1keoint","Summary":"This post highlights significant performance improvements in both mainline llama.cpp and the ik_llama.cpp fork, particularly for Qwen3 MoE models (235B and 30B). The updates focus on faster execution for fully offloaded Qwen3 MoE (single expert) and hybrid CPU+GPU configurations. The author recommends rebuilding repositories to leverage these gains, noting that mainline llama.cpp benefits existing 'normal' quantizations, while ik_llama.cpp offers SotA iqN_k quants and better hybrid offload support. Benchmarks show mixed results, with some users reporting speed increases and others noting potential regressions depending on hardware and model configurations.","CommentSummary":"The community is enthusiastic about the performance gains but highlights practical challenges. Users report 7 tok/s generation on older hardware (Xeon E5-2680v4 + RTX3090) with ik_llama.cpp, though multi-GPU support remains unstable. A debate arises over benchmark plots showing ik_llama.cpp sometimes lagging behind mainline llama.cpp, prompting requests for clarification on test setups. Some users express concern about a 15 TPS drop after updates, while others speculate on GGUF's potential to match MLX performance on Macs. Discussions emphasize hardware limitations (e.g., CPU bottlenecks) and CUDA-specific optimizations, with users questioning cross-platform compatibility for non-Nvidia devices.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/","Relevance":"","IsRelevant":true},{"Title":"Apparently shipping AI platforms is a thing now as per this post from the Qwen X account","ID":"t3_1kebauw","Summary":"The post features a promotional image announcing the 'shipping' of the Qwen AI platform, likely referring to the release of a deployable solution for Alibaba's open-source large language models (LLMs). The image uses anthropomorphic mascots (a sloth and bear) to emphasize accessibility, suggesting Qwen is moving beyond research-focused LLMs to provide user-friendly tools. While the exact nature of the 'shipping' (e.g., model weights, API access, or SDKs) remains unspecified, the announcement aligns with trends in making LLMs more commercially viable. The image's celebratory tone and cartoon style highlight a focus on developer experience, though no technical benchmarks or performance metrics are provided.","CommentSummary":"The comments range from lighthearted humor (e.g., 'AI romcom,' 'shipping AI platforms' as a pun on relationships) to speculative discussions about Qwen's potential features. Some users question the technical specifics of 'shipping,' while others joke about AI fan fiction competitions or VTuber integrations. A few commend Qwen's open-source contributions, but the overall tone is more playful than technical. The absence of substantive technical debate suggests the post primarily serves as a marketing teaser rather than a deep technical release.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kebauw/apparently_shipping_ai_platforms_is_a_thing_now/","Relevance":"","IsRelevant":false},{"Title":"What do I test out / run first?","ID":"t3_1kexdgy","Summary":"A Reddit user received new GPU hardware (likely an expensive workstation card like the RTX 4090 or similar) and is seeking recommendations for initial tests. The post includes comments suggesting running specific large language models (LLMs) like Llama 3.2, Qwen 3, and benchmarks, as well as gaming titles like Crysis and GTA V. Discussions touch on hardware specifications (VRAM, CUDA, PyTorch), performance comparisons, and practical use cases like AI model inference and 3D rendering. The post lacks specific technical details about the hardware itself but reflects community interest in AI/ML workloads.","CommentSummary":"The comments reveal a mix of enthusiasm and practicality. Users suggest testing LLMs (e.g., Llama 3.3 70b, Qwen 3 32B) and benchmarks to evaluate performance, while others joke about running games or asking about hardware specs (e.g., VRAM, ECC mode). There is curiosity about real-world applications like cancer research or media servers. Some comments highlight concerns about hardware cost, availability, and driver issues. The community emphasizes sharing benchmarks and experiences, with a few users planning live streams or detailed reviews.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/","Relevance":"","IsRelevant":false}],"persona":"LocalLLaMa"}