{"raw_input":["Title: What do I test out / run first?\nID: t3_1kexdgy\nSummary:       Just got her in the mail. Haven't had a chance to put her in yet.    submitted by    /u/Recurrents   [link]   [comments] \nImageDescription: \nComment:       Just got her in the mail. Haven't had a chance to put her in yet.    submitted by    /u/Recurrents   [link]   [comments] \nComment: First run home. Preferably safely. \nComment: llama 3.2 1b \nComment: Bro is loaded. How many kidneys did you sell for that?! \nComment: LLAMA 405B Q.000016 \nComment: all the new qwen 3 models \nComment: sexy ass card \nComment: Old School Runescape \nComment: Are they selling those already? \nComment: Download cuda and make sure your pytorch is the cuda version \nComment: Can it run Crysis? \nComment: You bought it just to benchmark it, didn't you? \nComment: Hello Kitty Island Adventures, butters would be proud of you. \nComment: Would you mind sharing or DMing retailer info? I don‚Äôt have a preferred vendor and am curious on your experience. \nComment: Llama 3.3 70b at 8-bit. Would be interesting to see how many tokens per second gives. \nComment: Try Super Mario Bros ü•∏ \nComment: you don't need it. gimme that. \nComment: Rtx pro 6000 is 96Gb it is beast. Without pro is 48gb. I really want to know how many FOPS it is. Or the t/s for a deepseek 70B or largest model it can fit. \nComment: Get some silly concurrency going on qwen 3 32b awq and run the aider benchmark. \nComment: Flux to generate pics of your dream Audi. Find out your use case and try some models that fit. I was first impressed by GLM 4 in one shot coding, but it fails to use other tools. Mistral small is my daily driver currently. It's even fluent in most languages. \nComment: That‚Äôs some expensive computer hardware. Congratulations. \nComment: That‚Äôs our serial number now \nComment: Your power connectors. \nComment: i cant imagine spending that much money on a gpu with that power connector \nComment: Houston we have lift off  https://preview.redd.it/v3z4prno2wye1.png?width=780\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=6a6156b3fc0818b93b0459a14c86a0e0dd1d70d7 \nComment: https://preview.redd.it/5bnvabxayvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=9516acddbdda888267887c823c70c25db1ba8c6e New card installed! \nComment: Is it better then a h100 performance wise? i know the vram is slightly bigger. \nComment: Quake I \nComment: Everything. In all seriousness, I would reaaally like to see the benchmarks on that thing \nComment: Old school runescape \nComment: Cancer research. \nComment: OT, but run 3Dmark and confirm if it really is faster in games than the 5090 (for once in the history of workstation cards). \nComment: dude you are so lucky congrats!! run every qwen 3 model and make videos! i hear you stream, how about a live stream using llama.cpp and testing out models, or lm studio. this card is so awesome üòç \nComment: Something like Gemma 3 27B/Mistral small-3/Qwen 3 32B with maximum context size? \nComment: Qwen3 and don‚Äôt look back \nComment: Qwen 30B A3B q8 has something around 30 GB file size. Should run very fast and have plent of room for context. \nComment: Bios \nComment: Where did you buy it from? \nComment: About $12,000 to $16,000 for the 48gb vram editions here .. not sure we can get the 96gb \nComment: What CPU are you pairing with? Linux? \nComment: https://preview.redd.it/glj9rjmk9vye1.jpeg?width=1280\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=64d6eac13d0a6aaed4b500953bfd300dcea46322 \nComment: ü•∫ü•πüò≠ \nComment: Wan 2.1 fp 32 model \nComment: Crysis. \nComment: Haha I thought it had a plaid pattern printed on it üòÖ \nComment: Dude so cheesed, could've even wait to get home \nComment: Hey, I was looking to buy one as well, how much did you pay and how long did it take to arrive. They are releasing so many cards these days I get confused. \nComment: How much \nComment: what version is it? Max‚ÄìQ? Workstation edition? Etc‚Ä¶ \nComment: GTA V \nComment: Grounding strap. \nComment: Crysis \nComment: Plex Media Server. But make sure to hack your drivers. \nComment: Mate, share some benchmarks!  I‚Äôm about ready to pull the trigger on one too, but the price gouging here is insane. They‚Äôre still selling Ampere A6000s for 6‚Äì7K AUD, and the Ada version is going for as much as 12K.  Instead of dropping prices on the older cards, they‚Äôre just marking up the new Blackwell ones way above MSRP. The server variant of this exact card is already sitting at 17K AUD (~11K USD)‚Äîabsolute piss take tbh. \nComment: Image and clip generation \nComment: I think I'll stream getting some LLMs and comfyui up tomorrow and the next few days. give a follow if you want to be notified https://twitch.tv/faustcircuits \nComment: Get that unsloth 235B Qwen3 model at Q2K_XL. It should fit. Q2 is the most efficient size when it comes to benchmark score to size ratio according to unsloths documentation. It should be fast AF too since only 22B active parameters.  \nComment: Nice! Still waiting for mine. Can you let me know if you are able to disable ECC or not? \nComment: what Audi is that? S4? \nComment: Nice. Run stuff and share stats! Would be cool to see. \nComment: https://preview.redd.it/ems9w2z6yvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=76b13f186be7cb783727c000bda533c92c1e8c56 here is the old card lol \n","Title: Visa is looking for vibe coders - thoughts?\nID: t3_1keolh9\nSummary:         submitted by    /u/eastwindtoday   [link]   [comments] \nImageDescription: ### Image Description:\n\nThe image is a job posting from **Visa** for the role of **Staff Gen AI Engineer (Engineer)**. The post is dated **3 days ago** and is located in **Austin, TX, US**. Below are the key elements visible in the image:\n\n#### 1. **Header Information:**\n   - **Company:** Visa\n   - **Role Title:** Staff Gen AI Engineer (Engineer)\n   - **Location:** Austin, TX, US\n   - **Date Posted:** 3 days ago\n\n#### 2. **Skill Tags:**\n   A set of tags indicates the technical skills and technologies relevant to the role:\n   - **LLM**\n   - **generative ai**\n   - **python**\n   - **postgres**\n   - **javascript**\n   - **prompt engineer**\n   - **claude**\n\n#### 3. **Job Description:**\n   The job description lists specific technical requirements and qualifications:\n   - **Experience with vector databases:** Familiarity with tools like Pinecone, ChromaDB, and embedding models.\n   - **Experience with containerization technologies:** Knowledge of Docker, Kubernetes, and other related technologies.\n   - **Familiarity with Vibe coding tools:** Explicit mention of tools such as Bolt, Lovable, and VO.\n   - **Frontend Design Systems:** Experience with frontend design systems is a plus.\n   - **Data Science Background:** Strong background in data science is beneficial.\n   - **Problem-solving skills:** Ability to tackle complex technical challenges.\n\n#### 4. **Highlighted Text:**\n   - The phrase \"**Vibe coding tools such as Bolt, Lovable, VO**\" is highlighted in blue, drawing attention to these specific tools.\n\n#### 5. **UI Elements:**\n   - The post is formatted in a clean, structured layout typical of professional job boards or internal company systems.\n   - A small orange circular icon with a smiling face is visible in the bottom-right corner, possibly representing an employee or company mascot.\n\n---\n\n### Technical Details Visible:\n1. **AI and ML Skills:**\n   - The role requires expertise in LLMs (Large Language Models) and generative AI, indicating a focus on advanced machine learning applications.\n   - Familiarity with embedding models and vector databases (e.g., Pinecone, ChromaDB) suggests work in areas like semantic search or similarity-based applications.\n\n2. **Database and Backend Skills:**\n   - Proficiency in PostgreSQL is required, indicating experience with relational databases.\n   - Containerization technologies like Docker and Kubernetes are highlighted, suggesting a need for cloud-native or scalable backend systems.\n\n3. **Frontend and Design Experience:**\n   - Familiarity with frontend design systems is mentioned, indicating a blend of backend and frontend expertise.\n\n4. **Data Science Background:**\n   - A strong background in data science is a plus, implying that the role may involve processing and analyzing large datasets.\n\n5. **Vibe Coding Tools:**\n   - The mention of \"Bolt,\" \"Lovable,\" and \"VO\" as Vibe coding tools is specific to Visa's internal or proprietary frameworks. These are likely custom or specialized tools designed for their development environment.\n\n---\n\n### Relation to the Post Title:\nThe post title, **\"Visa is looking for vibe coders - thoughts?\"**, directly relates to the image. The phrase \"vibe coders\" is a play on words, referring to individuals who are familiar with Visa's internal coding tools (Vibe coding tools). The job description explicitly mentions \"Familiarity with Vibe coding tools such as Bolt, Lovable, VO,\" which aligns perfectly with the post title. This suggests that Visa is seeking developers who are not only technically skilled in AI, data science, and backend/frontend technologies but also have experience with their proprietary tools.\n\n---\n\n### Key Insights:\n1. **Proprietary Toolset:**\n   - Visa is emphasizing the use of their internal Vibe coding tools (Bolt, Lovable, VO), indicating that candidates should either have prior experience with these tools or be open to learning them quickly.\n\n2. **Hybrid Skillset:**\n   - The role requires a blend of skills, including AI/ML expertise (LLMs and generative models), backend development (PostgreSQL, Kubernetes), frontend design systems, and data science. This suggests a need for versatile engineers who can work across different domains.\n\n3. **Focus on Generative AI:**\n   - The emphasis on LLMs and generative AI indicates that Visa is actively investing in these technologies, likely for applications such as natural language processing, conversational AI, or other advanced use cases.\n\n4. **Internal Culture and Tools:**\n   - The mention of \"Vibe coding tools\" highlights Visa's internal development ecosystem. This could reflect a company culture that values proprietary solutions and custom tools to optimize workflows.\n\n5. **Problem-solving Orientation:**\n   - The requirement for strong problem-solving skills and the ability to tackle complex technical challenges suggests that Visa is looking for engineers who can handle challenging, real-world problems.\n\n---\n\n### Conclusion:\nThe image provides a detailed look into Visa's hiring requirements for a Staff Gen AI Engineer. The emphasis on Vibe coding tools, combined with skills in generative AI, containerization, and data science, indicates a role that is both technically demanding and aligned with Visa's internal development ecosystem. The post title, \"Visa is looking for vibe coders,\" directly references the proprietary tools mentioned in the job description, emphasizing the importance of familiarity with Visa's internal technology stack. This suggests that candidates should focus on demonstrating their ability to work with custom tools while showcasing a broad range of technical skills.\nComment:         submitted by    /u/eastwindtoday   [link]   [comments] \nComment: The way it's written with the other requirements makes me think that they want a programmer able to write vibe coding tools, not specifically hiring a vibe coder for programming. \nComment: \"Background in Data Science is a big plus\" - Be prepared to be payed less than a proper data scientist. \nComment: Dumb. Vibe coding is a bit ill-defined but what I see is a niche where people like product managers can use AI to build tiny tools that would be hard to get into a product team's priority list. In particular what I saw was using Zapier which is a user-friendly automation tool that lets you run snippets of Python or js inside workflows and now lets you generate them with AI. Great way to build little utilities without a developer but also very narrow, low stakes applications that aren't critical path. This job req wants familiarity with vector DBs and containers which means they want an actual experienced software engineer. Someone who can use a coding assistant but probably doesn't need one. That's not vibe coding. \nComment: Vibe coding 2025 is same as slapping an untested, ill designed, 700-lines-a-function Python script on anything and calling it a day in the 2015s Edit: You scrolled down on some requirements tho https://www.visa.ca/fr_ca/jobs/REF061638W \u0026gt; Strong proficiency with Python, FastAPI and PostgreSQL for backend development. \nComment: These days, stupid HRs are stuffing the job requirements sections with as many tech buzzwords as possible, regardless of whether these buzzwords actually reflect the technologies used in the company. I feel like \"vibe coding\" may be one of those buzzwords. \nComment: now exactly nobody will know why credit card applications failed \nComment: Maybe we should all \"vibe-pay\" our Visa card bills. \nComment: Click click, sold all visa stocks instantly \nComment: I for one am happy my CC transactions are handled by vibed up systems. \nComment: they are starting up an ml intership for students next summer in poland too \nComment: I don't even know what to think. My general reaction of most job posts is to assume the companily already has someone they're gonna hire, and the position is only being posted as a formality and to make it appear that the company has fair hiring practices (even though requirements weed out people that are perfectly capable of being trained). \nComment: I know some folks working with such teams. They have ambitions to build vibe coding tools (like cursor) but without the dependency on letting data go out of the org (from a financial reg pov). Visa‚Äôs CTO is one of the highest paid CTOs in the US, and they are building agents replicating (gamma.ai, OpenAI deep research, Agentic Commerce) to start with. \nComment: They are even Vibe-using Caps. That's so Vibe. It gives me Bad Vibes. \nComment: That job posting is clear as mud. Title is an obvious mismatch for the skillset. \nComment: well I'm glad none of my credit cards are visa...  \nComment: what could possibly go wrong? \nComment: If they are listing the tech stacks that should be enough. \nComment: At least they're not requiring minimum 15 years of experience \nComment: Lmao \nComment: AI generated job posting ? :p \nComment: Vibe coding doesn't work. It takes me 10-20 prompts to generate anything useful with gemini 2.5.  Sometimes 30+ prompts even with explicit instructions. \nComment: And I thought it was a big deal 15 years ago when I talked NASA into letting me put ‚Äúauto-generated‚Äù guidance and control code into a satellite. That particular NASA center had never done such a thing, and I had to pop their cherry on it (as a contractor). Partly because a colleague of mine was doing the same thing on a bigger project, and he really wanted to be able to say his project wasn‚Äôt the first. The ‚Äúauto-generation‚Äù step is just compiling MatLab down into C code, and from there is just standard C compilation on the target environment. But the MatLab becomes the source code, in this case. And you have to version-control the auto-generation configuration as well.  Honestly, with all the other shit we had to deal with on that program, just telling an AI to ‚Äúwrite some code that might look like spacecraft guidance and control software, no particular requirements because we‚Äôre don‚Äôt have the money to actually test the hardware before launch, and most of it is going to fail anyway‚Äù would have been such a stress-saver. That‚Äôs a good vibe. \nComment: As someone already posted, these are not the full requirements, but another thing also:  Vibe-coding is expensive. Requests can quickly become $1-2 a piece. Over the course of a month, you could easily rack up $500+ if you're using it liberally. Now consider that you're already paying a regular salary, put this on top, and it becomes kind of unsustainable.  The money has to come from somewhere, and it's most probably the base pay. \nComment: I don't know much about developers as vibe coders. But, a lot of project managers \u0026amp; UI UX folks and product managers are starting to learn these tools (lovable, cursor, etc.,) and will roll out the basic screens for evaluation.  Also, this will start getting back to actual developers on justifying their timelines, complexity justifications, etc., This is mostly as nobody gives a fuck about code maintainability, reusability, tech debt, security, etc,.  Other side to this coin is 90% of code written by \u0026lt;5 years experienced devs are not following these standards anyways ü§∑‚Äç‚ôÇÔ∏è 90% of products from start-ups and other IT companies never see the light anyways ü§¶‚Äç‚ôÇÔ∏èü§£ \n","Title: UI-Tars-1.5 reasoning never fails to entertain me.\nID: t3_1keo3te\nSummary:       7B parameter computer use agent.    submitted by    /u/Impressive_Half_2819   [link]   [comments] \nImageDescription: ### Image Description\n\nThe image shows a user interface (UI) for configuring and interacting with an AI-based tool named **UI-TARS-1.5**. The interface appears to be part of a web application running locally on `http://127.0.0.1`. Here's a detailed breakdown:\n\n#### **Left Panel: Configuration**\n- **Prerequisites \u0026 Installation**: \n  - This section is likely for setting up the necessary dependencies and installing required components. It does not show any specific details in this view.\n  \n- **Configuration**:\n  - **Agent Loop**: \n    - A dropdown menu is labeled \"Select the agent loop provider,\" and it has an option selected: **UI-TARS**.\n  - **LLM Provider and Model**: \n    - Another dropdown menu labeled \"Select model or choose 'Custom model...'\" to enter a custom name.\n    - The selected option is **ByteDance-Seed/UI-TARS-7B**, indicating a specific language model provider and version.\n  - **Custom Model Name**: \n    - A text field to specify a custom model name if needed.\n  - **Provider Base URL**: \n    - A field for entering the base URL of the LLM provider's API.\n\n#### **Right Panel: Interaction**\n- The right side shows a chatbot interface within the browser, indicating interaction with an AI system.\n  - **Chat Input Area**: \n    - A textbox at the bottom for users to input queries or commands.\n  - **Chat History**:\n    - A conversation thread is visible, showing interactions between the user and the AI.\n    - **User Query**: \n      - The user appears to be interacting with Google and is seeking help with understanding the purpose of some code.\n    - **AI Response**: \n      - The AI responds thoughtfully, explaining its reasoning and actions:\n        1. It acknowledges the user's query about code.\n        2. It expresses that it is not in the mood to read through the code right now.\n        3. It decides to search for information instead of analyzing the code directly, demonstrating a form of decision-making.\n        4. It performs an action (e.g., clicking the \"Accept All\" button) to dismiss a popup, showing its ability to simulate user actions.\n\n#### **Technical Details**\n- The interface is likely built using a modern web framework, given the clean design and browser-based interaction.\n- The configuration panel suggests that the tool is highly customizable, allowing users to:\n  - Choose different agent loop providers.\n  - Select pre-trained models or specify custom LLMs.\n  - Enter a base URL for model providers, indicating support for external APIs.\n- The chatbot interaction demonstrates the tool's ability to:\n  - Simulate user behavior (e.g., clicking buttons).\n  - Reason through tasks and make decisions based on the context.\n  - Provide transparent explanations of its actions.\n\n### Relation to Post Title: \"UI-Tars-1.5 reasoning never fails to entertain me.\"\n\nThe post title suggests that the user finds the reasoning capabilities of UI-TARS-1.5 particularly engaging or amusing. The image supports this by showcasing the following:\n- **Reasoning Process**: \n  - The AI's response demonstrates a clear, step-by-step reasoning process. For example, it explains why it chose not to read the code and opted for a search instead.\n- **Human-like Behavior**: \n  - The AI's ability to simulate user actions (e.g., dismissing a popup) adds an element of realism and interactivity.\n- **Entertaining Nature**: \n  - The transparent explanation of the AI's thought process, combined with its ability to mimic human behavior, likely contributes to the \"entertaining\" aspect mentioned in the post title.\n\n### Key Insights from the Image\n\n1. **Customizability and Flexibility**:\n   - The tool allows users to customize the agent loop, LLM provider, and model, indicating a high degree of flexibility for different use cases.\n\n2. **Agent Loop Integration**:\n   - The \"UI-TARS\" agent loop provider suggests that the tool is designed to simulate user interactions with web interfaces, likely for testing or automation purposes.\n\n3. **LLM Integration**:\n   - The selection of a specific LLM (ByteDance-Seed/UI-TARS-7B) highlights the tool's dependence on large language models for reasoning and decision-making.\n\n4. **Transparent Reasoning**:\n   - The chatbot's detailed explanations of its actions and thought processes provide insight into how the AI makes decisions, which is valuable for debugging or understanding its behavior.\n\n5. **Simulated User Actions**:\n   - The AI's ability to perform simulated actions (e.g., clicking a button) indicates that the tool can interact with web interfaces programmatically, likely for testing or automating tasks.\n\n6. **Entertainment and Engagement**:\n   - The AI's reasoning process, combined with its ability to simulate human-like behavior, aligns with the post title's emphasis on \"entertainment,\" suggesting that users find interacting with this tool both informative and engaging.\n\n### Conclusion\nThe image depicts a sophisticated AI-based tool, UI-TARS-1.5, designed for interacting with web interfaces and performing tasks through reasoning and simulation. The configuration options demonstrate its flexibility, while the chatbot interaction showcases its ability to reason, make decisions, and simulate user actions. The post title reflects the user's appreciation for the tool's entertaining and engaging nature, likely due to its transparent reasoning process and human-like behavior.\nComment:       7B parameter computer use agent.    submitted by    /u/Impressive_Half_2819   [link]   [comments] \nComment: What's more important here is the model used - ByteDance-Seed/UI-TARS-1.5-7B the model which it is meant to be used with, so how did you make it work? Because last time I checked I haven't seen that model being converted to GGUF format, nor having vision support added into llama.cpp for it. \nComment: I guess: https://github.com/trycua/cua \nComment: When you train a model to use computers for humans and do the tiresome ToS reading, but it can't be bothered to do it either \nComment: Most probably trained on Gen-Z data. \nComment: https://preview.redd.it/4ignwtxrhuye1.jpeg?width=1079\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=e5d30270b51854c061aebdd502448897c63fed18 \nComment: On one hand, I guess I'd like the language model to read language on my behalf - on the other hand I wouldn't want the model to decide the cookies policy warrants user review or some other distraction so maybe skipping it is for the best after all. It does seem reading the pop-up falls within the scope of accessing the site to search for a repository \nComment: Try out yourself using cu/a! \nComment: I mean, fair \nComment: tiktok ai getting lazy \nComment: TARS, would you set your attention span setting to 8 for me? \nComment: It‚Äôs the defaut personality? \n","Title: Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)\nID: t3_1kepuli\nSummary:       Hey LocalLlama! We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon. We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device). Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist. We figured we'd kickstart this since we already maintain this benchmarking infra/tooling at RunLocal for our enterprise customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and TFLite to date, so a few things are still WIP for GGUF support.  Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill Tokens=512, Num Generation Tokens=128). GGUFs are from Unsloth üêê Qwen3 GGUF benchmarks on laptops Qwen3 GGUF benchmarks on phones You can see more of the benchmark data for Qwen3 here. We realize there are so many variables (devices, backends, etc.) that interpreting the data is currently harder than it should be. We'll work on that! You can also see benchmarks for a few other models here. If you want to see benchmarks for any others, feel free to request them and we‚Äôll try to publish ASAP! Lastly, you can run your own benchmarks on our devices for free (limited to some degree to avoid our devices melting!). This free/public version is a bit of a frankenstein fork of our enterprise product, so any benchmarks you run would be private to your account. But if there's interest, we can add a way for you to also publish them so that the public benchmarks aren‚Äôt bottlenecked by us.  It‚Äôs still very early days for us with this, so please let us know what would make it better/cooler for the community: https://edgemeter.runlocal.ai/public/pipelines To more on-device AI in production! üí™ https://i.redd.it/aev5rgjazsye1.gif    submitted by    /u/intofuture   [link]   [comments] \nImageDescription: ### Image Description:\n\nThe image shows a user interface from the **RunLocal** platform, which appears to be a tool for managing and benchmarking machine learning models. The interface is dark-themed, with a clean and organized layout. Here are the key elements visible in the image:\n\n#### **1. UI Elements:**\n- **Header:** \n  - The top-left corner displays the RunLocal logo, which is a purple icon with \"F\" and \"L\" intertwined.\n  - The header includes tabs labeled: **Run Benchmark**, **Models**, **Optimization**, and **Pricing**.\n  - There is a search bar for filtering models by name, with the placeholder text: \"Search by name...\".\n  - A dropdown menu labeled **All Tasks** is present, indicating the ability to filter models by task type.\n  - A green button labeled **Add New Model** is visible on the far right.\n\n- **Model List:**\n  - The central part of the interface lists several models, each with details such as:\n    - **Model Name:** e.g., \"Qwen3-7B\", \"Depth Anylyzing Small\", etc.\n    - **Task Type:** e.g., \"Text Generation\", \"Depth Estimation\".\n    - **File Name/Path:** e.g., \"Qwen3-7B-804_8_gpu.pppf\".\n    - **Size:** e.g., \"2.6GB\", indicating the file size of each model.\n    - **Status:** e.g., \"Updated\" or \"Uploaded\".\n  - Each entry has a checkbox on the left, suggesting the ability to select models for actions.\n\n- **Actions Column:**\n  - For each model, there are buttons or links labeled:\n    - **View Details:** For more information about the model.\n    - **Run Benchmark:** To execute benchmarks on the selected model.\n\n#### **2. Models Listed:**\nThe interface displays several models, including:\n- **Qwen3 (Text Generation):** Multiple versions of Qwen3 are listed, such as \"Qwen3-7B\", with file sizes ranging from 2.6GB to larger variants.\n- **Depth Anylyzing Small (Depth Estimation):** A model for depth estimation tasks, with a file size of 2.6GB.\n- **I-Lakes (Text Generation):** Another text generation model with a file size of 2.6GB.\n- **Kakuro-BM (Text Generation):** Another text generation model.\n\n#### **3. Technical Details:**\n- The models are likely stored in a proprietary format, as indicated by the `.pppf` extension (e.g., `Qwen3-7B-804_8_gpu.pppf`).\n- The file sizes suggest these are large models, typical for advanced tasks like text generation and depth estimation.\n- The presence of a \"Run Benchmark\" option indicates that the platform supports performance testing, which aligns with the post title.\n\n#### **4. Platform Features:**\n- The interface suggests that RunLocal is designed for managing, benchmarking, and optimizing machine learning models.\n- The ability to add new models implies support for custom or third-party model integration.\n\n---\n\n### Relation to the Post Title:\n\nThe post title states: **\"Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)\"**. The image directly relates to this title in the following ways:\n- **Qwen3 Models:** Several Qwen3 models are listed, including \"Qwen3-7B\", which is the primary focus of the post.\n- **Benchmarking Feature:** The \"Run Benchmark\" button for each model aligns with the post's emphasis on performance benchmarks.\n- **Cross-Platform Context:** Although not explicitly shown in the image, the mention of \"iOS, Android, Mac, Windows\" suggests that RunLocal supports running these models across different platforms. The UI indicates a centralized management system, which could facilitate cross-platform testing.\n\n---\n\n### Key Insights:\n\n1. **Model Management:**\n   - The RunLocal platform provides a centralized interface for managing models, including uploading, viewing details, and benchmarking.\n   - The list of Qwen3 models suggests a focus on large-scale language models, which are typically resource-intensive.\n\n2. **Benchmarking Capabilities:**\n   - The \"Run Benchmark\" option implies that RunLocal supports performance testing, likely covering metrics such as tokens per second (toks/s) and RAM utilization.\n   - This aligns with the post's focus on evaluating Qwen3 performance across various devices.\n\n3. **Cross-Platform Support:**\n   - While not explicitly shown, the post title mentions testing on multiple platforms (iOS, Android, Mac, Windows). The RunLocal interface likely facilitates this by providing a unified environment for deploying and benchmarking models across different systems.\n\n4. **Scalability:**\n   - The presence of multiple Qwen3 variants (e.g., \"Qwen3-7B\") suggests that the platform can handle large models and different configurations, making it suitable for enterprise-level use.\n\n5. **Customization:**\n   - The \"Add New Model\" button indicates that users can integrate their own models into the platform, expanding its utility beyond preloaded options.\n\n---\n\n### Conclusion:\n\nThe image provides a glimpse into the RunLocal platform's capabilities, emphasizing model management and benchmarking. It directly supports the post's theme by showcasing Qwen3 models, a key focus of the performance benchmarks. The interface suggests a robust tool for evaluating and optimizing machine learning models across various devices, aligning with the technical details implied by the post title.\n\nExternal URL Summaries:\n- https://huggingface.co/collections/unsloth/qwen3-680edabfb790c8c34a242f95: \n\n\n\nThe article introduces a collection of Qwen3 models hosted on Hugging Face by unsloth. It includes various model sizes (from 0.6B to 32B parameters) in multiple formats such as GGUF, 4-bit and 16-bit Safetensors, and FP8. Some variants offer a 128K context length. The models support text generation and are optimized for fine-tuning and deployment, with notes on dynamic quantization, bitsandbytes 4-bit, and FP8 versions.\nComment:       Hey LocalLlama! We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon. We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device). Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist. We figured we'd kickstart this since we already maintain this benchmarking infra/tooling at RunLocal for our enterprise customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and TFLite to date, so a few things are still WIP for GGUF support.  Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill Tokens=512, Num Generation Tokens=128). GGUFs are from Unsloth üêê Qwen3 GGUF benchmarks on laptops Qwen3 GGUF benchmarks on phones You can see more of the benchmark data for Qwen3 here. We realize there are so many variables (devices, backends, etc.) that interpreting the data is currently harder than it should be. We'll work on that! You can also see benchmarks for a few other models here. If you want to see benchmarks for any others, feel free to request them and we‚Äôll try to publish ASAP! Lastly, you can run your own benchmarks on our devices for free (limited to some degree to avoid our devices melting!). This free/public version is a bit of a frankenstein fork of our enterprise product, so any benchmarks you run would be private to your account. But if there's interest, we can add a way for you to also publish them so that the public benchmarks aren‚Äôt bottlenecked by us.  It‚Äôs still very early days for us with this, so please let us know what would make it better/cooler for the community: https://edgemeter.runlocal.ai/public/pipelines To more on-device AI in production! üí™ https://i.redd.it/aev5rgjazsye1.gif    submitted by    /u/intofuture   [link]   [comments] \nComment: Iphone 16's Metal performance is pretty impressive for 1.6b-q8. But I do wonder why q8's performance is faster than q4 in that particular setup. \nComment: It‚Äôs interesting to see that performance in m4 is pretty similar in both cpu and gpu \nComment: There's one edge factor you missed - on Metal backend when you get OOM you get completely wrong results. For example on Qwen3 8B Q4 your results are like this: - MacBook Pro M1, 8GB = 99232.83tok/s prefill, 2133.70tok/s generation - MacBook Pro M3, 8GB = 90508.66tok/s prefill, 2507.50tok/s generation If you wouldn't get OOM the correct results for that model should be around ~100-150tok/s prefill and ~10tok/s generation. Additionally, all results for RAM usage on Apple silicon \u0026amp; Metal are not correct. In terms of your UX/UI there's tons of stuff that should be improved. but to not make this into very long post I'll write about biggest problems that can be fixed rather easily. First, add option to hide columns, there's too much redundant information that should be possible to hide with just couple of clicks. Second, decide on some naming scheme for components and stick with it. https://preview.redd.it/8idjxghteuye1.png?width=91\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=12d1d9317495b745768b9dd012288be1eb804964 I would suggest to get rid of 'Apple'/'Bionic' names altogether - it just adds to complexity and cognitive load to a table that is already very dense. There is no non-Apple M1 in Macbooks or non-Bionic A12 in iPad, so you don't need to clarify that much in a first place and additionally this page is aimed at technical people. Exact same problem with Samsung/Google vs Snapdragon. Third, if both CPU and Metal failed don't create two entries. Table is 2x longer than it should be with results that are non-comparable to anything. Just combine it into one entry. \nComment: How to run on metal on iphone 16 pro? I have pocketpal app and how to switch from cpu to metal? \nComment: if i'm reading this correctly the load time on cpu is better than gpu/metal for macbook pro but the gpu/metal is less memory intensive? also metal perf on iphone 16 is pretty impressive. \nComment: How do I run this on Android? Rn it just crashes \nComment: Why is Q8 faster than Q4??? \nComment: For laptops, is vulkan using the igpu ? \nComment: according to this data on iphone 16 you have 24 t/s on Q8 and 22 t/s on Q4 why so tiny models? \nComment: The iPhone 16e is listed to have the A18 Pro SoC but it actually has the A18.  https://preview.redd.it/h1dx2hgphvye1.png?width=623\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c2f8794bf27c6a9042b81040ebefa09873eae989 \n","Title: Qwen 30B A3B performance degradation with KV quantization\nID: t3_1kewkno\nSummary: I came across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4 that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization. I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens. I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens without finding any solution. Then I saw that sunpazed didn't used KV quantization and tried the same: boom! First time right. It worked with Q5_K_S and also with Q4_K_XL For who wants more details I leave here a gist https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef Do you have any report of performance degradation with long generations on Qwen3 30B A3B and KV quantization?    submitted by    /u/fakezeta   [link]   [comments]\nImageDescription: \n\nExternal URL Summaries:\n- https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4: \n\n\n\nThe article details the successful decryption of a cipher problem first introduced in OpenAI's o1-preview technical paper. Qwen3 30B A3B (4_0 quant) solved the cipher in **5 minutes**, significantly faster than QwQ, which took **32 minutes**. The solution highlights the model's advanced reasoning and cryptographic capabilities.\nComment: I came across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4 that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization. I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens. I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens without finding any solution. Then I saw that sunpazed didn't used KV quantization and tried the same: boom! First time right. It worked with Q5_K_S and also with Q4_K_XL For who wants more details I leave here a gist https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef Do you have any report of performance degradation with long generations on Qwen3 30B A3B and KV quantization?    submitted by    /u/fakezeta   [link]   [comments]\nComment: What KV quant level were you using? IMO on llama.cpp you shouldn‚Äôt push it past Q8_0. Q4_0 cache quant tanks quality in any model and especially models that heavily leverage GQA.  \nComment: I have one rule: I always test ALL new models without flash attention and with full 16bit KV cache. \nComment: Which KV quantization are you using? Don't have time to run this test right now, but I usually use -ctk q8_0 -ctv q5_1 (requires -DGGML_CUDA_FA_ALL_QUANTS=on) \nComment: Ive only tried KV quantization once and saw that any amount of it makes models super dumb. Not sure why anybody uses it tbh \nComment: Which quantization did you use initially? \nComment: Interested here since I'm running a q6 \nComment: Use these parameters: Thinking Mode Settings: Temperature = 0.6 Min_P = 0.0 Top_P = 0.95 TopK = 20 Non-Thinking Mode Settings: Temperature = 0.7  Min_P = 0.0 Top_P = 0.8 TopK = 20 \nComment: Could you please tell us how to disable KV cache quantisation? I'd also like to check the difference. What is the difference in the amount of memory used with KV running at fp16 in comparison with regular q4? \nComment: I‚Äôm confused. Isn‚Äôt K_M KV quantization? And yet you said Qwen 30b solved the rest with Q4 K_M? \nComment: Of course  Cache should always be fp16 even Q8 has degradation. Only flash attention is ok...ish ( as is fp16 ) \n","Title: QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.\nID: t3_1kenk4f\nSummary:       All models are from Bartowski - q4km version Test only HTML frontend. My assessment lauout quality from 0 to 10 Prompt \"Generate a beautiful website for Steve's pc repair using a single html script.\" QwQ 32b - 3/10 - poor layout but ..works , very basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b Qwen 3 32b - 6/10 - much better looks but still not too complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b272399ba8140785feb429a196ecc5173 GLM-4-32b 9/10 - looks insanely good , quality layout like sonnet 3.7 easily - 1500+ code lines https://preview.redd.it/3zj2lr2ahsye1.png?width=2469\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964 GLM-4-32b is insanely good for html code frontend. I say that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most. Other coding language like python , c , c++ or any other quality of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on the seme level but for html and JavaScript ... is GREAT.    submitted by    /u/Healthy-Nebula-3603   [link]   [comments] \nImageDescription: \nComment:       All models are from Bartowski - q4km version Test only HTML frontend. My assessment lauout quality from 0 to 10 Prompt \"Generate a beautiful website for Steve's pc repair using a single html script.\" QwQ 32b - 3/10 - poor layout but ..works , very basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b Qwen 3 32b - 6/10 - much better looks but still not too complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b272399ba8140785feb429a196ecc5173 GLM-4-32b 9/10 - looks insanely good , quality layout like sonnet 3.7 easily - 1500+ code lines https://preview.redd.it/3zj2lr2ahsye1.png?width=2469\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964 GLM-4-32b is insanely good for html code frontend. I say that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most. Other coding language like python , c , c++ or any other quality of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on the seme level but for html and JavaScript ... is GREAT.    submitted by    /u/Healthy-Nebula-3603   [link]   [comments] \nComment: Yeah, I have also tried to generate webpages with a couple of models, like GLM-4, Qwen3, Phi-4 Reasoning, etc. GLM-4 is so far the clear winner at these tasks. It's a gem in my model collection. \nComment: I've created a list of one-shot generated HTML pages using different models, big and small. https://blog.kekepower.com/ai/ \nComment: Ironically, so far userscript (javascript): Qwen 3 32B \u0026gt; GLM-4-0414. Don't get me wrong. I love GLM-4-0414, but it feels like it lacked the required understanding for my particular requests that Qwen 3 32B understood well. \nComment: Whats the temp? Did you rerun multiple times? \nComment: Why not try a slightly more complex task? E.g. a mini-game?  Create a single-HTML-page game using Babylon.js where the player controls a ship and moves about the open sea exploring islands to find treasure. A single, small map with 3 islands of which only one has the treasure is enough  In the first reply, the camera works, but WASD didn't. I copied-pasted the errors from the console a couple times and WASD works now. It looks terrible, but I guess that's expected without external assets. https://preview.redd.it/lf5foomq1tye1.png?width=1288\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b02ed407b3fbef31bf6d83aae5491cacaa93e150 Edit: This is my AWQ quant of Qwen-32B. \nComment: GLM falls flat on its face when I try to continue developing after the first prompt. It feels like a model trained (very well) for one-shots \nComment: Try UIGEN-T2 for html generation! There's also a react model.  https://huggingface.co/Tesslate/UIGEN-T2 \nComment: Would be interesting to see how these results compare to the recent Tesslate/UIGEN-T2-7B. It's a tuned version of Qwen 2.5 Coder 7B specifically for UI generation. \nComment: What quants what context window size? Ollama default size will kill QWQ reasoning if you don‚Äôt know how to set it up properly. \nComment: Still kind of freaked out that smaller Qwen 3 models are probably as good at website development as I was as a teenager. And a damn sight quicker too. \nComment: Thanks for posting the eval. Would be curious to see the prompt used as well. \nComment: Interesting. I ran the same input on QwQ with these settings: Temp: 0.6 Top p: 0.95 Min p: 0.0 Top k: 40 And quite a bit different output. Output: https://pastebin.com/Ntc8QQfH \nComment: GPT 4.1 is clearly the winner here in my opinion as well as claude sonnet 3.7 \n","Title: LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!\nID: t3_1keoint\nSummary:       You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B) mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert) tl;dr; I highly recommend doing a git pull and re-building your ik_llama.cpp or llama.cpp repo to take advantage of recent major performance improvements just released. The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving r/LocalLLaMA community! If you have enough VRAM to fully offload and already have an existing \"normal\" quant of Qwen3 MoE then you'll get a little more speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork! Details I spent yesterday compiling and running benhmarks on the newest versions of both ik_llama.cpp and mainline llama.cpp. For those that don't know, ikawrakow was an early contributor to mainline llama.cpp working on important features that have since trickled down into ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my understanding) the ik_llama.cpp fork was built and has a number of interesting features including SotA iqN_k quantizations that pack in a lot of quality for the size while retaining good speed performance. (These new quants are not available in ollma, lmstudio, koboldcpp, etc.) A few recent PRs made by ikawrakow to ik_llama.cpp and by JohannesGaessler to mainline have boosted performance across the board and especially on CUDA with Flash Attention implementations for Grouped Query Attention (GQA) models and also Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B releases! References  ikawrakow/ik_llama.cpp/pull/370     submitted by    /u/VoidAlchemy   [link]   [comments] \nImageDescription: \n\nExternal URL Summaries:\n- https://github.com/ikawrakow/ik_llama.cpp: \n\n\n\nThis is a fork of **llama.cpp** optimized for better **CPU and hybrid GPU/CPU performance**, featuring new **SOTA quantization methods**, **Bitnet support**, and enhanced **DeepSeek performance** via **MLA, FlashMLA, and fused MoE operations**. It includes numerous performance improvements, such as faster token generation, better batch processing, and support for models like **LLaMA-3-Nemotron**, **Qwen3**, **GLM-4**, and **Gemma3**. Note that **new GGUF formats for DeepSeek-V3/R1/Lite are incompatible**, and users should use original GGUF or convert from HF safetensors. The project is actively maintained with frequent updates and contributions welcome.\nComment:       You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B) mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert) tl;dr; I highly recommend doing a git pull and re-building your ik_llama.cpp or llama.cpp repo to take advantage of recent major performance improvements just released. The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving r/LocalLLaMA community! If you have enough VRAM to fully offload and already have an existing \"normal\" quant of Qwen3 MoE then you'll get a little more speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork! Details I spent yesterday compiling and running benhmarks on the newest versions of both ik_llama.cpp and mainline llama.cpp. For those that don't know, ikawrakow was an early contributor to mainline llama.cpp working on important features that have since trickled down into ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my understanding) the ik_llama.cpp fork was built and has a number of interesting features including SotA iqN_k quantizations that pack in a lot of quality for the size while retaining good speed performance. (These new quants are not available in ollma, lmstudio, koboldcpp, etc.) A few recent PRs made by ikawrakow to ik_llama.cpp and by JohannesGaessler to mainline have boosted performance across the board and especially on CUDA with Flash Attention implementations for Grouped Query Attention (GQA) models and also Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B releases! References  ikawrakow/ik_llama.cpp/pull/370     submitted by    /u/VoidAlchemy   [link]   [comments] \nComment: I'm currently running ik_llama.cpp with Qwen3-235B-A22 on a Xeon E5-2680v4, that's a 10 year old CPU with 128GB ddr4 memory, and a single RTX3090. I'm getting 7 tok/s generation, very usable if you don't use reasoning. BTW the server is multi-GPU but ik_llama.cpp just crash trying to use multiple-gpus, but I don't think it would improve speed a lot, as the CPU is always the bottleneck. \nComment: Could you explain how to read your pictures? I see orange plot below red plot, so ik_llama.cpp is slower than llama.cpp? \nComment: Can you post some of the commands you use for the benchmarks? I want to tinker to see what is best for my use case \nComment: Oh, just updated. My rig is busy for running deepseek \u0026amp; ik_llama (1 week jobs). I will update after that :) \nComment: Maybe GGUF will now give same speed as MLX on Mac devices \nComment: I have a 3090. Doesn't this say it's slower, not faster? \nComment: https://preview.redd.it/0zroyhg1qsye1.png?width=3404\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b3e55128b1aac3f6d2ddfbd22597b9cd6d7dd02c In my limited testing you probably want to go with ik_llama.cpp for fully offloaded non-MoE models like the recent GLM-4 which is crazy efficient on kv-cache VRAM usage due to its GQA design. \nComment: I just pulled and rebuilt and I'm now actually going about 15 tps slower. My previous build was from about a week ago, and I was getting an eval time of about 54 tps. Now I'm only getting 39 tokens per second, so pretty significant drop. I just downloaded the latest unsloth model I'm running on 2 3090s, using this command: ``` .\\bin\\Release\\llama-server.exe -m C:\\shared-drive\\llm_models\\unsloth-2-Qwen3-30B-A3B-128K-Q8_0.gguf --host 0.0.0.0 --ctx-size 50000 --n-predict 10000 --jinja --tensor-split 14,14 --top_k 20 --min_p 0.0 --top_p 0.8 --flash-attn --n-gpu-layers 9999 --threads 24 ``` Prompt: \"tell me a 2 paragraph story\" \nComment: How close is llamacpp to vLLM and exllama now? \nComment: Seems like it is related to CUDA only, so I guess only for people with Nvidia cards and not folks on Apple Silicon and others. \n","Title: Apparently shipping AI platforms is a thing now as per this post from the Qwen X account\nID: t3_1kebauw\nSummary:         submitted by    /u/MushroomGecko   [link]   [comments] \nImageDescription: ### Image Description:\n\nThe image is a colorful and playful graphic featuring two cartoon characters: a sloth on the left and a bear on the right. Both characters have large, expressive eyes, giving them an endearing appearance. The sloth is holding a small orange flower with a yellow center, and the bear appears to be gently receiving it. The background is plain white, which makes the characters and text stand out prominently.\n\n#### Key Elements:\n\n1. **Characters:**\n   - **Sloth:** The sloth has a light brown fur texture, green eyes, and is depicted in a friendly manner. It is holding the flower with both hands.\n   - **Bear:** The bear has light brown fur, a white shirt, and is also depicted in a friendly manner. It looks like it is receiving the flower from the sloth.\n\n2. **Logos and Text:**\n   - On the left side, there is a logo for \"unsloth,\" which appears to be an abstract green design resembling a stylized paw print or leaf, followed by the text \"unsloth.\"\n   - On the right side, there is a purple logo with an abstract design resembling interconnected shapes or letters. Next to it, the text reads \"Qwenen3.\"\n\n3. **Flower:** The flower is small and orange with a yellow center, adding a touch of warmth to the image.\n\n#### Technical Details:\n- The image uses vibrant colors (green, orange, purple) to create a visually appealing and engaging design.\n- The characters are cartoonish with exaggerated features (large eyes, soft expressions), which is typical for branding or promotional materials aimed at a friendly and approachable audience.\n- The logos are simple yet distinctive, suggesting they represent different entities or brands.\n\n### Relation to the Post Title:\nThe post title mentions \"shipping AI platforms\" and refers to a post from the \"Qwen X account.\" The image likely represents a collaboration or partnership between two entities, as indicated by the presence of two distinct logos (\"unsloth\" and \"Qwenen3\"). The act of one character (the sloth) giving a flower to another (the bear) symbolizes collaboration, partnership, or the exchange of something valuable. This aligns with the concept of \"shipping\" in a metaphorical sense, referring to the combination or partnership between two entities (in this case, AI platforms).\n\n### Key Insights:\n1. **Collaboration:** The image suggests a partnership or collaboration between \"unsloth\" and \"Qwenen3,\" likely in the context of AI platforms. The sloth giving a flower to the bear symbolizes mutual cooperation and exchange.\n   \n2. **Branding:** The use of distinct logos (\"unsloth\" and \"Qwenen3\") indicates that both entities are involved in the collaboration. The colorful, friendly design of the characters suggests a focus on approachability and user-friendliness in their branding.\n\n3. **Metaphorical Representation:** The act of giving a flower is often used to symbolize friendship, respect, or the exchange of something valuable. In this context, it likely represents a positive and beneficial collaboration between two AI platforms.\n\n4. **Target Audience:** The cartoonish style of the characters suggests that the image is aimed at a broad audience, potentially including developers, AI enthusiasts, or general consumers interested in technology and collaboration.\n\n### Conclusion:\nThe image effectively communicates a message of partnership and collaboration between two entities, \"unsloth\" and \"Qwenen3,\" using a visually appealing and metaphorical representation. The post title about \"shipping AI platforms\" aligns with the image's theme of combining or partnering two entities, reinforcing the idea that this collaboration involves AI technology. The use of friendly cartoon characters and vibrant colors suggests a focus on approachability and positive engagement with the audience.\nComment:         submitted by    /u/MushroomGecko   [link]   [comments] \nComment: Ah yes, the AI romcom. \nComment: theres lore now? \nComment: -Ok, lets see, i want to make love to you-. -But...wait. no no \nComment: What do you mean by \"shipping AI platforms\"? \nComment: Can't wait for the competitions to see which model writes the best fan fiction \nComment: Should AI companies also create their own VTuber characters that use their models? \nComment: Qwen guy looks mean. His expression is like he's never felt love. \nComment: I ship it! \nComment: And those two ship often \nComment: Hell yeah \nComment: Has anyone noticed better performance for similar quants of the same model by different creators? \nComment: Oh no \nComment: Love the contribution to the OSS these guys make!! Pure awesomeness \nComment: ü§¶üèª‚Äç‚ôÇÔ∏è \nComment: Anyone feeding into this just entirely lacks a social life, why are some of you like this? \n","Title: IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models\nID: t3_1kedu0d\nSummary:         submitted by    /u/ab2377   [link]   [comments] \nImageDescription: \n\nExternal URL Summaries:\n- https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek: \n\n\n\nIBM has released the **Granite 4.0 Tiny Preview**, a compact and efficient language model designed for consumer-grade hardware. Despite being partially trained, it outperforms larger models like Granite 3.3 2B Instruct while using significantly less memory. The model uses a **hybrid Mamba-2/Transformer architecture**, combining the efficiency of Mamba with the precision of transformers. It supports long contexts (up to 128K tokens) and multiple concurrent sessions on modest GPUs. The preview is available on Hugging Face, with full enterprise support expected later this summer. The model‚Äôs **NoPE (no positional encoding)** design allows for flexible context lengths, and it is part of IBM‚Äôs broader Granite 4.0 family, which includes Small and Medium variants.\nComment:   \n                \n            submitted by   \n            /u/ab2377   [link]\n              [comments]\n            \nComment: Hope they\n            can release a larger one like 30b-a3b  \nComment: so a new\n            architecture, more moe goodness  \"Whereas prior generations\n            of Granite LLMs utilized a conventional transformer architecture, all models in the\n            Granite 4.0 family utilize a new hybrid Mamba-2/Transformer architecture, marrying the\n            speed and efficiency of Mamba with the precision of transformer-based self-attention.\n            Granite 4.0 Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts\n            (MoE) model, with 7B total parameters and only 1B active parameters at inference\n            time. Many of the innovations informing the Granite 4 architecture\n            arose from IBM Research‚Äôs collaboration with the original Mamba creators on Bamba, an\n            experimental open source hybrid model whose successor (Bamba v2) was released earlier\n            this week.\" \nComment: Please\n            look here: https://huggingface.co/ibm-granite/granite-4.0-tiny-preview/discussions/2\n            gabegoodhart IBM\n            Granite org 1\n            day ago Since this model is hot-off-the-press, we\n            don't have inference support in llama.cpp yet.\n            I'm actively working on it, but since this is one of the first major models\n            using a hybrid-recurrent architecture, there are a number of in-flight architectural\n            changes in the codebase that need to all meet up to get this supported. We'll\n            keep you posted! gabegoodhart IBM\n            Granite org 1\n            day ago We definitely expect the model quality to improve\n            beyond this preview. So far, this preview checkpoint has been trained on ~2.5T tokens,\n            but it will continue to train up to ~15T tokens before final release.\n            \nComment: i hope we\n            can see some larger models too! I really want them to scale those more experimental\n            architectures and see where it leads. I think there is huge potential in combining\n            attention with hidden state models. attention to understand context, hidden state to\n            think ahead, remember key information etc. \nComment: Read the\n            full thing. It‚Äôs worth it. \nComment: Holy,\n            this actually looks really good. IBM might actually be able to catch up with Alibaba\n            with this one. \nComment: Neat but\n            unless folks really start working to help add support for mamba architectures to\n            llama.cpp it'll be dead on arrival. It would be great to see\n            the folks at /u/IBM step up and help out\n            llama.cpp to support things like this. \nComment: \n            The Granite 4.0 architecture uses no positional encoding (NoPE). Our testing\n            demonstrates convincingly that this has had no adverse effect on long-context\n            performance.   This is interesting. Are there any\n            papers that explain why this still works? \nComment: Looking\n            very promising... \nComment: Is IBM\n            going to be the silent winner? It‚Äôs impressive that their tiny model is 8b MOE and\n            likely to perform at the same level as their previous dense 8b: Granite 4.0\n            Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts (MoE) model,\n            with 7B total parameters and only 1B active parameters at inference time.\n            I hope their efforts attempt to improve in https://fiction.live/stories/Fiction-liveBench-April-6-2025/oQdzQvKHw8JyXbN87\n            and not just passkey testing. \nComment: I'm just a dreamer without much background in\n            ML stuff. Can anyone with sense comment on how likely it is we'll ever see\n            something that might be so efficient it'll run well on CPU? I mean, this model\n            already sounds pretty exciting from an efficiency perspective. Wondering if\n            we've exhausted architectural changes that would e.g. reduce memory bandwidth\n            requirements \nComment: ‚ÄúWe‚Äôre\n            excited to continue pre-training Granite 4.0 Tiny, given such promising results so early\n            in the process. We‚Äôre also excited to apply our learnings from post-training Granite\n            3.3, particularly with regard to reasoning capabilities and complex instruction\n            following, to the new models. Like its predecessors in Granite 3.2 and Granite 3.3,\n            Granite 4.0 Tiny Preview offers toggleablethinking on andthinking off functionality\n            (though its reasoning-focused post-training is very much incomplete).‚Äù\n            I hope some of this involves interacting with fictional text in a creative\n            fashion: scene summaries, character profiles, plot outlining, hypothetical change\n            impacts - books are great datasets just like large code bases and just need a good set\n            of training data ‚Äî use Guttenberg public domain books that are modernized with AI and\n            then create training around the above elements. \nComment: Now if\n            only we could get IBM to sell a version of their AI card to the public\n            \nComment: ibm doing\n            better work than meta theyre surprisingly becoming a big player in open source (for\n            small models) \nComment: I wonder\n            what is prompt processing speed for semi-recurrent stuff compared to transformers.\n            Transformers have fantastic prompt processing speed like 1000t/s easy even on crap like\n            3060, but slow down during token generation as context grows. This seems to be the other\n            way around, slow PP but fast TG. I might be completely\n            wrong. \nComment: Large\n            datasets: all of Harry Potter series asking questions like, what would have to change in\n            the series for Harry to end up with Hermione or for Voldemort to win. It‚Äôs a series\n            everyone knows fairly well and requires details in the story and the story\n            whole. \nComment: I\n            remember seeing this model a few days ago. There's no gguf so i cant try it out.\n            I guess there's not a lot of interest in this moe or it's not currently\n            possibly to make ggufs for it at the moment.  Webui stopped working\n            for me last year after i updated it and I've never been able to get it working\n            right since then, so been using lm studio appimages. That program runs everything good\n            for me but only runs ggufs. \nComment: Yall need\n            to learn Transformers and stop hating on llama.cpp \n","Title: Qwen3 no reasoning vs Qwen2.5\nID: t3_1kegrce\nSummary: It seems evident that Qwen3 with reasoning beats Qwen2.5. But I wonder if the Qwen3 dense models with reasoning turned off also outperforms Qwen2.5. Essentially what I am wondering is if the improvements mostly come from the reasoning.    submitted by    /u/No-Bicycle-132   [link]   [comments]\nImageDescription: \nComment: It seems evident that Qwen3 with reasoning beats Qwen2.5. But I wonder if the Qwen3 dense models with reasoning turned off also outperforms Qwen2.5. Essentially what I am wondering is if the improvements mostly come from the reasoning.    submitted by    /u/No-Bicycle-132   [link]   [comments]\nComment: On aider polyglot there was a huge boost from 16.4% of qwen 2.5 coder whole to qwen 3 32B (no thinking) achieving 45.8% being ahead of gpt 4.5. \nComment: Don't stop at wondering. Why don't you test it and share your result with us? \nComment: https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/ \nComment: Depends on the task. For code autocomplete Qwen/Qwen3-14B-AWQ nothink is awful. I like Qwen2.5-coder:14b. Additionally: some quants might be broken. \nComment: The 2.5 coders are better at complex one shots. 3.0 seems to generalize better and retains logic over a multiturn edit. My work involves updating lots of legacy Fortran and cobol that is written with very specific formatting and comment practices. 3.0 is the first open model that can run reasonably at 48gb vram that can reliably port my code. Also I think, for coding one shot diffs, reasoning turned off produces better results. \nComment: I have compared them far too little to be able to draw a serious conclusion, but from the very few comparisons I have made in coding, Qwen3 (no thinking) outputs better code, more accordingly to the prompt, than Qwen2.5. \nComment: The short answer is it entirely depends on your use case. In my limited testing, their overall performance was pretty close, with Qwen 3 probably being better overall.  I know the benchmarks say otherwise, but when translating Japanese to English, I found Qwen 2.5 to sound more natural.  However, when summarizing short stories, Qwen 2.5 dissected the story like a technical manual, whereas Qwen 3 wrote (or tried to write) in the tone of the original story.  Qwen 3 seems to lose less when quantized than Qwen 2.5. I was shocked at how well Qwen 3 32b functioned even down to IQ2 (except for factual retrieval which as usual takes a big hit). Coding, logical puzzles, and problem-solving seemed like a toss up. They both did it with more or less the same success; although, enabling reason will likely give Qwen 3 the edge. \nComment: Yes from what I have seen for apples to apples. But the 2.5 coding models will probably still hold tier own vs regular 3 models with thinking off. \nComment: They do. Qwen3 8b outperforms 7b 2.5; at least because of that extra 1b. \n"],"results":[{"title":"What do I test out / run first?","id":"t3_1kexdgy","overview":"This Reddit post features a user who just received a high-end GPU (likely an NVIDIA RTX A6000 or similar) and is seeking recommendations for initial testing. The discussion revolves around running large language models (LLMs) like Llama 3.2, Qwen 3, and Mistral, with technical details about VRAM requirements (e.g., 48GB vs. 96GB variants), quantization formats (Q000016, AWQ), and benchmarking scenarios. Comments mention specific models (e.g., 'LLAMA 405B Q.000016', 'Qwen 30B A3B q8') and hardware considerations like CUDA versions, PyTorch compatibility, and power connectors. While the post itself is a user inquiry rather than a technical announcement, the comments provide a snapshot of current LLM experimentation trends and hardware capabilities.","comment_overview":"The community response is a mix of technical curiosity and humor. Users suggest running benchmarks for models like DeepSeek 70B, Crysis (as a stress test), and Qwen 3 variants, while others joke about the GPU's cost ('How many kidneys did you sell?'). Discussions highlight practical concerns like ECC memory disablement, CPU pairing, and driver hacking. There's also enthusiasm for streaming model tests and sharing benchmarks, with some users expressing frustration over GPU pricing inflation. The post reflects the vibrant but sometimes chaotic nature of AI hardware experimentation communities.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/","relevance":"","is_relevant":false},{"title":"Visa is looking for vibe coders - thoughts?","id":"t3_1keolh9","overview":"This job posting from Visa for a Staff Gen AI Engineer highlights a unique blend of cutting-edge AI/ML expertise and proprietary tooling. The role explicitly requires experience with Large Language Models (LLMs), generative AI, vector databases (Pinecone, ChromaDB), and containerization technologies like Docker/Kubernetes. Notably, Visa is seeking candidates familiar with 'Vibe coding tools' such as Bolt, Lovable, and VO‚Äîinternal or proprietary systems likely designed to streamline development workflows. The posting also emphasizes a hybrid skillset: backend development (Python, PostgreSQL, FastAPI), frontend design systems, and data science. This reflects Visa's push to integrate AI into financial infrastructure while maintaining control over custom tooling. The mention of 'vibe coding'‚Äîa term implying low-code/no-code or AI-assisted development‚Äîraises questions about how Visa balances innovation with traditional engineering rigor. The role underscores the growing trend of enterprises building specialized AI workflows, but its ambiguity around 'vibe tools' leaves room for speculation about their actual scope and impact.","comment_overview":"The community debate centers on the enigmatic nature of 'vibe coding' and Visa's job requirements. Some commenters question whether the role prioritizes familiarity with proprietary tools over traditional engineering skills, while others mock the term as a buzzword-laden HR tactic. Concerns include potential underpayment compared to data science roles, skepticism about the practicality of 'vibe tools' (e.g., AI-generated code), and worries about unsustainable costs if such tools are expensive. A few highlight the irony of 'vibe coding' being paired with demanding technical requirements like vector databases and Kubernetes, suggesting a mismatch between the job's title and actual responsibilities. Others note Visa's broader AI ambitions, referencing their CTO's focus on agent-based systems and generative AI applications. The comments reveal both curiosity about Visa's internal tech stack and frustration with vague job descriptions.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/","relevance":"","is_relevant":true},{"title":"UI-Tars-1.5 reasoning never fails to entertain me.","id":"t3_1keo3te","overview":"The post showcases a web-based interface for UI-TARS-1.5, a 7B-parameter AI agent designed to simulate human-like interactions with software. The UI allows users to configure the tool with specific LLM providers (e.g., 'ByteDance-Seed/UI-TARS-7B') and demonstrates the agent's ability to perform tasks like clicking 'Accept All' buttons or deciding not to analyze code directly. Technical details include customizable agent loops, LLM model selection, and integration with external APIs via base URLs. The interface's transparency in explaining its reasoning process‚Äîsuch as opting for a search over code analysis‚Äîhighlights its decision-making framework. However, no new model architecture or performance benchmarks are disclosed, and the tool appears to leverage existing LLMs rather than introducing novel techniques.","comment_overview":"The community debate centers on technical limitations: one user notes the absence of GGUF format support or vision capabilities for the 'ByteDance-Seed/UI-TARS-7B' model in llama.cpp. Others humorously critique the AI's laziness in avoiding cookie policy reviews, while some speculate about its training data ('Gen-Z data') or compare it to 'TikTok AI.' A link to a GitHub repo (cua) suggests potential integration paths, but no concrete implementation details are shared. The discussion underscores skepticism about the tool's novelty and practicality, with many treating it as a niche curiosity rather than a groundbreaking development.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/","relevance":"","is_relevant":false},{"title":"Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)","id":"t3_1kepuli","overview":"RunLocal has launched an open-source initiative to standardize performance benchmarking for large language models (LLMs) across 50+ devices, including iOS, Android, Mac, and Windows. The project focuses on Qwen3, testing it with 512 prefill tokens and 128 generation tokens, while also supporting formats like Core ML, ONNX, TFLite, and GGUF (with ongoing work for full GGUF support). The benchmarks measure critical metrics like tokens per second (toks/s) and RAM utilization, which are vital for determining model viability in real-world applications. The platform allows users to run custom benchmarks on shared hardware, though the free version has limitations to prevent resource overuse. Notably, the data highlights device-specific quirks‚Äîsuch as Metal backend OOM (out-of-memory) errors causing misleading performance results‚Äîand underscores the complexity of cross-platform model optimization. The initiative aims to become a centralized hub for standardized metrics, addressing the fragmented nature of current benchmarking efforts.","comment_overview":"The community is intrigued by the project's potential but raises technical concerns. Users note discrepancies like Q8 quantization outperforming Q4 on iPhones, sparking debates about model optimization tradeoffs. A critical issue highlighted is Metal backend OOM errors leading to incorrect results, which could mislead users about actual performance. UI/UX feedback emphasizes the need for cleaner data presentation, such as hiding redundant columns and simplifying device naming conventions (e.g., removing 'Apple' or 'Bionic' labels). Discussions also reveal practical challenges, like Android app crashes and confusion over switching between CPU/GPU backends on iOS. Despite these issues, the project is seen as a valuable step toward democratizing edge AI performance analysis, with users eager to see expanded model support and improved data interpretability.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/","relevance":"","is_relevant":true},{"title":"Qwen 30B A3B Performance Degradation with KV Quantization","id":"t3_1kewkno","overview":"A user reported inconsistent performance when running Qwen 30B A3B with KV quantization, specifically encountering repetition loops and incorrect outputs during long-generation tasks. The issue was traced to KV cache quantization levels, as disabling it allowed successful completion of the OpenAI cipher test. The original gist by sunpazed demonstrated Qwen 30B solving the cipher in 5 minutes using Q4_K_M quantization, but local replication failed until KV quantization was removed. The user tested multiple quantization schemes (Unsloth Q4_K_XL, Bartowski Q5_K_S) before discovering that avoiding KV quantization resolved the problem. Technical details include comparisons between Q4_K_M, Q5_K_S, and Q8_0 quantization levels, with community discussions highlighting risks of KV cache quantization on models using GQA (Grouped Query Attention). The post underscores the tradeoff between memory efficiency and model fidelity when applying quantization.","comment_overview":"The community emphasized caution with KV quantization, with users warning that Q4_0 cache quantization degrades quality, especially for models relying on GQA. Several commenters recommended using fp16 KV caches or limiting quantization to Q8_0 for stability. A debate emerged about the utility of KV quantization, with some calling it 'super dumb' and others noting its memory-saving benefits. Technical advice included specific llama.cpp parameters (-ctk q8_0 -ctv q5_1) and testing without flash attention. The discussion revealed a lack of consensus on optimal quantization strategies, with users sharing conflicting experiences about model behavior under different settings.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/","relevance":"","is_relevant":true},{"title":"QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison","id":"t3_1kenk4f","overview":"This post presents a comparative analysis of three large language models‚ÄîQwQ 32b, Qwen 3 32b, and GLM-4-32B‚Äîfocused exclusively on their ability to generate HTML code. The test involved generating a single HTML script for a 'Steve's PC Repair' website, with evaluations based on layout quality (0-10 scale), code complexity (line count), and visual output. GLM-4-32B achieved the highest score (9/10) with a visually polished layout requiring 1,500+ lines of code, while QwQ 32b scored poorly (3/10) with a minimalistic, functional but unremarkable design. Qwen 3 32b fell in the middle (6/10) with a more aesthetically pleasing but still simple layout. The author emphasizes that GLM-4-32B excels specifically in HTML and JavaScript generation, performing on par with Qwen 2.5 32b for other tasks like Python coding, reasoning, and math. The comparison highlights the importance of domain-specific training, as GLM-4-32B's specialized optimization for frontend development starkly outperforms its peers in this niche.","comment_overview":"The community largely validated the findings, with users noting GLM-4-32B's superiority in HTML generation but cautioning about its limitations outside frontend development. One user praised it as a 'gem' for web tasks, while another highlighted that Qwen 3 32b outperformed GLM-4-32B in specific JavaScript-driven tasks like Babylon.js game development. Discussions also touched on the impact of quantization (e.g., q4km version) and inference settings, with warnings about Ollama's default context window size potentially hindering QwQ's performance. Critics questioned the test's scope, suggesting more complex tasks like interactive games or multi-page applications would better stress-test these models. A few users shared alternative tools like Tesslate/UIGEN-T2 for UI generation, indicating ongoing experimentation with specialized models.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/","relevance":"","is_relevant":true},{"title":"LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!","id":"t3_1keoint","overview":"The llama.cpp ecosystem has seen significant performance boosts, particularly for large models like Qwen3 MoE (235B and 30B). Mainline llama.cpp now features Flash Attention optimizations for Grouped Query Attention (GQA) and Mixture of Experts (MoE) architectures, while the ik_llama.cpp fork introduces SOTA iqN_k quantizations that balance model quality and speed. These improvements are critical for users leveraging CUDA-enabled GPUs, with ik_llama.cpp excelling in hybrid CPU+GPU workflows and specialized quantization needs. The updates highlight competitive innovation within the open-source community, benefiting users through enhanced efficiency for models like Qwen3, GLM-4, and Gemma3. Technical details include faster token generation, better batch processing, and support for new GGUF formats, though compatibility notes apply for DeepSeek-V3/R1/Lite models.","comment_overview":"The community is enthusiastic about the performance gains, with users reporting tangible speed improvements for Qwen3 MoE models. However, some users note inconsistencies‚Äîsuch as a 15 TPS drop after updating, sparking debates about optimization trade-offs. Questions about benchmarking commands and confusion over plot comparisons (e.g., ik_llama.cpp vs. mainline) reveal a need for clearer documentation. Discussions also highlight hardware-specific limitations, like the 3090's performance and multi-GPU crashes in ik_llama.cpp. While many praise the fork's SOTA quantizations, others caution that benefits depend on use cases (e.g., full GPU offload vs. hybrid setups). The conversation underscores both the excitement around these updates and the challenges of balancing optimization across diverse hardware configurations.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/","relevance":"","is_relevant":true},{"title":"Apparently shipping AI platforms is a thing now as per this post from the Qwen X account","id":"t3_1kebauw","overview":"This post features a cartoonish image with two characters‚Äîa sloth and a bear‚Äîsymbolizing collaboration between 'unsloth' and 'Qwenen3,' likely AI-related entities. The image uses vibrant colors and friendly aesthetics to convey partnership, with the sloth gifting a flower to the bear as a metaphor for 'shipping' (i.e., combining or collaborating). However, the post lacks technical details about AI models, infrastructure, or performance metrics. The image‚Äôs primary purpose appears to be branding or promotional messaging rather than announcing a specific AI advancement. While the concept of 'shipping AI platforms' is mentioned, it remains abstract without concrete explanations of what technologies are being combined or how.","comment_overview":"The comments lean heavily into humor, with users joking about 'AI romcoms,' 'shipping' as a metaphor for relationships, and playful banter about AI models writing fanfiction. Some comments critique the post as trivial or socially awkward, while others mention OSS contributions and speculate about VTuber integrations. The discussion highlights the community‚Äôs tendency to meme around AI topics but lacks technical depth or substantive analysis of the image‚Äôs implied collaboration.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kebauw/apparently_shipping_ai_platforms_is_a_thing_now/","relevance":"","is_relevant":false},{"title":"IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models","id":"t3_1kedu0d","overview":"IBM's Granite 4.0 Tiny Preview introduces a groundbreaking hybrid Mamba-2/Transformer architecture, combining the efficiency of state-space models (Mamba) with the precision of transformers. This 7B-parameter model uses only 1B active parameters at inference time via a fine-grained MoE (Mixture of Experts) design, achieving 128K token context lengths without positional encoding (NoPE). Despite being a 'tiny' preview, it outperforms Granite 3.3 2B Instruct while running on consumer-grade hardware. The NoPE architecture, which avoids traditional positional encodings, demonstrates robust long-context performance, a claim supported by IBM's internal testing. The model's hybrid design stems from IBM Research's collaboration with Mamba creators on Bamba, with Bamba v2 released alongside this announcement. Training is ongoing, progressing from 2.5T to 15T tokens, with enterprise support expected this summer. Its ability to handle multiple concurrent sessions on modest GPUs suggests significant optimization for real-world deployment.","comment_overview":"The community is enthusiastic about the technical innovation but cautious about practical adoption. Many users highlighted the potential of hybrid architectures to bridge attention and recurrent models, with hopes for larger variants and open-source support. Concerns emerged about limited compatibility with tools like llama.cpp, which currently lacks inference support for Mamba-2 hybrids. Discussions also focused on the significance of NoPE, with requests for academic papers explaining its effectiveness. Some users criticized IBM's reliance on proprietary hardware, while others praised their growing open-source footprint. The model's performance in creative tasks like fictional text interaction and reasoning benchmarks sparked speculative debates about its future capabilities.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/","relevance":"","is_relevant":true},{"title":"Qwen3 no reasoning vs Qwen2.5","id":"t3_1kegrce","overview":"This discussion revolves around comparing the performance of Qwen3 and Qwen2.5 models, particularly focusing on whether Qwen3's improvements stem from reasoning capabilities or other factors. The user asks if Qwen3 dense models without reasoning outperform Qwen2.5, suggesting that the advancements might be primarily due to reasoning enhancements. Comments highlight mixed results across tasks: some note Qwen3's superiority in code generation and reasoning-heavy tasks, while others find Qwen2.5 better for specific use cases like Japanese-to-English translation. Technical details include benchmarks (e.g., 45.8% on a coding task for Qwen3-32B) and observations about quantization effects, but no official specifications are provided. The conversation underscores the importance of task-specific performance and model configuration.","comment_overview":"The community debate reflects nuanced perspectives. Some users claim Qwen3 excels in logical reasoning and code porting, while others prefer Qwen2.5 for natural language tasks. Critics point out that Qwen3's reasoning mode may not always be beneficial, and quantized versions of Qwen2.5 might still hold advantages. A recurring theme is the dependency on use cases, with no clear consensus. One user emphasizes testing specific configurations, while another highlights Qwen3's resilience to quantization. The discussion reveals practical challenges in evaluating model performance without standardized benchmarks.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/","relevance":"","is_relevant":false}],"persona":"LocalLLaMa"}