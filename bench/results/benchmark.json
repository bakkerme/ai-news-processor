{"raw_input":["Title: What do I test out / run first?\nID: t3_1kexdgy\nSummary:   \n               Just got her in the mail. Haven't had a chance\n            to put her in yet.    submitted by\n              \n            /u/Recurrents   [link]\n              [comments]\n            \nImageDescription: \nComment:       Just got her in the mail. Haven't had a chance to put her in yet.    submitted by    /u/Recurrents   [link]   [comments] \nComment: First run home. Preferably safely. \nComment: llama 3.2 1b \nComment: Bro is loaded. How many kidneys did you sell for that?! \nComment: LLAMA 405B Q.000016 \nComment: all the new qwen 3 models \nComment: sexy ass card \nComment: Old School Runescape \nComment: Are they selling those already? \nComment: Download cuda and make sure your pytorch is the cuda version \nComment: Can it run Crysis? \nComment: You bought it just to benchmark it, didn't you? \nComment: Hello Kitty Island Adventures, butters would be proud of you. \nComment: Would you mind sharing or DMing retailer info? I don‚Äôt have a preferred vendor and am curious on your experience. \nComment: Llama 3.3 70b at 8-bit. Would be interesting to see how many tokens per second gives. \nComment: Try Super Mario Bros ü•∏ \nComment: you don't need it. gimme that. \nComment: Rtx pro 6000 is 96Gb it is beast. Without pro is 48gb. I really want to know how many FOPS it is. Or the t/s for a deepseek 70B or largest model it can fit. \nComment: Get some silly concurrency going on qwen 3 32b awq and run the aider benchmark. \nComment: Flux to generate pics of your dream Audi. Find out your use case and try some models that fit. I was first impressed by GLM 4 in one shot coding, but it fails to use other tools. Mistral small is my daily driver currently. It's even fluent in most languages. \nComment: That‚Äôs some expensive computer hardware. Congratulations. \nComment: That‚Äôs our serial number now \nComment: Your power connectors. \nComment: i cant imagine spending that much money on a gpu with that power connector \nComment: Houston we have lift off  https://preview.redd.it/v3z4prno2wye1.png?width=780\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=6a6156b3fc0818b93b0459a14c86a0e0dd1d70d7 \nComment: https://preview.redd.it/5bnvabxayvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=9516acddbdda888267887c823c70c25db1ba8c6e New card installed! \nComment: Is it better then a h100 performance wise? i know the vram is slightly bigger. \nComment: Quake I \nComment: Everything. In all seriousness, I would reaaally like to see the benchmarks on that thing \nComment: Old school runescape \nComment: Cancer research. \nComment: OT, but run 3Dmark and confirm if it really is faster in games than the 5090 (for once in the history of workstation cards). \nComment: dude you are so lucky congrats!! run every qwen 3 model and make videos! i hear you stream, how about a live stream using llama.cpp and testing out models, or lm studio. this card is so awesome üòç \nComment: Something like Gemma 3 27B/Mistral small-3/Qwen 3 32B with maximum context size? \nComment: Qwen3 and don‚Äôt look back \nComment: Qwen 30B A3B q8 has something around 30 GB file size. Should run very fast and have plent of room for context. \nComment: Bios \nComment: Where did you buy it from? \nComment: About $12,000 to $16,000 for the 48gb vram editions here .. not sure we can get the 96gb \nComment: What CPU are you pairing with? Linux? \nComment: https://preview.redd.it/glj9rjmk9vye1.jpeg?width=1280\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=64d6eac13d0a6aaed4b500953bfd300dcea46322 \nComment: ü•∫ü•πüò≠ \nComment: Wan 2.1 fp 32 model \nComment: Crysis. \nComment: Haha I thought it had a plaid pattern printed on it üòÖ \nComment: Dude so cheesed, could've even wait to get home \nComment: Hey, I was looking to buy one as well, how much did you pay and how long did it take to arrive. They are releasing so many cards these days I get confused. \nComment: How much \nComment: what version is it? Max‚ÄìQ? Workstation edition? Etc‚Ä¶ \nComment: GTA V \nComment: Grounding strap. \nComment: Crysis \nComment: Plex Media Server. But make sure to hack your drivers. \nComment: Mate, share some benchmarks!  I‚Äôm about ready to pull the trigger on one too, but the price gouging here is insane. They‚Äôre still selling Ampere A6000s for 6‚Äì7K AUD, and the Ada version is going for as much as 12K.  Instead of dropping prices on the older cards, they‚Äôre just marking up the new Blackwell ones way above MSRP. The server variant of this exact card is already sitting at 17K AUD (~11K USD)‚Äîabsolute piss take tbh. \nComment: Image and clip generation \nComment: I think I'll stream getting some LLMs and comfyui up tomorrow and the next few days. give a follow if you want to be notified https://twitch.tv/faustcircuits \nComment: Get that unsloth 235B Qwen3 model at Q2K_XL. It should fit. Q2 is the most efficient size when it comes to benchmark score to size ratio according to unsloths documentation. It should be fast AF too since only 22B active parameters.  \nComment: Nice! Still waiting for mine. Can you let me know if you are able to disable ECC or not? \nComment: what Audi is that? S4? \nComment: Nice. Run stuff and share stats! Would be cool to see. \nComment: https://preview.redd.it/ems9w2z6yvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=76b13f186be7cb783727c000bda533c92c1e8c56 here is the old card lol \n","Title: Visa is looking for vibe coders - thoughts?\nID: t3_1keolh9\nSummary:   \n                \n            submitted by    /u/eastwindtoday\n              [link]\n              [comments]\n            \nImageDescription: \nComment:         submitted by    /u/eastwindtoday   [link]   [comments] \nComment: The way it's written with the other requirements makes me think that they want a programmer able to write vibe coding tools, not specifically hiring a vibe coder for programming. \nComment: \"Background in Data Science is a big plus\" - Be prepared to be payed less than a proper data scientist. \nComment: Dumb. Vibe coding is a bit ill-defined but what I see is a niche where people like product managers can use AI to build tiny tools that would be hard to get into a product team's priority list. In particular what I saw was using Zapier which is a user-friendly automation tool that lets you run snippets of Python or js inside workflows and now lets you generate them with AI. Great way to build little utilities without a developer but also very narrow, low stakes applications that aren't critical path. This job req wants familiarity with vector DBs and containers which means they want an actual experienced software engineer. Someone who can use a coding assistant but probably doesn't need one. That's not vibe coding. \nComment: Vibe coding 2025 is same as slapping an untested, ill designed, 700-lines-a-function Python script on anything and calling it a day in the 2015s Edit: You scrolled down on some requirements tho https://www.visa.ca/fr_ca/jobs/REF061638W \u0026gt; Strong proficiency with Python, FastAPI and PostgreSQL for backend development. \nComment: These days, stupid HRs are stuffing the job requirements sections with as many tech buzzwords as possible, regardless of whether these buzzwords actually reflect the technologies used in the company. I feel like \"vibe coding\" may be one of those buzzwords. \nComment: now exactly nobody will know why credit card applications failed \nComment: Maybe we should all \"vibe-pay\" our Visa card bills. \nComment: Click click, sold all visa stocks instantly \nComment: I for one am happy my CC transactions are handled by vibed up systems. \nComment: they are starting up an ml intership for students next summer in poland too \nComment: I don't even know what to think. My general reaction of most job posts is to assume the companily already has someone they're gonna hire, and the position is only being posted as a formality and to make it appear that the company has fair hiring practices (even though requirements weed out people that are perfectly capable of being trained). \nComment: I know some folks working with such teams. They have ambitions to build vibe coding tools (like cursor) but without the dependency on letting data go out of the org (from a financial reg pov). Visa‚Äôs CTO is one of the highest paid CTOs in the US, and they are building agents replicating (gamma.ai, OpenAI deep research, Agentic Commerce) to start with. \nComment: They are even Vibe-using Caps. That's so Vibe. It gives me Bad Vibes. \nComment: That job posting is clear as mud. Title is an obvious mismatch for the skillset. \nComment: well I'm glad none of my credit cards are visa...  \nComment: what could possibly go wrong? \nComment: If they are listing the tech stacks that should be enough. \nComment: At least they're not requiring minimum 15 years of experience \nComment: Lmao \nComment: AI generated job posting ? :p \nComment: Vibe coding doesn't work. It takes me 10-20 prompts to generate anything useful with gemini 2.5.  Sometimes 30+ prompts even with explicit instructions. \nComment: And I thought it was a big deal 15 years ago when I talked NASA into letting me put ‚Äúauto-generated‚Äù guidance and control code into a satellite. That particular NASA center had never done such a thing, and I had to pop their cherry on it (as a contractor). Partly because a colleague of mine was doing the same thing on a bigger project, and he really wanted to be able to say his project wasn‚Äôt the first. The ‚Äúauto-generation‚Äù step is just compiling MatLab down into C code, and from there is just standard C compilation on the target environment. But the MatLab becomes the source code, in this case. And you have to version-control the auto-generation configuration as well.  Honestly, with all the other shit we had to deal with on that program, just telling an AI to ‚Äúwrite some code that might look like spacecraft guidance and control software, no particular requirements because we‚Äôre don‚Äôt have the money to actually test the hardware before launch, and most of it is going to fail anyway‚Äù would have been such a stress-saver. That‚Äôs a good vibe. \nComment: As someone already posted, these are not the full requirements, but another thing also:  Vibe-coding is expensive. Requests can quickly become $1-2 a piece. Over the course of a month, you could easily rack up $500+ if you're using it liberally. Now consider that you're already paying a regular salary, put this on top, and it becomes kind of unsustainable.  The money has to come from somewhere, and it's most probably the base pay. \nComment: I don't know much about developers as vibe coders. But, a lot of project managers \u0026amp; UI UX folks and product managers are starting to learn these tools (lovable, cursor, etc.,) and will roll out the basic screens for evaluation.  Also, this will start getting back to actual developers on justifying their timelines, complexity justifications, etc., This is mostly as nobody gives a fuck about code maintainability, reusability, tech debt, security, etc,.  Other side to this coin is 90% of code written by \u0026lt;5 years experienced devs are not following these standards anyways ü§∑‚Äç‚ôÇÔ∏è 90% of products from start-ups and other IT companies never see the light anyways ü§¶‚Äç‚ôÇÔ∏èü§£ \n","Title: UI-Tars-1.5 reasoning never fails to entertain me.\nID: t3_1keo3te\nSummary:   \n             \n             7B\n            parameter computer use agent.   \n            submitted by   \n            /u/Impressive_Half_2819   [link]\n              [comments]\n            \nImageDescription: \nComment:       7B parameter computer use agent.    submitted by    /u/Impressive_Half_2819   [link]   [comments] \nComment: What's more important here is the model used - ByteDance-Seed/UI-TARS-1.5-7B the model which it is meant to be used with, so how did you make it work? Because last time I checked I haven't seen that model being converted to GGUF format, nor having vision support added into llama.cpp for it. \nComment: I guess: https://github.com/trycua/cua \nComment: When you train a model to use computers for humans and do the tiresome ToS reading, but it can't be bothered to do it either \nComment: Most probably trained on Gen-Z data. \nComment: https://preview.redd.it/4ignwtxrhuye1.jpeg?width=1079\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=e5d30270b51854c061aebdd502448897c63fed18 \nComment: On one hand, I guess I'd like the language model to read language on my behalf - on the other hand I wouldn't want the model to decide the cookies policy warrants user review or some other distraction so maybe skipping it is for the best after all. It does seem reading the pop-up falls within the scope of accessing the site to search for a repository \nComment: Try out yourself using cu/a! \nComment: I mean, fair \nComment: tiktok ai getting lazy \nComment: TARS, would you set your attention span setting to 8 for me? \nComment: It‚Äôs the defaut personality? \n","Title: Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS,\n            Android, Mac, Windows)\nID: t3_1kepuli\nSummary:   \n             \n             Hey\n            LocalLlama! We've started publishing open-source model\n            performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS,\n            Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+\n            soon. We‚Äôre doing this because perf metrics determine the viability\n            of shipping models in apps to users (no end-user wants crashing/slow AI features that\n            hog up their specific device). Although benchmarks get posted in\n            threads here and there, we feel like a more consolidated and standardized hub should\n            probably exist. We figured we'd kickstart this since we\n            already maintain this benchmarking infra/tooling at RunLocal for our enterprise\n            customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and\n            TFLite to date, so a few things are still WIP for GGUF support. \n            Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill\n            Tokens=512, Num Generation Tokens=128). GGUFs\n            are from Unsloth üêê Qwen3\n            GGUF benchmarks on laptops Qwen3\n            GGUF benchmarks on phones You can see more of the benchmark\n            data for Qwen3 here.\n            We realize there are so many variables (devices, backends, etc.) that interpreting the\n            data is currently harder than it should be. We'll work on that!\n            You can also see benchmarks for a few other models here. If\n            you want to see benchmarks for any others, feel free to request them and we‚Äôll try to\n            publish ASAP! Lastly, you can run your own benchmarks on our devices\n            for free (limited to some degree to avoid our devices melting!). This\n            free/public version is a bit of a frankenstein fork of our enterprise product, so any\n            benchmarks you run would be private to your account. But if there's interest, we\n            can add a way for you to also publish them so that the public benchmarks aren‚Äôt\n            bottlenecked by us.  It‚Äôs still very early days for us with this, so\n            please let us know what would make it better/cooler for the community: https://edgemeter.runlocal.ai/public/pipelines\n            To more on-device AI in production! üí™ https://i.redd.it/aev5rgjazsye1.gif\n               submitted by    /u/intofuture \n             [link]\n              [comments]\n            \nImageDescription: \nComment:       Hey LocalLlama! We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon. We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device). Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist. We figured we'd kickstart this since we already maintain this benchmarking infra/tooling at RunLocal for our enterprise customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and TFLite to date, so a few things are still WIP for GGUF support.  Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill Tokens=512, Num Generation Tokens=128). GGUFs are from Unsloth üêê Qwen3 GGUF benchmarks on laptops Qwen3 GGUF benchmarks on phones You can see more of the benchmark data for Qwen3 here. We realize there are so many variables (devices, backends, etc.) that interpreting the data is currently harder than it should be. We'll work on that! You can also see benchmarks for a few other models here. If you want to see benchmarks for any others, feel free to request them and we‚Äôll try to publish ASAP! Lastly, you can run your own benchmarks on our devices for free (limited to some degree to avoid our devices melting!). This free/public version is a bit of a frankenstein fork of our enterprise product, so any benchmarks you run would be private to your account. But if there's interest, we can add a way for you to also publish them so that the public benchmarks aren‚Äôt bottlenecked by us.  It‚Äôs still very early days for us with this, so please let us know what would make it better/cooler for the community: https://edgemeter.runlocal.ai/public/pipelines To more on-device AI in production! üí™ https://i.redd.it/aev5rgjazsye1.gif    submitted by    /u/intofuture   [link]   [comments] \nComment: Iphone 16's Metal performance is pretty impressive for 1.6b-q8. But I do wonder why q8's performance is faster than q4 in that particular setup. \nComment: It‚Äôs interesting to see that performance in m4 is pretty similar in both cpu and gpu \nComment: There's one edge factor you missed - on Metal backend when you get OOM you get completely wrong results. For example on Qwen3 8B Q4 your results are like this: - MacBook Pro M1, 8GB = 99232.83tok/s prefill, 2133.70tok/s generation - MacBook Pro M3, 8GB = 90508.66tok/s prefill, 2507.50tok/s generation If you wouldn't get OOM the correct results for that model should be around ~100-150tok/s prefill and ~10tok/s generation. Additionally, all results for RAM usage on Apple silicon \u0026amp; Metal are not correct. In terms of your UX/UI there's tons of stuff that should be improved. but to not make this into very long post I'll write about biggest problems that can be fixed rather easily. First, add option to hide columns, there's too much redundant information that should be possible to hide with just couple of clicks. Second, decide on some naming scheme for components and stick with it. https://preview.redd.it/8idjxghteuye1.png?width=91\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=12d1d9317495b745768b9dd012288be1eb804964 I would suggest to get rid of 'Apple'/'Bionic' names altogether - it just adds to complexity and cognitive load to a table that is already very dense. There is no non-Apple M1 in Macbooks or non-Bionic A12 in iPad, so you don't need to clarify that much in a first place and additionally this page is aimed at technical people. Exact same problem with Samsung/Google vs Snapdragon. Third, if both CPU and Metal failed don't create two entries. Table is 2x longer than it should be with results that are non-comparable to anything. Just combine it into one entry. \nComment: How to run on metal on iphone 16 pro? I have pocketpal app and how to switch from cpu to metal? \nComment: if i'm reading this correctly the load time on cpu is better than gpu/metal for macbook pro but the gpu/metal is less memory intensive? also metal perf on iphone 16 is pretty impressive. \nComment: How do I run this on Android? Rn it just crashes \nComment: Why is Q8 faster than Q4??? \nComment: For laptops, is vulkan using the igpu ? \nComment: according to this data on iphone 16 you have 24 t/s on Q8 and 22 t/s on Q4 why so tiny models? \nComment: The iPhone 16e is listed to have the A18 Pro SoC but it actually has the A18.  https://preview.redd.it/h1dx2hgphvye1.png?width=623\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c2f8794bf27c6a9042b81040ebefa09873eae989 \n","Title: Qwen 30B A3B performance degradation with KV quantization\nID: t3_1kewkno\nSummary: \n            I came\n            across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4\n            that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M\n            quantization. I tried to replicate locally but could I was not able,\n            model sometimes entered in a repetition loop even with dry sampling or came to wrong\n            conclusion after generating lots of thinking tokens. I was using\n            Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested\n            Bartowski Q5_K_S but it had no improvement. The model didn't entered in any\n            repetition loop but generated lots of thinking tokens without finding any\n            solution. Then I saw that sunpazed didn't used KV\n            quantization and tried the same: boom! First time right. It worked\n            with Q5_K_S and also with Q4_K_XL For who wants more details I leave\n            here a gist https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef\n            Do you have any report of performance degradation with long generations on\n            Qwen3 30B A3B and KV quantization?   \n            submitted by   \n            /u/fakezeta   [link]\n              [comments]\nImageDescription: \nComment: I came across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4 that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization. I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens. I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens without finding any solution. Then I saw that sunpazed didn't used KV quantization and tried the same: boom! First time right. It worked with Q5_K_S and also with Q4_K_XL For who wants more details I leave here a gist https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef Do you have any report of performance degradation with long generations on Qwen3 30B A3B and KV quantization?    submitted by    /u/fakezeta   [link]   [comments]\nComment: What KV quant level were you using? IMO on llama.cpp you shouldn‚Äôt push it past Q8_0. Q4_0 cache quant tanks quality in any model and especially models that heavily leverage GQA.  \nComment: I have one rule: I always test ALL new models without flash attention and with full 16bit KV cache. \nComment: Which KV quantization are you using? Don't have time to run this test right now, but I usually use -ctk q8_0 -ctv q5_1 (requires -DGGML_CUDA_FA_ALL_QUANTS=on) \nComment: Ive only tried KV quantization once and saw that any amount of it makes models super dumb. Not sure why anybody uses it tbh \nComment: Which quantization did you use initially? \nComment: Interested here since I'm running a q6 \nComment: Use these parameters: Thinking Mode Settings: Temperature = 0.6 Min_P = 0.0 Top_P = 0.95 TopK = 20 Non-Thinking Mode Settings: Temperature = 0.7  Min_P = 0.0 Top_P = 0.8 TopK = 20 \nComment: Could you please tell us how to disable KV cache quantisation? I'd also like to check the difference. What is the difference in the amount of memory used with KV running at fp16 in comparison with regular q4? \nComment: I‚Äôm confused. Isn‚Äôt K_M KV quantization? And yet you said Qwen 30b solved the rest with Q4 K_M? \nComment: Of course  Cache should always be fp16 even Q8 has degradation. Only flash attention is ok...ish ( as is fp16 ) \n","Title: QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.\nID: t3_1kenk4f\nSummary:   \n               All models are from Bartowski - q4km version\n            Test only HTML frontend. My assessment lauout quality from 0\n            to 10 Prompt \"Generate a beautiful\n            website for Steve's pc repair using a single html script.\"\n            QwQ 32b - 3/10 - poor layout but ..works , very\n            basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b\n            Qwen 3 32b - 6/10 - much better looks but still not too\n            complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b272399ba8140785feb429a196ecc5173\n            GLM-4-32b 9/10 - looks insanely good , quality layout like\n            sonnet 3.7 easily - 1500+ code lines https://preview.redd.it/3zj2lr2ahsye1.png?width=2469\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964\n            GLM-4-32b is insanely good for html code frontend. I say\n            that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most.\n            Other coding language like python , c , c++ or any other quality\n            of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on\n            the seme level but for html and JavaScript ... is GREAT.\n               submitted by   \n            /u/Healthy-Nebula-3603   [link]\n              [comments]\n            \nImageDescription: \nComment:   \n               All models are from Bartowski - q4km version\n            Test only HTML frontend. My assessment lauout quality from 0\n            to 10 Prompt \"Generate a beautiful\n            website for Steve's pc repair using a single html script.\"\n            QwQ 32b - 3/10 - poor layout but ..works , very\n            basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b\n            Qwen 3 32b - 6/10 - much better looks but still not too\n            complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b272399ba8140785feb429a196ecc5173\n            GLM-4-32b 9/10 - looks insanely good , quality layout like\n            sonnet 3.7 easily - 1500+ code lines https://preview.redd.it/3zj2lr2ahsye1.png?width=2469\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964\n            GLM-4-32b is insanely good for html code frontend. I say\n            that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most.\n            Other coding language like python , c , c++ or any other quality\n            of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on\n            the seme level but for html and JavaScript ... is GREAT.\n               submitted by   \n            /u/Healthy-Nebula-3603   [link]\n              [comments]\n            \nComment: Yeah, I\n            have also tried to generate webpages with a couple of models, like GLM-4, Qwen3, Phi-4\n            Reasoning, etc. GLM-4 is so far the clear winner at these tasks. It's a gem in\n            my model collection. \nComment: I've created a list of one-shot generated HTML\n            pages using different models, big and small. https://blog.kekepower.com/ai/\n            \nComment: Ironically, so far userscript (javascript): Qwen 3 32B\n            \u0026gt; GLM-4-0414. Don't get me wrong. I love GLM-4-0414, but it feels like it\n            lacked the required understanding for my particular requests that Qwen 3 32B understood\n            well. \nComment: Whats the\n            temp? Did you rerun multiple times? \nComment: Why not\n            try a slightly more complex task? E.g. a mini-game? \n            Create a single-HTML-page game using Babylon.js where the player controls a\n            ship and moves about the open sea exploring islands to find treasure. A single, small\n            map with 3 islands of which only one has the treasure is enough\n             In the first reply, the camera works, but WASD\n            didn't. I copied-pasted the errors from the console a couple times and WASD\n            works now. It looks terrible, but I guess that's expected without external\n            assets. https://preview.redd.it/lf5foomq1tye1.png?width=1288\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b02ed407b3fbef31bf6d83aae5491cacaa93e150\n            Edit: This is my AWQ quant of Qwen-32B. \nComment: GLM falls\n            flat on its face when I try to continue developing after the first prompt.\n            It feels like a model trained (very well) for one-shots\n            \nComment: Try\n            UIGEN-T2 for html generation! There's also a react model. \n            https://huggingface.co/Tesslate/UIGEN-T2\n            \nComment: Would be\n            interesting to see how these results compare to the recent Tesslate/UIGEN-T2-7B.\n            It's a tuned version of Qwen 2.5 Coder 7B specifically for UI\n            generation. \nComment: What\n            quants what context window size? Ollama default size will kill QWQ reasoning if you\n            don‚Äôt know how to set it up properly. \nComment: Still\n            kind of freaked out that smaller Qwen 3 models are probably as good at website\n            development as I was as a teenager. And a damn sight quicker too.\n            \nComment: Thanks\n            for posting the eval. Would be curious to see the prompt used as well.\n            \nComment: Interesting. I ran the same input on QwQ with these\n            settings: Temp: 0.6 Top p: 0.95 Min p: 0.0 Top k: 40 And quite a bit different\n            output. Output: https://pastebin.com/Ntc8QQfH\n            \nComment: GPT 4.1\n            is clearly the winner here in my opinion as well as claude sonnet 3.7\n            \n","Title: LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!\nID: t3_1keoint\nSummary:   \n               You\n            can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B\n            and 30B) mainline\n            llama.cpp just got a boost for fully offloaded Qwen3 MoE (single\n            expert) tl;dr; I highly recommend\n            doing a git pull and re-building your\n            ik_llama.cpp or llama.cpp repo to take\n            advantage of recent major performance improvements just released. The\n            friendly competition between these amazing projects is producing delicious fruit for the\n            whole GGUF loving r/LocalLLaMA community! If\n            you have enough VRAM to fully offload and already have an existing\n            \"normal\" quant of Qwen3 MoE then you'll get a little more\n            speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take\n            advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork!\n            Details I spent yesterday compiling and running benhmarks\n            on the newest versions of both ik_llama.cpp and\n            mainline llama.cpp.\n            For those that don't know, ikawrakow was an early contributor to\n            mainline llama.cpp working on important features that have since trickled down into\n            ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my\n            understanding) the ik_llama.cpp fork was built and has a number\n            of interesting features including SotA iqN_k quantizations that\n            pack in a lot of quality for the size while retaining good speed performance. (These new\n            quants are not available in ollma, lmstudio, koboldcpp,\n            etc.) A few recent PRs made by ikawrakow to\n            ik_llama.cpp and by JohannesGaessler to mainline have\n            boosted performance across the board and especially on CUDA with\n            Flash Attention implementations for Grouped Query Attention (GQA) models and also\n            Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B\n            releases! References  ikawrakow/ik_llama.cpp/pull/370\n                submitted by    /u/VoidAlchemy \n             [link]\n              [comments]\n            \nImageDescription: \nComment:       You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B) mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert) tl;dr; I highly recommend doing a git pull and re-building your ik_llama.cpp or llama.cpp repo to take advantage of recent major performance improvements just released. The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving r/LocalLLaMA community! If you have enough VRAM to fully offload and already have an existing \"normal\" quant of Qwen3 MoE then you'll get a little more speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork! Details I spent yesterday compiling and running benhmarks on the newest versions of both ik_llama.cpp and mainline llama.cpp. For those that don't know, ikawrakow was an early contributor to mainline llama.cpp working on important features that have since trickled down into ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my understanding) the ik_llama.cpp fork was built and has a number of interesting features including SotA iqN_k quantizations that pack in a lot of quality for the size while retaining good speed performance. (These new quants are not available in ollma, lmstudio, koboldcpp, etc.) A few recent PRs made by ikawrakow to ik_llama.cpp and by JohannesGaessler to mainline have boosted performance across the board and especially on CUDA with Flash Attention implementations for Grouped Query Attention (GQA) models and also Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B releases! References  ikawrakow/ik_llama.cpp/pull/370     submitted by    /u/VoidAlchemy   [link]   [comments] \nComment: I'm currently running ik_llama.cpp with Qwen3-235B-A22 on a Xeon E5-2680v4, that's a 10 year old CPU with 128GB ddr4 memory, and a single RTX3090. I'm getting 7 tok/s generation, very usable if you don't use reasoning. BTW the server is multi-GPU but ik_llama.cpp just crash trying to use multiple-gpus, but I don't think it would improve speed a lot, as the CPU is always the bottleneck. \nComment: Could you explain how to read your pictures? I see orange plot below red plot, so ik_llama.cpp is slower than llama.cpp? \nComment: Can you post some of the commands you use for the benchmarks? I want to tinker to see what is best for my use case \nComment: Oh, just updated. My rig is busy for running deepseek \u0026amp; ik_llama (1 week jobs). I will update after that :) \nComment: Maybe GGUF will now give same speed as MLX on Mac devices \nComment: I have a 3090. Doesn't this say it's slower, not faster? \nComment: https://preview.redd.it/0zroyhg1qsye1.png?width=3404\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b3e55128b1aac3f6d2ddfbd22597b9cd6d7dd02c In my limited testing you probably want to go with ik_llama.cpp for fully offloaded non-MoE models like the recent GLM-4 which is crazy efficient on kv-cache VRAM usage due to its GQA design. \nComment: I just pulled and rebuilt and I'm now actually going about 15 tps slower. My previous build was from about a week ago, and I was getting an eval time of about 54 tps. Now I'm only getting 39 tokens per second, so pretty significant drop. I just downloaded the latest unsloth model I'm running on 2 3090s, using this command: ``` .\\bin\\Release\\llama-server.exe -m C:\\shared-drive\\llm_models\\unsloth-2-Qwen3-30B-A3B-128K-Q8_0.gguf --host 0.0.0.0 --ctx-size 50000 --n-predict 10000 --jinja --tensor-split 14,14 --top_k 20 --min_p 0.0 --top_p 0.8 --flash-attn --n-gpu-layers 9999 --threads 24 ``` Prompt: \"tell me a 2 paragraph story\" \nComment: How close is llamacpp to vLLM and exllama now? \nComment: Seems like it is related to CUDA only, so I guess only for people with Nvidia cards and not folks on Apple Silicon and others. \n"],"results":[{"title":"What do I test out / run first?","id":"t3_1kexdgy","overview":"This Reddit post features a user who recently received a high-end GPU (likely an RTX Pro 6000 or similar) and is seeking recommendations for initial tests. The discussion revolves around running large language models (LLMs), benchmarks, and performance comparisons. Comments suggest testing specific models like Llama 3.2 1B, Qwen 3.3 70B, and Mistral small-3, while others focus on hardware capabilities such as VRAM (48GB vs 96GB), CUDA setup, and PyTorch configurations. Technical debates include comparisons to H100 GPUs, gaming performance (Crysis, Quake I), and specialized workloads like cancer research or 3D rendering. The post highlights community interest in evaluating the GPU's potential for AI workloads, but no official technical details about the hardware or software are provided.","comment_overview":"The community is highly engaged, with users proposing specific LLMs to test (e.g., Qwen 30B A3B, Gemma 3 27B) and discussing hardware limitations like VRAM constraints. Some comments express skepticism about the GPU's value proposition, citing high prices and questions about performance gains over existing cards. Others focus on practical use cases, such as streaming benchmarks or running AI models with tools like llama.cpp. The tone ranges from enthusiastic (e.g., 'dude you are so lucky') to critical ('price gouging'), reflecting diverse perspectives on the hardware's utility and cost.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/","relevance":"","is_relevant":false,"thumbnail_url":"https://external-preview.redd.it/Gj8FzKNPTvVSOxJwgeuufUJzmZ6BR-6YWri04zLtxfs.jpeg?width=640\u0026crop=smart\u0026auto=webp\u0026s=99bf755df82e7e9bb2bc2cafc9271bdc27217ed4"},{"title":"Visa is looking for vibe coders - thoughts?","id":"t3_1keolh9","overview":"This Reddit post discusses Visa's ambiguous job posting for 'vibe coders,' sparking debate about the term's meaning and the technical requirements listed. The posting mentions skills like Python, FastAPI, PostgreSQL, vector databases, and containers, but the term 'vibe coding' remains undefined. Comments speculate that it might refer to AI-assisted development tools (e.g., Cursor, Zapier) or buzzword-laden HR jargon. The discussion highlights skepticism about the role's actual scope, with some arguing it could involve building low-stakes automation tools or leveraging AI for rapid prototyping. However, no concrete technical details about new models, infrastructure, or methodologies are presented in the original post. The conversation primarily revolves around job market practices and terminology confusion rather than advancements in AI/ML technology.","comment_overview":"The community is divided on the meaning of 'vibe coding,' with many dismissing it as vague or buzzword-heavy. Some commenters suggest it could relate to AI-powered coding assistants or low-stakes automation, while others mock the term as nonsense. Discussions touch on concerns about over-reliance on AI tools, unsustainable costs of generative AI usage, and skepticism about job postings prioritizing buzzwords over actual requirements. A few comments note Visa's broader AI ambitions, but these remain speculative. The tone is largely critical and humorous, with little technical analysis of the role itself.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/","relevance":"","is_relevant":false,"thumbnail_url":"https://preview.redd.it/gefvhv84qsye1.png?width=640\u0026crop=smart\u0026auto=webp\u0026s=235b3e1de1b7df4bd1bc1f7519f84b5259303d05"},{"title":"UI-Tars-1.5 reasoning never fails to entertain me.","id":"t3_1keo3te","overview":"This post references a 7B-parameter model called UI-TARS-1.5, developed by ByteDance-Seed, which appears to be a computer interaction agent. The discussion revolves around its potential capabilities and technical implementation, with users speculating about its training data (possibly 'Gen-Z data') and compatibility with tools like llama.cpp. However, no official documentation or technical specifications are provided in the post itself. The model's purpose seems to involve automating tasks like reading terms of service or navigating websites, but the lack of concrete details about its architecture, training methodology, or performance metrics makes it difficult to assess its significance. The mention of GGUF format conversion and vision support in llama.cpp suggests community efforts to adapt the model for specific use cases, but these remain unverified claims.","comment_overview":"The comments highlight a mix of curiosity and skepticism. Users question how the model works, noting that it hasn't been officially converted to GGUF format or integrated with vision capabilities in llama.cpp. Some jokingly suggest it's 'trained on Gen-Z data' or 'tiktok AI getting lazy.' Others debate the practicality of delegating tasks like cookie policy reviews to an AI, while a few point to a GitHub repository (cua) as a potential implementation. The conversation lacks technical depth, leaning more toward speculation and humor than rigorous analysis.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/","relevance":"","is_relevant":false,"thumbnail_url":"https://preview.redd.it/627wnr5emsye1.jpeg?width=640\u0026crop=smart\u0026auto=webp\u0026s=b896f5165e878160c1e104137518ab1d80b3addc"},{"title":"Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)","id":"t3_1kepuli","overview":"LocalLlama has launched an open-source initiative to standardize on-device AI performance benchmarks for Qwen3 across 50+ devices, including iOS, Android, Mac, and Windows. The project focuses on metrics like tokens-per-second (toks/s) and RAM utilization, with initial data for Qwen3 using GGUF formats from Unsloth. Key technical details include prefill tokens (512) and generation tokens (128), with benchmarks highlighting device-specific performance nuances. The effort addresses a critical gap in evaluating model viability for edge deployment, where slow or resource-heavy models risk user dissatisfaction. While the platform currently supports Core ML, ONNX, and TFLite, GGUF support is still under development. The data reveals intriguing anomalies, such as Q8 quantizations outperforming Q4 on some devices, and raises questions about memory management (e.g., Metal backend OOM errors producing incorrect results). The project also invites community participation for custom benchmarks, though scalability and data interpretation remain challenges.","comment_overview":"The community is split between enthusiasm for the standardized benchmarks and frustration with technical limitations. Users highlighted impressive iPhone 16 Pro Metal performance for Qwen3-1.6B-Q8 but questioned why Q8 models outperformed Q4 variants despite higher precision. Discussions around MacBooks showed mixed results, with some noting similar CPU/GPU performance on M-series chips. A critical comment pointed out severe issues: Metal backend OOM errors caused wildly inaccurate results (e.g., 99k tok/s prefill instead of ~150), and RAM usage metrics on Apple silicon were deemed unreliable. UI/UX feedback emphasized the need for column customization, simplified device naming (e.g., removing 'Apple'/'Bionic' labels), and merging redundant CPU/GPU entries. Practical questions emerged about switching to Metal on iOS via PocketPal, Android compatibility issues, and Vulkan GPU usage on laptops. The project's transparency and community-driven approach were praised, but users stressed the need for clearer data normalization and error handling.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/","relevance":"","is_relevant":true,"thumbnail_url":"https://preview.redd.it/z5qxhpc1zsye1.png?width=913\u0026format=png\u0026auto=webp\u0026s=c48aa6c5c753c7dc74c4397aac34f92383d17afe"},{"title":"Qwen 30B A3B Performance Degradation with KV Quantization","id":"t3_1kewkno","overview":"A user reported significant performance issues with Qwen 30B A3B when using KV (key-value) cache quantization during long-generation tasks, particularly in solving the OpenAI cypher test. The original successful test by sunpazed used Q4_K_M quantization without KV quantization, achieving correct results. However, the user encountered repetition loops and incorrect conclusions when employing Unsloth Q4_K_XL and Bartowski Q5_K_S quantizations with KV enabled. Disabling KV quantization resolved the issues, allowing successful execution with Q5_K_S and Q4_K_XL. The post highlights the critical role of KV cache precision in maintaining model coherence and task-solving ability, especially for complex reasoning tasks. Technical details include comparisons between quantization schemes (Q4_K_M, Q5_K_S, Q4_K_XL) and their interaction with KV cache settings. The findings suggest that KV quantization may introduce errors in long-context generation, potentially due to reduced numerical precision affecting attention mechanisms or state maintenance.","comment_overview":"The community emphasized the risks of aggressive KV quantization, with users warning that Q4_0 cache quantization severely degrades model quality, particularly for models using GQA (Grouped Query Attention). Some advocated for maintaining FP16 KV caches or limiting quantization to Q8_0. Discussions revealed debates over the trade-offs between memory efficiency and performance, with several users questioning the utility of KV quantization altogether. Technical troubleshooting suggestions included specific llama.cpp parameters (-ctk q8_0 -ctv q5_1) and testing without flash attention. A few users expressed confusion about the relationship between K_M quantization and KV cache settings, while others requested clarification on disabling KV quantization and measuring memory impacts.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/","relevance":"","is_relevant":true,"thumbnail_url":""},{"title":"QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison","id":"t3_1kenk4f","overview":"This post presents a detailed comparison of three large language models (LLMs) - QwQ 32b, Qwen 3 32b, and GLM-4-32B - focused specifically on their ability to generate HTML code for a frontend website. The evaluation uses a standardized prompt: 'Generate a beautiful website for Steve's pc repair using a single html script.' Each model's output is rated on layout quality (0-10) and analyzed for code complexity. GLM-4-32B achieves the highest score (9/10) with a 1,500+ line codebase featuring 'insanely good' layout quality comparable to Anthropic's Sonnet 3.7, while QwQ 32b scores poorly (3/10) with minimalistic code. The author notes GLM-4-32B's exceptional specialization in HTML/JavaScript but highlights its limitations in other domains like Python/C++ coding, reasoning, and math - where it performs similarly to Qwen 2.5 32b. Technical details include specific quantization (Bartowski's q4km version) and code line counts, providing concrete metrics for comparison.","comment_overview":"The community discussion highlights both technical insights and practical considerations. Many users corroborate GLM-4-32B's superiority in HTML generation while noting its limitations in other domains. Some compare it to other models like Phi-4 and UIGEN-T2, with one user praising GLM-4's 'gem' status for frontend tasks. However, criticisms emerge regarding its difficulty in handling iterative development and complex tasks like game creation with Babylon.js. The conversation also touches on technical implementation details - quantization methods, context window sizes, and inference settings - with warnings about Ollama's default configurations potentially harming model performance. Several users suggest further testing with more complex prompts, while others express surprise at the capabilities of smaller Qwen 3 models compared to human teenagers.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/","relevance":"","is_relevant":true,"thumbnail_url":"https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026format=png\u0026auto=webp\u0026s=f65d811c4859178fe80cbcb50312217ba5591c5b"},{"title":"LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!","id":"t3_1keoint","overview":"The llama.cpp ecosystem has seen significant performance boosts for Qwen3 MoE models, with both mainline and the ik_llama.cpp fork receiving critical optimizations. Mainline llama.cpp now features Flash Attention implementations for Grouped Query Attention (GQA) and Mixture of Experts (MoE) architectures, dramatically improving inference speeds for Qwen3 235B and 30B. The ik_llama.cpp fork, maintained by ikawrakow, introduced SotA iqN_k quantizations that balance model quality and size while maintaining speed. These updates are particularly impactful for users leveraging CUDA acceleration, with benchmarks showing measurable gains for fully offloaded MoE models. The competition between these projects has driven innovation in GGUF format optimization, benefiting the broader local LLM community. Notably, ik_llama.cpp's hybrid CPU+GPU offload capabilities and unique quantization schemes make it a compelling choice for users with heterogeneous hardware setups.","comment_overview":"The community is divided but excited about the updates. Some users report significant speed improvements, while others note regressions (e.g., a 15 TPS drop for one user). Discussions highlight practical considerations: ik_llama.cpp's multi-GPU crashes, the importance of Flash Attention for CUDA users, and debates over quantization tradeoffs. A key concern is platform specificity - many note these optimizations are CUDA-centric, leaving Apple Silicon users behind. Benchmarking enthusiasm is high, with requests for command examples to replicate results. The post sparks technical debates about MoE efficiency, GQA advantages, and the future of GGUF vs MLX formats on different hardware.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/","relevance":"","is_relevant":true,"thumbnail_url":"https://preview.redd.it/3bwwfd4epsye1.png?width=3404\u0026format=png\u0026auto=webp\u0026s=adbb0bce2c13bc560499b0d3459329d16d0a3291"}],"persona":"LocalLLaMa"}