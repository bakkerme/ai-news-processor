{"raw_input":["Title: What do I test out / run first?\nID: t3_1kexdgy\nSummary:       Just got her in the mail. Haven't had a chance to put her in yet.    submitted by    /u/Recurrents   [link]   [comments] \nImageDescription: \nComment:       Just got her in the mail. Haven't had a chance to put her in yet.    submitted by    /u/Recurrents   [link]   [comments] \nComment: First run home. Preferably safely. \nComment: llama 3.2 1b \nComment: Bro is loaded. How many kidneys did you sell for that?! \nComment: LLAMA 405B Q.000016 \nComment: all the new qwen 3 models \nComment: sexy ass card \nComment: Old School Runescape \nComment: Are they selling those already? \nComment: Download cuda and make sure your pytorch is the cuda version \nComment: Can it run Crysis? \nComment: You bought it just to benchmark it, didn't you? \nComment: Hello Kitty Island Adventures, butters would be proud of you. \nComment: Would you mind sharing or DMing retailer info? I don‚Äôt have a preferred vendor and am curious on your experience. \nComment: Llama 3.3 70b at 8-bit. Would be interesting to see how many tokens per second gives. \nComment: Try Super Mario Bros ü•∏ \nComment: you don't need it. gimme that. \nComment: Rtx pro 6000 is 96Gb it is beast. Without pro is 48gb. I really want to know how many FOPS it is. Or the t/s for a deepseek 70B or largest model it can fit. \nComment: Get some silly concurrency going on qwen 3 32b awq and run the aider benchmark. \nComment: Flux to generate pics of your dream Audi. Find out your use case and try some models that fit. I was first impressed by GLM 4 in one shot coding, but it fails to use other tools. Mistral small is my daily driver currently. It's even fluent in most languages. \nComment: That‚Äôs some expensive computer hardware. Congratulations. \nComment: That‚Äôs our serial number now \nComment: Your power connectors. \nComment: i cant imagine spending that much money on a gpu with that power connector \nComment: Houston we have lift off  https://preview.redd.it/v3z4prno2wye1.png?width=780\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=6a6156b3fc0818b93b0459a14c86a0e0dd1d70d7 \nComment: https://preview.redd.it/5bnvabxayvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=9516acddbdda888267887c823c70c25db1ba8c6e New card installed! \nComment: Is it better then a h100 performance wise? i know the vram is slightly bigger. \nComment: Quake I \nComment: Everything. In all seriousness, I would reaaally like to see the benchmarks on that thing \nComment: Old school runescape \nComment: Cancer research. \nComment: OT, but run 3Dmark and confirm if it really is faster in games than the 5090 (for once in the history of workstation cards). \nComment: dude you are so lucky congrats!! run every qwen 3 model and make videos! i hear you stream, how about a live stream using llama.cpp and testing out models, or lm studio. this card is so awesome üòç \nComment: Something like Gemma 3 27B/Mistral small-3/Qwen 3 32B with maximum context size? \nComment: Qwen3 and don‚Äôt look back \nComment: Qwen 30B A3B q8 has something around 30 GB file size. Should run very fast and have plent of room for context. \nComment: Bios \nComment: Where did you buy it from? \nComment: About $12,000 to $16,000 for the 48gb vram editions here .. not sure we can get the 96gb \nComment: What CPU are you pairing with? Linux? \nComment: https://preview.redd.it/glj9rjmk9vye1.jpeg?width=1280\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=64d6eac13d0a6aaed4b500953bfd300dcea46322 \nComment: ü•∫ü•πüò≠ \nComment: Wan 2.1 fp 32 model \nComment: Crysis. \nComment: Haha I thought it had a plaid pattern printed on it üòÖ \nComment: Dude so cheesed, could've even wait to get home \nComment: Hey, I was looking to buy one as well, how much did you pay and how long did it take to arrive. They are releasing so many cards these days I get confused. \nComment: How much \nComment: what version is it? Max‚ÄìQ? Workstation edition? Etc‚Ä¶ \nComment: GTA V \nComment: Grounding strap. \nComment: Crysis \nComment: Plex Media Server. But make sure to hack your drivers. \nComment: Mate, share some benchmarks!  I‚Äôm about ready to pull the trigger on one too, but the price gouging here is insane. They‚Äôre still selling Ampere A6000s for 6‚Äì7K AUD, and the Ada version is going for as much as 12K.  Instead of dropping prices on the older cards, they‚Äôre just marking up the new Blackwell ones way above MSRP. The server variant of this exact card is already sitting at 17K AUD (~11K USD)‚Äîabsolute piss take tbh. \nComment: Image and clip generation \nComment: I think I'll stream getting some LLMs and comfyui up tomorrow and the next few days. give a follow if you want to be notified https://twitch.tv/faustcircuits \nComment: Get that unsloth 235B Qwen3 model at Q2K_XL. It should fit. Q2 is the most efficient size when it comes to benchmark score to size ratio according to unsloths documentation. It should be fast AF too since only 22B active parameters.  \nComment: Nice! Still waiting for mine. Can you let me know if you are able to disable ECC or not? \nComment: what Audi is that? S4? \nComment: Nice. Run stuff and share stats! Would be cool to see. \nComment: https://preview.redd.it/ems9w2z6yvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=76b13f186be7cb783727c000bda533c92c1e8c56 here is the old card lol \n","Title: Visa is looking for vibe coders - thoughts?\nID: t3_1keolh9\nSummary:         submitted by    /u/eastwindtoday   [link]   [comments] \nImageDescription: ## Image Analysis: Visa \"Vibe Coder\" Job Requirements\n\nThe image displays a snippet of text outlining requirements for a position at Visa, seemingly related to \"Vibe Coding\" as indicated by the post title. Here's a detailed breakdown:\n\n**What is shown:**\n\n*   The image consists primarily of text, formatted as bullet points.\n*   A small icon resembling a stylized flame (likely representing \"technical\" skills) is present in the bottom right corner.\n*   There's a repeated pattern of seemingly random characters (in Tamil script) at the bottom, appearing as filler or potentially obfuscated text. This is unusual and warrants further investigation (see Insights).\n\n**Technical Details:**\n\n*   **Vector Databases:** Explicitly lists experience with `ChromaDB` and `Conecone`, indicating a focus on similarity search and embedding storage. This suggests the role involves working with large datasets where semantic understanding is crucial, rather than exact keyword matching.\n*   **Embedding Models:** The requirement for experience with embedding models directly ties into the vector database needs. This could involve using pre-trained models (e.g., Sentence Transformers, OpenAI Embeddings) or potentially training custom ones.\n*   **Containerization:**  `Docker` is specifically mentioned, indicating a modern DevOps workflow and likely deployment of models or applications within containers for scalability and reproducibility.\n*   **Kubernetes:**  The inclusion of `Kubernetes` further solidifies the DevOps aspect, suggesting orchestration and management of containerized applications at scale.\n*   **Frontend Design Systems:** Experience with `Lovable` and `V0` (likely a design system framework) points to the role involving building user interfaces that interact with these models or data.\n*   **Data Science Background:**  A \"big plus\" indicates the role likely utilizes data analysis and potentially model evaluation/tuning.\n\n**Relation to Post Title (\"Visa is looking for vibe coders - thoughts?\"):**\n\nThe term \"vibe coding\" appears to be a colloquialism for building applications that leverage LLMs (Large Language Models) and semantic search. The requirements strongly support this interpretation: the focus on vector databases, embedding models, and LLM-adjacent technologies (like semantic similarity) are core to building applications that \"understand\" user intent and provide relevant responses. It suggests the role is less about traditional coding and more about crafting experiences around AI-powered functionality.\n\n**Key Insights:**\n\n*   **LLM Focus:** Visa is actively investing in LLM-powered applications. The emphasis on vector databases and embedding models clearly indicates this trend.\n*   **Full-Stack Role:** The combination of backend (vector databases, Kubernetes) and frontend (design systems) requirements suggests a full-stack position.\n*   **Unusual Text Pattern:** The repeated Tamil script at the bottom is highly suspicious. It could be a deliberate obfuscation tactic, an error in text generation (if the job description was AI-assisted), or a test to identify bots/automated scrapers. This is the most interesting technical detail and deserves further investigation, as it could indicate a unique approach to applicant screening.\n*   **Modern Tech Stack:** Visa is utilizing current and popular technologies (Docker, Kubernetes) for deploying and managing their AI infrastructure.\n\n\n\n\nComment:         submitted by    /u/eastwindtoday   [link]   [comments] \nComment: The way it's written with the other requirements makes me think that they want a programmer able to write vibe coding tools, not specifically hiring a vibe coder for programming. \nComment: \"Background in Data Science is a big plus\" - Be prepared to be payed less than a proper data scientist. \nComment: Dumb. Vibe coding is a bit ill-defined but what I see is a niche where people like product managers can use AI to build tiny tools that would be hard to get into a product team's priority list. In particular what I saw was using Zapier which is a user-friendly automation tool that lets you run snippets of Python or js inside workflows and now lets you generate them with AI. Great way to build little utilities without a developer but also very narrow, low stakes applications that aren't critical path. This job req wants familiarity with vector DBs and containers which means they want an actual experienced software engineer. Someone who can use a coding assistant but probably doesn't need one. That's not vibe coding. \nComment: Vibe coding 2025 is same as slapping an untested, ill designed, 700-lines-a-function Python script on anything and calling it a day in the 2015s Edit: You scrolled down on some requirements tho https://www.visa.ca/fr_ca/jobs/REF061638W \u0026gt; Strong proficiency with Python, FastAPI and PostgreSQL for backend development. \nComment: These days, stupid HRs are stuffing the job requirements sections with as many tech buzzwords as possible, regardless of whether these buzzwords actually reflect the technologies used in the company. I feel like \"vibe coding\" may be one of those buzzwords. \nComment: now exactly nobody will know why credit card applications failed \nComment: Maybe we should all \"vibe-pay\" our Visa card bills. \nComment: Click click, sold all visa stocks instantly \nComment: I for one am happy my CC transactions are handled by vibed up systems. \nComment: they are starting up an ml intership for students next summer in poland too \nComment: I don't even know what to think. My general reaction of most job posts is to assume the companily already has someone they're gonna hire, and the position is only being posted as a formality and to make it appear that the company has fair hiring practices (even though requirements weed out people that are perfectly capable of being trained). \nComment: I know some folks working with such teams. They have ambitions to build vibe coding tools (like cursor) but without the dependency on letting data go out of the org (from a financial reg pov). Visa‚Äôs CTO is one of the highest paid CTOs in the US, and they are building agents replicating (gamma.ai, OpenAI deep research, Agentic Commerce) to start with. \nComment: They are even Vibe-using Caps. That's so Vibe. It gives me Bad Vibes. \nComment: That job posting is clear as mud. Title is an obvious mismatch for the skillset. \nComment: well I'm glad none of my credit cards are visa...  \nComment: what could possibly go wrong? \nComment: If they are listing the tech stacks that should be enough. \nComment: At least they're not requiring minimum 15 years of experience \nComment: Lmao \nComment: AI generated job posting ? :p \nComment: Vibe coding doesn't work. It takes me 10-20 prompts to generate anything useful with gemini 2.5.  Sometimes 30+ prompts even with explicit instructions. \nComment: And I thought it was a big deal 15 years ago when I talked NASA into letting me put ‚Äúauto-generated‚Äù guidance and control code into a satellite. That particular NASA center had never done such a thing, and I had to pop their cherry on it (as a contractor). Partly because a colleague of mine was doing the same thing on a bigger project, and he really wanted to be able to say his project wasn‚Äôt the first. The ‚Äúauto-generation‚Äù step is just compiling MatLab down into C code, and from there is just standard C compilation on the target environment. But the MatLab becomes the source code, in this case. And you have to version-control the auto-generation configuration as well.  Honestly, with all the other shit we had to deal with on that program, just telling an AI to ‚Äúwrite some code that might look like spacecraft guidance and control software, no particular requirements because we‚Äôre don‚Äôt have the money to actually test the hardware before launch, and most of it is going to fail anyway‚Äù would have been such a stress-saver. That‚Äôs a good vibe. \nComment: As someone already posted, these are not the full requirements, but another thing also:  Vibe-coding is expensive. Requests can quickly become $1-2 a piece. Over the course of a month, you could easily rack up $500+ if you're using it liberally. Now consider that you're already paying a regular salary, put this on top, and it becomes kind of unsustainable.  The money has to come from somewhere, and it's most probably the base pay. \nComment: I don't know much about developers as vibe coders. But, a lot of project managers \u0026amp; UI UX folks and product managers are starting to learn these tools (lovable, cursor, etc.,) and will roll out the basic screens for evaluation.  Also, this will start getting back to actual developers on justifying their timelines, complexity justifications, etc., This is mostly as nobody gives a fuck about code maintainability, reusability, tech debt, security, etc,.  Other side to this coin is 90% of code written by \u0026lt;5 years experienced devs are not following these standards anyways ü§∑‚Äç‚ôÇÔ∏è 90% of products from start-ups and other IT companies never see the light anyways ü§¶‚Äç‚ôÇÔ∏èü§£ \n","Title: UI-Tars-1.5 reasoning never fails to entertain me.\nID: t3_1keo3te\nSummary:       7B parameter computer use agent.    submitted by    /u/Impressive_Half_2819   [link]   [comments] \nImageDescription: ## Image Analysis: UI-Tars-1.5 Reasoning Demonstration\n\nThe image showcases a user interface (UI) interacting with a Large Language Model (LLM), specifically identified as \"ByteDance-Seed/UI-Tars-1.5-7B\". The UI appears to be a macOS application, likely a desktop client for interacting with LLMs. \n\n**Key Elements \u0026 Technical Details:**\n\n* **LLM Provider and Model Selection:** A dropdown menu allows the user to select from pre-defined LLMs or input a \"Custom model...\" name. The selected model is \"ByteDance-Seed/UI-Tars-1.5-7B\", indicating a 7 billion parameter model developed by ByteDance's Seed team.  This suggests the UI is designed to support a variety of models, not just a single proprietary one.\n* **Chat Interface:**  A text box labeled \"Ask me to perform tasks in a virtual macOS environment\" is present, indicating the LLM's capability to simulate actions within a macOS context.\n* **LLM Response/Thought Process:**  A chat-like window displays the LLM's internal reasoning. The text reveals a fascinating \"chain of thought\" process where the model recognizes its inability to directly access information (specifically, a repository search) and explicitly states it will attempt a workaround (\"I get rid of this pop-up\"). This is crucial for understanding the model's behavior.\n* **Repetitive Text Artifact:** The long string of seemingly random characters (\"byte...\") below the \"Custom Model Name\" is a clear artifact. This likely indicates an issue with text encoding or rendering, potentially due to the model's output being misinterpreted by the UI. It could also be a deliberate attempt to obfuscate or pad the model's output for some reason.\n* **UI Framework:** The UI design elements (dropdown, text boxes, chat window) suggest a modern framework, likely using native macOS components or a cross-platform UI toolkit.\n\n**Relation to Post Title (\"UI-Tars-1.5 reasoning never fails to entertain me\"):**\n\nThe post title directly references the LLM being used (\"UI-Tars-1.5\"). The image demonstrates *why* the reasoning is entertaining.  The model's self-awareness (recognizing its limitations), explicit problem-solving attempt (\"get rid of this pop-up\"), and the somewhat humorous phrasing reveal a more nuanced reasoning process than simply generating text. The repetitive text artifact adds to the \"entertaining\" aspect, highlighting unexpected behavior.\n\n**Key Insights:**\n\n* **Chain-of-Thought Reasoning:** The model exhibits a clear chain of thought, articulating its reasoning steps before attempting an action. This is a key feature for improving LLM reliability and interpretability.\n* **Self-Awareness \u0026 Limitations:** The model acknowledges its inability to directly search for a repository, suggesting some level of awareness about its capabilities and limitations.\n* **Potential UI/Encoding Issues:** The repetitive text artifact points to potential problems with how the UI handles model output, potentially impacting user experience and data integrity.\n* **Virtual Environment Simulation:** The prompt and UI design suggest the model is capable of simulating tasks within a macOS environment, which opens up possibilities for automated testing, scripting, and interactive applications.\n\n\n\n\nComment:       7B parameter computer use agent.    submitted by    /u/Impressive_Half_2819   [link]   [comments] \nComment: What's more important here is the model used - ByteDance-Seed/UI-TARS-1.5-7B the model which it is meant to be used with, so how did you make it work? Because last time I checked I haven't seen that model being converted to GGUF format, nor having vision support added into llama.cpp for it. \nComment: I guess: https://github.com/trycua/cua \nComment: When you train a model to use computers for humans and do the tiresome ToS reading, but it can't be bothered to do it either \nComment: Most probably trained on Gen-Z data. \nComment: https://preview.redd.it/4ignwtxrhuye1.jpeg?width=1079\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=e5d30270b51854c061aebdd502448897c63fed18 \nComment: On one hand, I guess I'd like the language model to read language on my behalf - on the other hand I wouldn't want the model to decide the cookies policy warrants user review or some other distraction so maybe skipping it is for the best after all. It does seem reading the pop-up falls within the scope of accessing the site to search for a repository \nComment: Try out yourself using cu/a! \nComment: I mean, fair \nComment: tiktok ai getting lazy \nComment: TARS, would you set your attention span setting to 8 for me? \nComment: It‚Äôs the defaut personality? \n","Title: Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)\nID: t3_1kepuli\nSummary:       Hey LocalLlama! We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon. We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device). Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist. We figured we'd kickstart this since we already maintain this benchmarking infra/tooling at RunLocal for our enterprise customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and TFLite to date, so a few things are still WIP for GGUF support.  Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill Tokens=512, Num Generation Tokens=128). GGUFs are from Unsloth üêê Qwen3 GGUF benchmarks on laptops Qwen3 GGUF benchmarks on phones You can see more of the benchmark data for Qwen3 here. We realize there are so many variables (devices, backends, etc.) that interpreting the data is currently harder than it should be. We'll work on that! You can also see benchmarks for a few other models here. If you want to see benchmarks for any others, feel free to request them and we‚Äôll try to publish ASAP! Lastly, you can run your own benchmarks on our devices for free (limited to some degree to avoid our devices melting!). This free/public version is a bit of a frankenstein fork of our enterprise product, so any benchmarks you run would be private to your account. But if there's interest, we can add a way for you to also publish them so that the public benchmarks aren‚Äôt bottlenecked by us.  It‚Äôs still very early days for us with this, so please let us know what would make it better/cooler for the community: https://edgemeter.runlocal.ai/public/pipelines To more on-device AI in production! üí™ https://i.redd.it/aev5rgjazsye1.gif    submitted by    /u/intofuture   [link]   [comments] \nImageDescription: \n\nExternal URL Summaries:\n- https://huggingface.co/collections/unsloth/qwen3-680edabfb790c8c34a242f95: \n\n\n\nThe article introduces the Qwen3 model collection by unsloth on Hugging Face, featuring various versions of Qwen3 with different parameter sizes (from 0.6B to 32B), formats (GGUF, 4-bit and 16-bit Safetensors), and context lengths (up to 128K). It includes dynamic GGUFs, 4-bit and 16-bit safetensors for fine-tuning and deployment, as well as FP8 versions. Each model variant provides details on updates, downloads, and notes about its specific use cases.\nComment:       Hey LocalLlama! We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon. We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device). Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist. We figured we'd kickstart this since we already maintain this benchmarking infra/tooling at RunLocal for our enterprise customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and TFLite to date, so a few things are still WIP for GGUF support.  Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill Tokens=512, Num Generation Tokens=128). GGUFs are from Unsloth üêê Qwen3 GGUF benchmarks on laptops Qwen3 GGUF benchmarks on phones You can see more of the benchmark data for Qwen3 here. We realize there are so many variables (devices, backends, etc.) that interpreting the data is currently harder than it should be. We'll work on that! You can also see benchmarks for a few other models here. If you want to see benchmarks for any others, feel free to request them and we‚Äôll try to publish ASAP! Lastly, you can run your own benchmarks on our devices for free (limited to some degree to avoid our devices melting!). This free/public version is a bit of a frankenstein fork of our enterprise product, so any benchmarks you run would be private to your account. But if there's interest, we can add a way for you to also publish them so that the public benchmarks aren‚Äôt bottlenecked by us.  It‚Äôs still very early days for us with this, so please let us know what would make it better/cooler for the community: https://edgemeter.runlocal.ai/public/pipelines To more on-device AI in production! üí™ https://i.redd.it/aev5rgjazsye1.gif    submitted by    /u/intofuture   [link]   [comments] \nComment: Iphone 16's Metal performance is pretty impressive for 1.6b-q8. But I do wonder why q8's performance is faster than q4 in that particular setup. \nComment: It‚Äôs interesting to see that performance in m4 is pretty similar in both cpu and gpu \nComment: There's one edge factor you missed - on Metal backend when you get OOM you get completely wrong results. For example on Qwen3 8B Q4 your results are like this: - MacBook Pro M1, 8GB = 99232.83tok/s prefill, 2133.70tok/s generation - MacBook Pro M3, 8GB = 90508.66tok/s prefill, 2507.50tok/s generation If you wouldn't get OOM the correct results for that model should be around ~100-150tok/s prefill and ~10tok/s generation. Additionally, all results for RAM usage on Apple silicon \u0026amp; Metal are not correct. In terms of your UX/UI there's tons of stuff that should be improved. but to not make this into very long post I'll write about biggest problems that can be fixed rather easily. First, add option to hide columns, there's too much redundant information that should be possible to hide with just couple of clicks. Second, decide on some naming scheme for components and stick with it. https://preview.redd.it/8idjxghteuye1.png?width=91\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=12d1d9317495b745768b9dd012288be1eb804964 I would suggest to get rid of 'Apple'/'Bionic' names altogether - it just adds to complexity and cognitive load to a table that is already very dense. There is no non-Apple M1 in Macbooks or non-Bionic A12 in iPad, so you don't need to clarify that much in a first place and additionally this page is aimed at technical people. Exact same problem with Samsung/Google vs Snapdragon. Third, if both CPU and Metal failed don't create two entries. Table is 2x longer than it should be with results that are non-comparable to anything. Just combine it into one entry. \nComment: How to run on metal on iphone 16 pro? I have pocketpal app and how to switch from cpu to metal? \nComment: if i'm reading this correctly the load time on cpu is better than gpu/metal for macbook pro but the gpu/metal is less memory intensive? also metal perf on iphone 16 is pretty impressive. \nComment: How do I run this on Android? Rn it just crashes \nComment: Why is Q8 faster than Q4??? \nComment: For laptops, is vulkan using the igpu ? \nComment: according to this data on iphone 16 you have 24 t/s on Q8 and 22 t/s on Q4 why so tiny models? \nComment: The iPhone 16e is listed to have the A18 Pro SoC but it actually has the A18.  https://preview.redd.it/h1dx2hgphvye1.png?width=623\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c2f8794bf27c6a9042b81040ebefa09873eae989 \n","Title: Qwen 30B A3B performance degradation with KV quantization\nID: t3_1kewkno\nSummary: I came across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4 that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization. I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens. I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens without finding any solution. Then I saw that sunpazed didn't used KV quantization and tried the same: boom! First time right. It worked with Q5_K_S and also with Q4_K_XL For who wants more details I leave here a gist https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef Do you have any report of performance degradation with long generations on Qwen3 30B A3B and KV quantization?    submitted by    /u/fakezeta   [link]   [comments]\nImageDescription: \n\nExternal URL Summaries:\n- https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4: \n\n\n\nThe article discusses the performance of Qwen3 30B A3B (4_0 quant) in solving a cipher problem first introduced in the OpenAI o1-preview technical paper. The model successfully solved the cipher in 5 minutes, significantly faster than QwQ, which took 32 minutes. The text includes detailed technical logs of the model's initialization and execution on an Apple M4 Pro GPU, highlighting its architecture, quantization, and performance metrics. The decoded text from the logs reveals a message: \"There are three r's in strawberry.\"\nComment: I came across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4 that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization. I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens. I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens without finding any solution. Then I saw that sunpazed didn't used KV quantization and tried the same: boom! First time right. It worked with Q5_K_S and also with Q4_K_XL For who wants more details I leave here a gist https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef Do you have any report of performance degradation with long generations on Qwen3 30B A3B and KV quantization?    submitted by    /u/fakezeta   [link]   [comments]\nComment: What KV quant level were you using? IMO on llama.cpp you shouldn‚Äôt push it past Q8_0. Q4_0 cache quant tanks quality in any model and especially models that heavily leverage GQA.  \nComment: I have one rule: I always test ALL new models without flash attention and with full 16bit KV cache. \nComment: Which KV quantization are you using? Don't have time to run this test right now, but I usually use -ctk q8_0 -ctv q5_1 (requires -DGGML_CUDA_FA_ALL_QUANTS=on) \nComment: Ive only tried KV quantization once and saw that any amount of it makes models super dumb. Not sure why anybody uses it tbh \nComment: Which quantization did you use initially? \nComment: Interested here since I'm running a q6 \nComment: Use these parameters: Thinking Mode Settings: Temperature = 0.6 Min_P = 0.0 Top_P = 0.95 TopK = 20 Non-Thinking Mode Settings: Temperature = 0.7  Min_P = 0.0 Top_P = 0.8 TopK = 20 \nComment: Could you please tell us how to disable KV cache quantisation? I'd also like to check the difference. What is the difference in the amount of memory used with KV running at fp16 in comparison with regular q4? \nComment: I‚Äôm confused. Isn‚Äôt K_M KV quantization? And yet you said Qwen 30b solved the rest with Q4 K_M? \nComment: Of course  Cache should always be fp16 even Q8 has degradation. Only flash attention is ok...ish ( as is fp16 ) \n","Title: QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.\nID: t3_1kenk4f\nSummary:       All models are from Bartowski - q4km version Test only HTML frontend. My assessment lauout quality from 0 to 10 Prompt \"Generate a beautiful website for Steve's pc repair using a single html script.\" QwQ 32b - 3/10 - poor layout but ..works , very basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b Qwen 3 32b - 6/10 - much better looks but still not too complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b272399ba8140785feb429a196ecc5173 GLM-4-32b 9/10 - looks insanely good , quality layout like sonnet 3.7 easily - 1500+ code lines https://preview.redd.it/3zj2lr2ahsye1.png?width=2469\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964 GLM-4-32b is insanely good for html code frontend. I say that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most. Other coding language like python , c , c++ or any other quality of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on the seme level but for html and JavaScript ... is GREAT.    submitted by    /u/Healthy-Nebula-3603   [link]   [comments] \nImageDescription: \nComment:       All models are from Bartowski - q4km version Test only HTML frontend. My assessment lauout quality from 0 to 10 Prompt \"Generate a beautiful website for Steve's pc repair using a single html script.\" QwQ 32b - 3/10 - poor layout but ..works , very basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b Qwen 3 32b - 6/10 - much better looks but still not too complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b272399ba8140785feb429a196ecc5173 GLM-4-32b 9/10 - looks insanely good , quality layout like sonnet 3.7 easily - 1500+ code lines https://preview.redd.it/3zj2lr2ahsye1.png?width=2469\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964 GLM-4-32b is insanely good for html code frontend. I say that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most. Other coding language like python , c , c++ or any other quality of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on the seme level but for html and JavaScript ... is GREAT.    submitted by    /u/Healthy-Nebula-3603   [link]   [comments] \nComment: Yeah, I have also tried to generate webpages with a couple of models, like GLM-4, Qwen3, Phi-4 Reasoning, etc. GLM-4 is so far the clear winner at these tasks. It's a gem in my model collection. \nComment: I've created a list of one-shot generated HTML pages using different models, big and small. https://blog.kekepower.com/ai/ \nComment: Ironically, so far userscript (javascript): Qwen 3 32B \u0026gt; GLM-4-0414. Don't get me wrong. I love GLM-4-0414, but it feels like it lacked the required understanding for my particular requests that Qwen 3 32B understood well. \nComment: Whats the temp? Did you rerun multiple times? \nComment: Why not try a slightly more complex task? E.g. a mini-game?  Create a single-HTML-page game using Babylon.js where the player controls a ship and moves about the open sea exploring islands to find treasure. A single, small map with 3 islands of which only one has the treasure is enough  In the first reply, the camera works, but WASD didn't. I copied-pasted the errors from the console a couple times and WASD works now. It looks terrible, but I guess that's expected without external assets. https://preview.redd.it/lf5foomq1tye1.png?width=1288\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b02ed407b3fbef31bf6d83aae5491cacaa93e150 Edit: This is my AWQ quant of Qwen-32B. \nComment: GLM falls flat on its face when I try to continue developing after the first prompt. It feels like a model trained (very well) for one-shots \nComment: Try UIGEN-T2 for html generation! There's also a react model.  https://huggingface.co/Tesslate/UIGEN-T2 \nComment: Would be interesting to see how these results compare to the recent Tesslate/UIGEN-T2-7B. It's a tuned version of Qwen 2.5 Coder 7B specifically for UI generation. \nComment: What quants what context window size? Ollama default size will kill QWQ reasoning if you don‚Äôt know how to set it up properly. \nComment: Still kind of freaked out that smaller Qwen 3 models are probably as good at website development as I was as a teenager. And a damn sight quicker too. \nComment: Thanks for posting the eval. Would be curious to see the prompt used as well. \nComment: Interesting. I ran the same input on QwQ with these settings: Temp: 0.6 Top p: 0.95 Min p: 0.0 Top k: 40 And quite a bit different output. Output: https://pastebin.com/Ntc8QQfH \nComment: GPT 4.1 is clearly the winner here in my opinion as well as claude sonnet 3.7 \n","Title: LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!\nID: t3_1keoint\nSummary:       You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B) mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert) tl;dr; I highly recommend doing a git pull and re-building your ik_llama.cpp or llama.cpp repo to take advantage of recent major performance improvements just released. The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving r/LocalLLaMA community! If you have enough VRAM to fully offload and already have an existing \"normal\" quant of Qwen3 MoE then you'll get a little more speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork! Details I spent yesterday compiling and running benhmarks on the newest versions of both ik_llama.cpp and mainline llama.cpp. For those that don't know, ikawrakow was an early contributor to mainline llama.cpp working on important features that have since trickled down into ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my understanding) the ik_llama.cpp fork was built and has a number of interesting features including SotA iqN_k quantizations that pack in a lot of quality for the size while retaining good speed performance. (These new quants are not available in ollma, lmstudio, koboldcpp, etc.) A few recent PRs made by ikawrakow to ik_llama.cpp and by JohannesGaessler to mainline have boosted performance across the board and especially on CUDA with Flash Attention implementations for Grouped Query Attention (GQA) models and also Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B releases! References  ikawrakow/ik_llama.cpp/pull/370     submitted by    /u/VoidAlchemy   [link]   [comments] \nImageDescription: \n\nExternal URL Summaries:\n- https://github.com/ikawrakow/ik_llama.cpp: \n\n\n\nThis is a fork of **llama.cpp** focused on improving **CPU performance** and adding **hybrid GPU/CPU inference** capabilities. Key features include new SOTA quantization methods, Bitnet support, enhanced DeepSeek performance via MLA and FlashMLA, fused MoE operations, and more. Note that new GGUF formats for DeepSeek-V3/R1/Lite are incompatible; use original GGUF or convert from HF safetensors. The project has seen numerous performance updates, including faster token generation, better MoE handling, and support for models like Llama-3, Qwen3, GLM-4, and more. Contributions are welcome.\nComment:       You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B) mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert) tl;dr; I highly recommend doing a git pull and re-building your ik_llama.cpp or llama.cpp repo to take advantage of recent major performance improvements just released. The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving r/LocalLLaMA community! If you have enough VRAM to fully offload and already have an existing \"normal\" quant of Qwen3 MoE then you'll get a little more speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork! Details I spent yesterday compiling and running benhmarks on the newest versions of both ik_llama.cpp and mainline llama.cpp. For those that don't know, ikawrakow was an early contributor to mainline llama.cpp working on important features that have since trickled down into ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my understanding) the ik_llama.cpp fork was built and has a number of interesting features including SotA iqN_k quantizations that pack in a lot of quality for the size while retaining good speed performance. (These new quants are not available in ollma, lmstudio, koboldcpp, etc.) A few recent PRs made by ikawrakow to ik_llama.cpp and by JohannesGaessler to mainline have boosted performance across the board and especially on CUDA with Flash Attention implementations for Grouped Query Attention (GQA) models and also Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B releases! References  ikawrakow/ik_llama.cpp/pull/370     submitted by    /u/VoidAlchemy   [link]   [comments] \nComment: I'm currently running ik_llama.cpp with Qwen3-235B-A22 on a Xeon E5-2680v4, that's a 10 year old CPU with 128GB ddr4 memory, and a single RTX3090. I'm getting 7 tok/s generation, very usable if you don't use reasoning. BTW the server is multi-GPU but ik_llama.cpp just crash trying to use multiple-gpus, but I don't think it would improve speed a lot, as the CPU is always the bottleneck. \nComment: Could you explain how to read your pictures? I see orange plot below red plot, so ik_llama.cpp is slower than llama.cpp? \nComment: Can you post some of the commands you use for the benchmarks? I want to tinker to see what is best for my use case \nComment: Oh, just updated. My rig is busy for running deepseek \u0026amp; ik_llama (1 week jobs). I will update after that :) \nComment: Maybe GGUF will now give same speed as MLX on Mac devices \nComment: I have a 3090. Doesn't this say it's slower, not faster? \nComment: https://preview.redd.it/0zroyhg1qsye1.png?width=3404\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b3e55128b1aac3f6d2ddfbd22597b9cd6d7dd02c In my limited testing you probably want to go with ik_llama.cpp for fully offloaded non-MoE models like the recent GLM-4 which is crazy efficient on kv-cache VRAM usage due to its GQA design. \nComment: I just pulled and rebuilt and I'm now actually going about 15 tps slower. My previous build was from about a week ago, and I was getting an eval time of about 54 tps. Now I'm only getting 39 tokens per second, so pretty significant drop. I just downloaded the latest unsloth model I'm running on 2 3090s, using this command: ``` .\\bin\\Release\\llama-server.exe -m C:\\shared-drive\\llm_models\\unsloth-2-Qwen3-30B-A3B-128K-Q8_0.gguf --host 0.0.0.0 --ctx-size 50000 --n-predict 10000 --jinja --tensor-split 14,14 --top_k 20 --min_p 0.0 --top_p 0.8 --flash-attn --n-gpu-layers 9999 --threads 24 ``` Prompt: \"tell me a 2 paragraph story\" \nComment: How close is llamacpp to vLLM and exllama now? \nComment: Seems like it is related to CUDA only, so I guess only for people with Nvidia cards and not folks on Apple Silicon and others. \n","Title: Apparently shipping AI platforms is a thing now as per this post from the Qwen X account\nID: t3_1kebauw\nSummary:         submitted by    /u/MushroomGecko   [link]   [comments] \nImageDescription: ## Image Analysis: Qwen AI Platform Shipping Announcement\n\nThe image depicts two cartoon characters ‚Äì a sloth and a bear ‚Äì seemingly presenting or celebrating something. While visually whimsical, the context from the post title (\"Apparently shipping AI platforms is a thing now as per this post from the Qwen X account\") suggests it‚Äôs related to the release or promotion of the Qwen AI platform.\n\n**What is Shown:**\n\n*   **Characters:** A light brown sloth character holding a small bouquet of flowers and a dark brown bear character. Both are styled in a cute, animated fashion.\n*   **Background:** A plain white background.\n*   **Text/UI Elements:** There is no visible text or traditional UI elements within the image itself. The entire image appears to be a promotional graphic.\n\n**Technical Details (Inferred from Context):**\n\nThe core takeaway is the implication of a \"shipping\" event. In the AI/ML world, ‚Äúshipping‚Äù typically refers to:\n\n*   **Model Release:** This likely indicates the release of a new version or iteration of the Qwen large language model (LLM).  Qwen, developed by Alibaba, is known for its open-source LLMs.\n*   **Platform Availability:**  It could also mean the broader availability of a platform or suite of tools built *around* the Qwen model, allowing developers to easily access and utilize it. This could include APIs, SDKs, or a hosted inference service.\n*   **Deployment Infrastructure:**  Less likely but possible: the release of tools or infrastructure to deploy Qwen models on various hardware configurations.\n\nThe cartoon style suggests a focus on accessibility and ease-of-use, potentially targeting developers who are new to LLMs. The celebratory nature implies a significant update or milestone for the Qwen project.\n\n**Relation to Post Title:**\n\nThe post title directly connects this image to the emerging trend of \"shipping\" AI platforms.  Previously, LLMs were often research projects or available only through limited access. Now, multiple entities (including Alibaba with Qwen) are actively making their models and associated tools readily available to the public. This image is a marketing tactic to announce or celebrate that availability.\n\n**Key Insights:**\n\n*   **Increased Accessibility of LLMs:** The image reinforces the trend towards democratizing access to powerful LLMs like Qwen.\n*   **Focus on Developer Experience:** The cute and approachable aesthetic suggests a deliberate effort to lower the barrier to entry for developers.\n*   **Competitive Landscape:**  The fact that \"shipping\" is highlighted indicates a growing competition in the LLM space, where ease of deployment and accessibility are key differentiators.\n*   **Marketing Strategy:** Using playful imagery to announce technical releases is a common strategy for attracting attention and building community.\n\n\n\n\nComment:         submitted by    /u/MushroomGecko   [link]   [comments] \nComment: Ah yes, the AI romcom. \nComment: theres lore now? \nComment: -Ok, lets see, i want to make love to you-. -But...wait. no no \nComment: What do you mean by \"shipping AI platforms\"? \nComment: Can't wait for the competitions to see which model writes the best fan fiction \nComment: Should AI companies also create their own VTuber characters that use their models? \nComment: Qwen guy looks mean. His expression is like he's never felt love. \nComment: I ship it! \nComment: And those two ship often \nComment: Hell yeah \nComment: Has anyone noticed better performance for similar quants of the same model by different creators? \nComment: Oh no \nComment: Love the contribution to the OSS these guys make!! Pure awesomeness \nComment: ü§¶üèª‚Äç‚ôÇÔ∏è \nComment: Anyone feeding into this just entirely lacks a social life, why are some of you like this? \n","Title: IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models\nID: t3_1kedu0d\nSummary:         submitted by    /u/ab2377   [link]   [comments] \nImageDescription: \n\nExternal URL Summaries:\n- https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek: \n\n\n\nIBM has released the **Granite 4.0 Tiny Preview**, a compact and efficient language model as part of its upcoming Granite 4.0 family. It is designed for consumer-grade hardware, offering performance comparable to larger models like Granite 3.3 2B Instruct with significantly reduced memory usage (72% less). The model uses a **hybrid Mamba-2/Transformer architecture**, combining the efficiency of Mamba with the precision of transformers. It features a **fine-grained mixture of experts (MoE)** structure, with 7B total parameters but only 1B active at inference. The model also employs **NoPE (no positional encoding)**, enabling long context lengths without performance degradation. Available on Hugging Face under an Apache 2.0 license, it is intended for experimentation rather than enterprise use. IBM plans to release the full version this summer, with broader platform support expected.\nComment:   \n                \n            submitted by   \n            /u/ab2377   [link]\n              [comments]\n            \nComment: Hope they\n            can release a larger one like 30b-a3b  \nComment: so a new\n            architecture, more moe goodness  \"Whereas prior generations\n            of Granite LLMs utilized a conventional transformer architecture, all models in the\n            Granite 4.0 family utilize a new hybrid Mamba-2/Transformer architecture, marrying the\n            speed and efficiency of Mamba with the precision of transformer-based self-attention.\n            Granite 4.0 Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts\n            (MoE) model, with 7B total parameters and only 1B active parameters at inference\n            time. Many of the innovations informing the Granite 4 architecture\n            arose from IBM Research‚Äôs collaboration with the original Mamba creators on Bamba, an\n            experimental open source hybrid model whose successor (Bamba v2) was released earlier\n            this week.\" \nComment: Please\n            look here: https://huggingface.co/ibm-granite/granite-4.0-tiny-preview/discussions/2\n            gabegoodhart IBM\n            Granite org 1\n            day ago Since this model is hot-off-the-press, we\n            don't have inference support in llama.cpp yet.\n            I'm actively working on it, but since this is one of the first major models\n            using a hybrid-recurrent architecture, there are a number of in-flight architectural\n            changes in the codebase that need to all meet up to get this supported. We'll\n            keep you posted! gabegoodhart IBM\n            Granite org 1\n            day ago We definitely expect the model quality to improve\n            beyond this preview. So far, this preview checkpoint has been trained on ~2.5T tokens,\n            but it will continue to train up to ~15T tokens before final release.\n            \nComment: i hope we\n            can see some larger models too! I really want them to scale those more experimental\n            architectures and see where it leads. I think there is huge potential in combining\n            attention with hidden state models. attention to understand context, hidden state to\n            think ahead, remember key information etc. \nComment: Read the\n            full thing. It‚Äôs worth it. \nComment: Holy,\n            this actually looks really good. IBM might actually be able to catch up with Alibaba\n            with this one. \nComment: Neat but\n            unless folks really start working to help add support for mamba architectures to\n            llama.cpp it'll be dead on arrival. It would be great to see\n            the folks at /u/IBM step up and help out\n            llama.cpp to support things like this. \nComment: \n            The Granite 4.0 architecture uses no positional encoding (NoPE). Our testing\n            demonstrates convincingly that this has had no adverse effect on long-context\n            performance.   This is interesting. Are there any\n            papers that explain why this still works? \nComment: Looking\n            very promising... \nComment: Is IBM\n            going to be the silent winner? It‚Äôs impressive that their tiny model is 8b MOE and\n            likely to perform at the same level as their previous dense 8b: Granite 4.0\n            Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts (MoE) model,\n            with 7B total parameters and only 1B active parameters at inference time.\n            I hope their efforts attempt to improve in https://fiction.live/stories/Fiction-liveBench-April-6-2025/oQdzQvKHw8JyXbN87\n            and not just passkey testing. \nComment: I'm just a dreamer without much background in\n            ML stuff. Can anyone with sense comment on how likely it is we'll ever see\n            something that might be so efficient it'll run well on CPU? I mean, this model\n            already sounds pretty exciting from an efficiency perspective. Wondering if\n            we've exhausted architectural changes that would e.g. reduce memory bandwidth\n            requirements \nComment: ‚ÄúWe‚Äôre\n            excited to continue pre-training Granite 4.0 Tiny, given such promising results so early\n            in the process. We‚Äôre also excited to apply our learnings from post-training Granite\n            3.3, particularly with regard to reasoning capabilities and complex instruction\n            following, to the new models. Like its predecessors in Granite 3.2 and Granite 3.3,\n            Granite 4.0 Tiny Preview offers toggleablethinking on andthinking off functionality\n            (though its reasoning-focused post-training is very much incomplete).‚Äù\n            I hope some of this involves interacting with fictional text in a creative\n            fashion: scene summaries, character profiles, plot outlining, hypothetical change\n            impacts - books are great datasets just like large code bases and just need a good set\n            of training data ‚Äî use Guttenberg public domain books that are modernized with AI and\n            then create training around the above elements. \nComment: Now if\n            only we could get IBM to sell a version of their AI card to the public\n            \nComment: ibm doing\n            better work than meta theyre surprisingly becoming a big player in open source (for\n            small models) \nComment: I wonder\n            what is prompt processing speed for semi-recurrent stuff compared to transformers.\n            Transformers have fantastic prompt processing speed like 1000t/s easy even on crap like\n            3060, but slow down during token generation as context grows. This seems to be the other\n            way around, slow PP but fast TG. I might be completely\n            wrong. \nComment: Large\n            datasets: all of Harry Potter series asking questions like, what would have to change in\n            the series for Harry to end up with Hermione or for Voldemort to win. It‚Äôs a series\n            everyone knows fairly well and requires details in the story and the story\n            whole. \nComment: I\n            remember seeing this model a few days ago. There's no gguf so i cant try it out.\n            I guess there's not a lot of interest in this moe or it's not currently\n            possibly to make ggufs for it at the moment.  Webui stopped working\n            for me last year after i updated it and I've never been able to get it working\n            right since then, so been using lm studio appimages. That program runs everything good\n            for me but only runs ggufs. \nComment: Yall need\n            to learn Transformers and stop hating on llama.cpp \n","Title: Qwen3 no reasoning vs Qwen2.5\nID: t3_1kegrce\nSummary: It seems evident that Qwen3 with reasoning beats Qwen2.5. But I wonder if the Qwen3 dense models with reasoning turned off also outperforms Qwen2.5. Essentially what I am wondering is if the improvements mostly come from the reasoning.    submitted by    /u/No-Bicycle-132   [link]   [comments]\nImageDescription: \nComment: It seems evident that Qwen3 with reasoning beats Qwen2.5. But I wonder if the Qwen3 dense models with reasoning turned off also outperforms Qwen2.5. Essentially what I am wondering is if the improvements mostly come from the reasoning.    submitted by    /u/No-Bicycle-132   [link]   [comments]\nComment: On aider polyglot there was a huge boost from 16.4% of qwen 2.5 coder whole to qwen 3 32B (no thinking) achieving 45.8% being ahead of gpt 4.5. \nComment: Don't stop at wondering. Why don't you test it and share your result with us? \nComment: https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/ \nComment: Depends on the task. For code autocomplete Qwen/Qwen3-14B-AWQ nothink is awful. I like Qwen2.5-coder:14b. Additionally: some quants might be broken. \nComment: The 2.5 coders are better at complex one shots. 3.0 seems to generalize better and retains logic over a multiturn edit. My work involves updating lots of legacy Fortran and cobol that is written with very specific formatting and comment practices. 3.0 is the first open model that can run reasonably at 48gb vram that can reliably port my code. Also I think, for coding one shot diffs, reasoning turned off produces better results. \nComment: I have compared them far too little to be able to draw a serious conclusion, but from the very few comparisons I have made in coding, Qwen3 (no thinking) outputs better code, more accordingly to the prompt, than Qwen2.5. \nComment: The short answer is it entirely depends on your use case. In my limited testing, their overall performance was pretty close, with Qwen 3 probably being better overall.  I know the benchmarks say otherwise, but when translating Japanese to English, I found Qwen 2.5 to sound more natural.  However, when summarizing short stories, Qwen 2.5 dissected the story like a technical manual, whereas Qwen 3 wrote (or tried to write) in the tone of the original story.  Qwen 3 seems to lose less when quantized than Qwen 2.5. I was shocked at how well Qwen 3 32b functioned even down to IQ2 (except for factual retrieval which as usual takes a big hit). Coding, logical puzzles, and problem-solving seemed like a toss up. They both did it with more or less the same success; although, enabling reason will likely give Qwen 3 the edge. \nComment: Yes from what I have seen for apples to apples. But the 2.5 coding models will probably still hold tier own vs regular 3 models with thinking off. \nComment: They do. Qwen3 8b outperforms 7b 2.5; at least because of that extra 1b. \n"],"results":[{"title":"Unboxing a High-End GPU: What to Test First?","id":"t3_1kexdgy","overview":"This post documents a user's unboxing of a high-end GPU (likely an NVIDIA RTX A6000 or similar) and sparks a discussion about optimal use cases for the hardware. While the original post is minimalistic, the comment thread reveals technical details about potential workloads, including running large language models (LLMs) like Llama 3.2 1B, Qwen 3, and DeepSeek 70B, as well as benchmarks for gaming (Crysis, Quake I) and AI tasks. The conversation highlights the GPU's 48GB/96GB VRAM capabilities, CUDA setup requirements, and comparisons to older cards like the H100. However, no specific technical specifications about the GPU itself are provided in the post, and the discussion remains speculative rather than reporting on a concrete AI/ML development.","comment_overview":"The community is highly engaged, with users suggesting specific LLMs to test (e.g., Qwen 3 32B AWQ, Gemma 3 27B) and debating hardware capabilities. Discussions about VRAM limits, CUDA versions, and benchmarking tools (3DMark, lm-studio) dominate. Some users express skepticism about the GPU's value proposition, citing high costs and price gouging. Others focus on practical use cases like AI research, media processing, and gaming. The thread also includes lighthearted jokes about 'Crysis' and 'Old School Runescape,' but no significant technical analysis of the hardware itself is presented.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/","relevance":"","is_relevant":false},{"title":"Visa is looking for vibe coders - thoughts?","id":"t3_1keolh9","overview":"This Reddit post analyzes a Visa job posting that references 'vibe coding,' a term interpreted as building AI-powered applications using large language models (LLMs) and semantic search. The technical requirements explicitly mention vector databases like ChromaDB and Conecone, embedding models, Docker/Kubernetes for DevOps, and frontend design systems (Lovable/V0). The role appears to blend backend AI infrastructure (vector search, model deployment) with frontend development, suggesting a full-stack position. Notably, the job description includes suspicious Tamil script at the bottom, potentially indicating obfuscation, AI-generated content, or a bot-screening mechanism. The post highlights Visa's growing investment in LLMs for financial applications, though the term 'vibe coding' remains ambiguously defined. The technical stack aligns with modern AI workflows, emphasizing scalability and semantic understanding over traditional coding.","comment_overview":"The community is divided on the authenticity and clarity of the job posting. Many question whether 'vibe coding' is a genuine role or an HR buzzword for AI/ML positions. Critics argue that the requirements‚Äîlike Kubernetes and vector databases‚Äîsuggest a seasoned software engineer rather than a 'vibe coder.' Others speculate the posting might be AI-generated or overly technical to filter candidates. Concerns about pay disparities, job security, and the practicality of AI-assisted development dominate discussions. Some users humorously mock the term 'vibe coding,' while others note Visa's broader trend of adopting AI tools for financial systems. A few commenters highlight the suspicious Tamil script as a potential red flag or test for automated applicants.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/","relevance":"","is_relevant":true},{"title":"UI-Tars-1.5 reasoning never fails to entertain me.","id":"t3_1keo3te","overview":"The image reveals a macOS-based UI interface for ByteDance's 7B-parameter 'UI-Tars-1.5' model, showcasing its chain-of-thought reasoning capabilities. The UI allows selection of pre-defined LLMs or custom models, with the displayed interaction demonstrating the model's self-awareness of its limitations. When confronted with a repository search task, the model explicitly states it will 'get rid of this pop-up'‚Äîa clear example of deliberate problem-solving. The UI's chat interface highlights the model's internal monologue, revealing a structured reasoning process rather than direct output. A notable artifact appears as a repetitive 'byte...' string, likely stemming from encoding issues or model output padding. The interface's design suggests integration with a virtual macOS environment, implying potential applications in automation or interactive task execution. This development is significant as it demonstrates progress in making LLMs more transparent about their reasoning and limitations, while also highlighting challenges in UI/LLM integration.","comment_overview":"The community is fascinated by the model's self-aware reasoning but raised technical questions about its compatibility. Users debated whether ByteDance's UI-Tars-1.5-7B could run in llama.cpp, noting the lack of GGUF conversion or vision support. Some joked about the model's 'lazy' behavior, comparing it to TikTok AI, while others speculated about its training data ('Gen-Z data'). The UI's pop-up navigation dilemma sparked discussions about ethical AI decision-making. A GitHub link for 'cua' tools emerged as a potential solution, with users encouraging experimentation. Humor about adjusting 'attention span settings' underscored the model's anthropomorphic appeal, but technical concerns about text artifacts and format compatibility remained central.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/","relevance":"","is_relevant":true},{"title":"Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)","id":"t3_1kepuli","overview":"This post introduces an open-source initiative by RunLocal to benchmark Qwen3 performance across 50+ devices (iOS, Android, Mac, Windows), focusing on metrics like tokens-per-second (toks/s) and RAM utilization. The project aims to address the critical need for standardized performance data to ensure viable on-device AI deployment, as suboptimal metrics can lead to app crashes or poor user experiences. Key technical details include Qwen3 GGUF benchmarks with 512 prefill tokens and 128 generation tokens, supported formats (Core ML, ONNX, TFLite), and WIP GGUF support. The initiative highlights challenges like variable device backends (e.g., Metal on Apple silicon) and plans to expand to 100+ devices. Notably, the data reveals intriguing anomalies, such as Q8 quantization outperforming Q4 on iPhones, which sparks community debate about model optimization and backend interactions. The project also offers a free public tool for users to run custom benchmarks, though with limitations to prevent hardware overloads.","comment_overview":"The community is actively engaged, with users dissecting performance discrepancies (e.g., why Q8 outperforms Q4 on iPhones) and raising technical concerns. A critical comment highlights Metal backend issues causing incorrect results due to out-of-memory (OOM) errors, emphasizing the need for accurate RAM tracking. Others suggest UI improvements, like column customization and simplified device naming conventions, to reduce cognitive load. Questions about running benchmarks on Android and iOS (e.g., switching to Metal via PocketPal) reveal practical barriers. The discussion underscores the importance of transparency in benchmarking, with users advocating for clearer data presentation and standardized metrics. Despite challenges, the project is seen as a valuable step toward democratizing on-device AI evaluation.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/","relevance":"","is_relevant":true},{"title":"Qwen 30B A3B Performance Degradation with KV Quantization","id":"t3_1kewkno","overview":"A user reported significant performance issues with Qwen 30B A3B when using KV quantization during long-generation tasks, particularly in solving the OpenAI cypher test. The model exhibited repetition loops and incorrect conclusions when employing quantization schemes like Q4_K_XL and Q5_K_S, but worked correctly without KV quantization. Technical logs from a Gist show the model solved the cipher in 5 minutes with Q4_K_M quantization, achieving a decoded message: 'There are three r's in strawberry.' The issue appears tied to KV cache quantization, as disabling it restored proper behavior. Users experimenting with llama.cpp noted that pushing KV quantization beyond Q8_0 degrades quality, especially for models using GQA (Grouped Query Attention). The discussion highlights trade-offs between memory efficiency and task accuracy when applying quantization.","comment_overview":"The community emphasized caution with KV quantization, with users warning that Q4_0 cache quantization severely impacts model quality. Several recommended using fp16 KV caches or limiting quantization to Q8_0 for stability. Discussions revealed confusion about terminology (e.g., whether Q4_K_M involves KV quantization) and frustration with inconsistent results. Some users shared parameter configurations (e.g., temperature settings, quantization flags) to mitigate issues, while others questioned the utility of KV quantization altogether. A recurring theme was the need to test models with full fp16 KV caches for critical tasks, as quantization-induced degradation could undermine performance.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/","relevance":"","is_relevant":true},{"title":"QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison","id":"t3_1kenk4f","overview":"This post presents a user-driven comparison of three large language models (QwQ 32b, Qwen 3 32b, and GLM-4-32B) focused on their ability to generate HTML code for a 'Steve's PC Repair' website. The assessment uses a subjective 0-10 scale for layout quality and counts lines of code generated. GLM-4-32B receives the highest score (9/10) with a 1,500+ line output, while QwQ 32b scores poorly (3/10) with only 250 lines. The author notes GLM-4-32B's exceptional performance in HTML/Javascript but claims its capabilities in other domains (e.g., Python, math reasoning) align with Qwen 2.5 32b levels. Technical details include q4km quantization, but no official model specifications are provided. The comparison highlights significant variation in frontend code quality across models, though it remains a single-use case evaluation.","comment_overview":"The discussion reveals mixed perspectives. While some users corroborate GLM-4-32B's HTML superiority, others report conflicting results - one noted Qwen 3 32b outperforming GLM-4 in JavaScript tasks. Several commenters suggest limitations of single-shot generation and request more complex challenges (e.g., interactive games). Technical debates emerge about quantization methods (q4km vs AWQ), context window settings, and temperature parameters affecting outputs. A few users share alternative tools like UIGEN-T2 for UI generation, while others question the validity of subjective scoring without standardized benchmarks.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/","relevance":"","is_relevant":false},{"title":"LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!","id":"t3_1keoint","overview":"The llama.cpp ecosystem has seen significant performance boosts, particularly for large models like Qwen3 MoE (235B and 30B). Both mainline llama.cpp and the ik_llama.cpp fork have received optimizations, with ik_llama.cpp emphasizing hybrid CPU+GPU inference and SOTA iqN_k quantizations. Key improvements include CUDA-specific optimizations like Flash Attention for Grouped Query Attention (GQA) models and enhanced Mixtral of Experts (MoE) handling. Mainline llama.cpp now offers better speed for fully offloaded Qwen3 MoE models, while ik_llama.cpp provides unique features like iqN_k quants (not available in ollama or lmstudio) and better support for hybrid workloads. The updates highlight competitive innovation in the GGUF community, with users reporting varied results depending on hardware and model configurations.","comment_overview":"The community is split between excitement over performance gains and confusion about conflicting benchmark results. Some users report significant speed improvements, while others note slowdowns after updates, possibly due to model-specific optimizations. Discussions highlight hardware dependencies: NVIDIA GPU users benefit most from CUDA boosts, while Apple Silicon users remain unaffected. Questions about benchmarking commands and the mysterious 'orange plot below red plot' in a shared graph reveal technical curiosity. Concerns about ik_llama.cpp crashing on multi-GPU setups and compatibility issues with new GGUF formats for DeepSeek models also surface. Overall, the thread reflects a vibrant, hands-on community pushing inference frameworks to their limits.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/","relevance":"","is_relevant":true},{"title":"Apparently shipping AI platforms is a thing now as per this post from the Qwen X account","id":"t3_1kebauw","overview":"The post references a promotional image from Alibaba's Qwen X account featuring cartoon characters celebrating the 'shipping' of an AI platform. While the image itself lacks explicit technical details, the context implies a significant milestone for Qwen, Alibaba's open-source large language model (LLM) series. 'Shipping' in this context likely refers to either a new model release, broader platform availability (e.g., APIs/SDKs), or deployment infrastructure. The whimsical aesthetic suggests a focus on developer accessibility, aligning with trends in democratizing LLM access. Qwen's open-source nature and Alibaba's growing presence in the AI space make this announcement noteworthy, though specific technical specifications (e.g., model size, performance benchmarks, or architectural innovations) are absent from the post. The announcement reflects a broader industry shift toward making LLMs more approachable and deployable for developers.","comment_overview":"The comments blend humor with casual curiosity, with users joking about 'AI romcoms,' VTubers, and fanfiction competitions. While someË∞É‰æÉ the meme culture around AI announcements, others acknowledge Qwen's open-source contributions. The discussion lacks technical depth, focusing instead on playful speculation about the platform's capabilities and the anthropomorphized characters. A few users question the practical implications of 'shipping' AI platforms, but no concrete criticisms or technical debates emerge. The tone highlights the growing cultural saturation of AI news rather than its technical nuances.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kebauw/apparently_shipping_ai_platforms_is_a_thing_now/","relevance":"","is_relevant":true},{"title":"IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models","id":"t3_1kedu0d","overview":"IBM's release of the Granite 4.0 Tiny Preview marks a significant step in efficient large language model (LLM) design, combining novel architectural innovations with practical performance gains. The model employs a hybrid Mamba-2/Transformer architecture, leveraging Mamba's sequential efficiency for long-context processing while retaining transformers' precision through self-attention mechanisms. This approach enables a 7B-parameter model with only 1B active parameters at inference time via a fine-grained mixture-of-experts (MoE) structure, achieving 72% lower memory usage than the Granite 3.3 2B Instruct model. The NoPE (no positional encoding) technique allows extended context lengths without performance degradation, a critical advancement for tasks requiring temporal reasoning. IBM's decision to open-source the model under Apache 2.0 on Hugging Face emphasizes experimentation over enterprise deployment, with plans for a full release this summer featuring 15T token training (up from 2.5T in the preview). The architecture builds on IBM Research's collaboration with Mamba creators, including experimental work on Bamba, a hybrid model whose v2 version was released recently.","comment_overview":"The community reaction highlights both excitement and practical concerns. Enthusiasm centers on the hybrid architecture's potential to bridge Mamba's efficiency with transformers' accuracy, with users noting its promise for consumer-grade hardware. However, technical hurdles remain: lack of llama.cpp support and GGUF format compatibility limits accessibility for many users. Discussions emphasize the importance of scaling experimental architectures, with hopes IBM will release larger models (e.g., 30B) to test these innovations. Some users question the long-term viability of NoPE without academic validation, while others speculate about IBM's potential to challenge competitors like Alibaba. The preview's toggleable thinking functionality and reasoning-focused post-training also spark interest, though there are calls for more creative applications beyond passkey testing. A recurring theme is the need for broader ecosystem support to realize the model's efficiency gains.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/","relevance":"","is_relevant":true},{"title":"Qwen3 no reasoning vs Qwen2.5: Is reasoning the main improvement?","id":"t3_1kegrce","overview":"This discussion explores whether Qwen3's performance gains over Qwen2.5 stem primarily from its reasoning capabilities or other architectural improvements. Users report mixed results: some note Qwen3's no-reasoning variants outperform Qwen2.5 in coding tasks (e.g., 45.8% code completion accuracy vs. Qwen2.5's 16.4%), while others find Qwen2.5-coder models better at complex one-shot prompts. Technical nuances emerge, such as Qwen3's superior quantization resilience (e.g., functioning at IQ2 format) and better logical consistency in multi-turn edits. However, Qwen2.5 shows stronger naturalness in Japanese-to-English translation and structured story summarization. The debate highlights task-specific trade-offs between reasoning-enabled vs. disabled modes, with no clear consensus on whether reasoning is the dominant factor in Qwen3's improvements.","comment_overview":"The community is split between empirical observations and theoretical speculation. While some users cite benchmark data (e.g., Qwen3-32B no-think surpassing GPT-4.5 in coding), others emphasize practical use cases where Qwen2.5 still excels. Criticisms include inconsistent quantization performance and ambiguous reasoning benefits. A recurring theme is task dependency: Qwen3 shines in logical problem-solving and code porting, while Qwen2.5 maintains edge in linguistic nuance. The discussion underscores the complexity of evaluating LLM improvements without official technical breakdowns.","link":"https://www.reddit.com/r/LocalLLaMA/comments/1kegrce/qwen3_no_reasoning_vs_qwen25/","relevance":"","is_relevant":false}],"persona":"LocalLLaMa"}