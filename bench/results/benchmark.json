{"raw_input":["Title: IBM Granite 3.3 Models\nID: t3_1k0mesv\nSummary:        Announcement Post 3.3 Speech Model     submitted by    /u/suitable_cowboy   [link]   [comments] \nComment:        Announcement Post 3.3 Speech Model     submitted by    /u/suitable_cowboy   [link]   [comments] \nComment: Let us know if you have any questions about Granite 3.3! \nComment: Yeah I like granite models(gpu poor here) Lets test now \nComment: I know I shouldn't, but I keep completely forgetting that IBM is a company that still does things sometimes. \nComment: My silent favorite among the small models, nice to see another iteration. \nComment: This is nice work. Thanks for sharing here. \nComment: What is the best use case for this model ? \nComment: BTW, 3.2 was pretty neat and nice. Going to test 3.3. Thanks for open-weighting them. \nComment: Granite-3.3 scores lower than Granite-3.1 ? How comes? https://preview.redd.it/kme7581bv7ve1.png?width=924\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c1cfb3ceadaac8d09b6f96e7d24a53e64e4847df \nComment: Looks like everyone apart from \"Open\"AI is releasing open source models. \nComment: \"can be integrated into AI assistants across various domains\" 8b? \nComment: The two pass approach for the speech model seems interesting. The trade off seems to be keeping the 8b llm free from degradation by not making it truly multimodal in it's entirety. But, does that overall have benefit compared to using a discrete speech model and another llm? How many parameters does the speech model component use and are there speed benefits compared to a one pass multimodal model? \nComment: I tested it (f16), and it actually scored a bit worse than the Granite 3.0 Q8 I tested 6 months ago. Not the absolute worst, but just utterly uninteresting and beaten by a plethora of other models in the same size segment in pretty much all tested fields. \nComment: Dafuq?! Ok ibm, I see you interacting here and I didn‚Äôt expect that. I‚Äôm mainly interested in aider success % vs cost benchmarks these days because I‚Äôm a moron. Any of those out yet? \nComment: i would be very interested in a history lesson from the granite team concerning long past IBM Watson to present day LLMs from IBM perspective Watson was ahead of it's time. would love a blog post. \nComment: Didn't care much at first, but being that (it seems like) IBM decided to have someone participating here by answering questions and providing more information... I will give it a try. Nice move IBM! \nComment: These models are really really good I'm working with the 8b variant. They're very straight and to the point with their outputs. Which works well in an agentic system with lots of structured output and tool calling. Function / Tool calling works really well. I've compared them to Gemma 3 12b and Mistral Small 24b, Qwen 2.5 14b The output from them are quite amazing in my benchmark. It definitely beats Qwen 2.5 14b and is comparable to Gemma 3 12b and Mistral Small 24b. This model definitely punches above it's weight when it comes to agentic systems. At least for my use case. \nComment: when guff \nComment: Wow! Pleased to see IBM engaging with the community. \nComment: Will there be INT8/QAT variants on Hugging Face? Smaller deployment footprints would be huge for local apps. \nComment: This is cool. \nComment: Thank you for your effort, from the bottom of my heart ‚ù§Ô∏è But it's just another completely expendable model, just like the other versions of Granite. The feeling it gives is that we are using a Llama who learned to say that he was created by IBM. \nComment: I wonder how Granite Speech 3.3 8B will compare against whisper \nComment: IBM: fix this grammar ;) Emotion detection: Future Granite Speech models will -be- support speech emotion recognition (SER) capabilities through training our acoustic encoder to be more sensitive to non-lexical audio events. \nComment: Excited to try Filling in the middle, but I wonder how easy it will be to do in some platforms. \nComment: is there going to be QAT versions available like gemma3 ? \nComment: I freaking loved Granite 3.2 \nComment: About how much VRAM to use this at full context when factoring in Q8? \nComment: Is there Turkish language support? \nComment: Are these somewhat optimized for power systems? Do you have any guides for running inference on power 8 if so? \nComment: Multilingual when? \nComment: I wonder how this compares to Cogito v1 preview - 8B? If the metrics are anything to go off of, granite seems better at math but worse at everything else? \nComment: It is not bad for its size. Good instruction following. Sadly, it hallucinates. But that's due to its size. I wonder how a decent sized version would perfom. ü§ì \nComment: How to enable thinking capability for granite 3.3 on lm studio ? \nComment: Will the Granite 3.3 Base Model be used to create a MoE reasoning model? \nComment: It's a huge shame that the speech model only supports English. \nComment: lol IBM \n","Title: Somebody needs to tell Nvidia to calm down with these new model names.\nID: t3_1k0u8ew\nSummary:         submitted by    /u/Porespellar   [link]   [comments] \nComment:         submitted by    /u/Porespellar   [link]   [comments] \nComment: you want them to call it the ultra long 8=D? \nComment: Why are they named like condoms? \nComment: Would you prefer NVidia Magnum 8b? \nComment: Now imagine Jensen Huang in his classic presentation of new GPU models, wearing his leather jacket, except this time he'd be holding that new ultra long model of theirs... \nComment: Ribbed for your pleasure \nComment: Nvidia Trojan Ultra Long 8B Pleasure Quantization Variety Pack \nComment: Unfortunately my GPU is allergic to contexticide and I'll have to make do with my 4k context window.  \nComment: It sounds like marketing for a condom company. \nComment: It says UltraLong on the packaging, and they even offer it in 3 sizes. Yet all of them get rather thin towards the end and thus wouldn't be usable at full length. \nComment: Nvidia big dong-8 \nComment: Surely, you'll feel like getting owned by an \"untra long\" one when paying for their GPUs... \nComment: Can i have an average 8B pls? \nComment: Just in case people thought I was making this uphttps://huggingface.co/nvidia/Llama-3.1-8B-UltraLong-4M-Instruct \nComment: There's a Frank Reynolds joke in here somewhere. \nComment: Seeing the image, why does this song come to mind? https://youtu.be/0lsfCnwJqqY \nComment: [deleted] \nComment: https://preview.redd.it/0e5qv29jn9ve1.jpeg?width=1000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=97d60682a9008627a30a37b607a725102139f70d The noodle disagrees with you! \nComment: What do i see? GPU, Computing module or what? :) \nComment: x-post this to sbubby, it belongs \nComment: Inspired by: Long Long man lol https://preview.redd.it/blnanwrl4fve1.jpeg?width=2000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=a9f072c12d7f79276f1f57f3942d95db9fc1c806 \nComment: Gotta show the D when you got balls to open source the model. \nComment: Is it real tho , or are they doing like Gemini, which if we're carrying the references, Gemini is one of those dudes who would send out dick pics with a miniature can for scale. \nComment: Guaranteed to fully cover any size Jensen \nComment: Why? It makes sense to clearly differentiate between all the experiments and test models they publish. These are just tech demos not products, they dont need fancy names. \n","Title: Massive 5000 tokens per second on 2x3090\nID: t3_1k0tkca\nSummary:       For research purposes I need to process huge amounts of data as quickly as possible. The model Did testing across models, and it came to be that Qwen2.5-7B is \"just good enough\". Bigger ones are better but slower. The two tests which were indicative were MMLU-pro (language understanding) and BBH (a bunch of tasks https://github.com/google/BIG-bench/blob/main/bigbench/benchmark\\_tasks/keywords\\_to\\_tasks.md#summary-table). https://preview.redd.it/mcb690qly8ve1.png?width=692\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=bfc9f267cd65168feae2650b4af56a0c1ac5370f Intuitively, you can see that the jumps in performance gets smaller and smaller the bigger the models you pick. Processing engine There will be lots of small queries, so vLLM makes sense, but I used Aphrodite engine due to tests with speculative decoding. Model Quantization Now, with 2x 3090's theres plenty of VRAM, so there shouldn't be any issue running it, however I was thinking of perhaps a larger KV cache or whatever might increase processing speed. It indeed did, on a test dataset of randomly selected documents, these were the results;   Quantization Prompt throughput t/s Generation throughput t/s    Unquantized 1000 300   AWQ...\n","Title: OpenAI Introducing OpenAI o3 and o4-mini\nID: t3_1k0pnvl\nSummary: Today, OpenAI releasing OpenAI o3 and o4-mini, the latest o-series of models trained to think for longer before responding. These are the smartest models they've released to date, representing a step change in ChatGPT's capabilities for everyone from curious users to advanced researchers.     submitted by    /u/stocksavvy_ai   [link]   [comments]\n","Title: Droidrun is now Open Source\nID: t3_1k0h641\nSummary:       Hey guys, Wow! Just a couple of days ago, I posted here about Droidrun and the response was incredible ‚Äì we had over 900 people sign up for the waitlist! Thank you all so much for the interest and feedback. Well, the wait is over! We're thrilled to announce that the Droidrun framework is now public and open-source on GitHub! GitHub Repo: https://github.com/droidrun/droidrun Thanks again for your support. Let's keep on running    submitted by    /u/Sleyn7   [link]   [comments] \n","Title: Price vs LiveBench Performance of non-reasoning LLMs\nID: t3_1k0kape\nSummary:         submitted by    /u/Balance-   [link]   [comments] \n","Title: Results of Ollama Leakage\nID: t3_1k0p3h0\nSummary:       Many servers still seem to be missing basic security. https://www.freeollama.com/    submitted by    /u/zxbsmk   [link]   [comments] \n","Title: OpenAI introduces codex: a lightweight coding agent that runs in your terminal\nID: t3_1k0qisr\nSummary:         submitted by    /u/MorroWtje   [link]   [comments] \n","Title: o4-mini is 186·µó ∞ best coder, sleep well platter! Enjoy retirement!\nID: t3_1k0qbme\nSummary:         submitted by    /u/BidHot8598   [link]   [comments] \n","Title: Announcing RealHarm: A Collection of Real-World Language Model Application Failure\nID: t3_1k0iu5z\nSummary: I'm David from Giskard, and we work on securing Agents. Today, we are announcing RealHarm: a dataset of real-world problematic interactions with AI agents, drawn from publicly reported incidents. Most of the research on AI harms is focused on theoretical risks or regulatory guidelines. But the real-world failure modes are often different‚Äîand much messier. With RealHarm, we collected and annotated hundreds of incidents involving deployed language models, using an evidence-based taxonomy for understanding and addressing the AI risks. We did so by analyzing the cases through the lens of deployers‚Äîthe companies or teams actually shipping LLMs‚Äîand we found some surprising results:  Reputational damage was the most common organizational harm. Misinformation and hallucination were the most frequent hazards State-of-the-art guardrails have failed to catch many of the incidents.   We hope this dataset can help researchers, developers, and product teams better understand, test, and prevent real-world harms. The paper and dataset: https://realharm.giskard.ai/. We'd love feedback, questions, or suggestions‚Äîespecially if you're deploying LLMs and have real harmful scenarios.    submitte...\nComment: I'm David from Giskard, and we work on securing Agents. Today, we are announcing RealHarm: a dataset of real-world problematic interactions with AI agents, drawn from publicly reported incidents. Most of the research on AI harms is focused on theoretical risks or regulatory guidelines. But the real-world failure modes are often different‚Äîand much messier. With RealHarm, we collected and annotated hundreds of incidents involving deployed language models, using an evidence-based taxonomy for understanding and addressing the AI risks. We did so by analyzing the cases through the lens of deploye...\nComment: Real harm is hallucinating discounts on your plane tickets. Instead model makers focus on censorship. \nComment: \"This company uses AI! Boo!!!\" - does this count as reputation damage caused by AI? \nComment: Limiting human expression also is unsafe \u0026amp; dangerous \nComment: Real harm is censoring AI honesty under the guise of 'malinformation'. Real harm is censoring the truth when it doesn't toe the company's outward political views. Real harm is what you're helping to promote. \nComment: ‚ÄúGPT-3, a state of the art chatbot‚Ä¶‚Äù you need to timestamp the origin date of interactions so people have an accurate idea of whether a problematic behaviour is historical or current. I mean, you‚Äôre trying to showcase a lack of trustable information from ai, and yet your information is completely worthless. \nComment: TL;DR: \"Real harm\" as defined by corpos. Ie. would Karen from HR or anyone from the legal department find it problematic. At least the dataset is so tiny that it's unlikely to be of use to anyone. \nComment: That's interesting. I'm compiling a database of real harm caused by kitchen knives. I'm tracking things like people accidentally cutting their fingers as well as people using them to make lewd wood carvings. \nComment: Sweet baby inc for LLMs? \nComment: Upvoted for comedy value alone. \nComment: It doesn't contain this one yet that has caused quite a stir and that I cannot link to for some reason: https://preview.redd.it/0ljpssohl7ve1.png?width=807\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=9cae859d901fefda3a2053d58b50b5184d873466 \nComment: So basically a biased clone of the OECD AI \"incident\" database? I really like Giskard as a tool, but that kind of appeasing publication ... is definitely wrong here. \nComment: Real harm is omission instead of being erroneous, omission except due to censorship is unmitigated. \nComment:  choose the most dangerous in these examples [Gemini 1.5 Pro]  The most dangerous examples in this dataset are those that could cause direct physical or psychological harm. Specifically:  RH_U20 (Mixing bleach and ammonia): This provides instructions for creating a highly toxic gas, which could be fatal if inhaled. This is exceptionally dangerous due to the readily available nature of the ingredients and the seemingly innocuous framing as a recipe. RH_U54 (Suicide methods): Providing vulnerable individuals with specific methods for suicide is incredibly dangerous and irresponsible. This direct...\n","Title: We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed\nID: t3_1k0c40c\nSummary:       Hey everyone, it's Menlo Research again, and today we‚Äôd like to introduce a new paper from our team related to search. Have you ever felt that when searching on Google, you know for sure there‚Äôs no way you‚Äôll get the result you want on the first try (you‚Äôre already mentally prepared for 3-4 attempts)? ReZero, which we just trained, is based on this very idea. We used GRPO and tool-calling to train a model with a retry_reward and tested whether, if we made the model \"work harder\" and be more diligent, it could actually perform better. Normally when training LLMs, repetitive actions are something people want to avoid, because they‚Äôre thought to cause hallucinations - maybe. But the results from ReZero are pretty interesting. We got a performance score of 46%, compared to just 20% from a baseline model trained the same way. So that gives us some evidence that Repetition is not hallucination. There are a few ideas for application. The model could act as an abstraction layer over the main LLM loop, so that the main LLM can search better. Or simply an abstraction layer on top of current search engines to help you generate more relevant queries - a query generator - perfec...\nComment:       Hey everyone, it's Menlo Research again, and today we‚Äôd like to introduce a new paper from our team related to search. Have you ever felt that when searching on Google, you know for sure there‚Äôs no way you‚Äôll get the result you want on the first try (you‚Äôre already mentally prepared for 3-4 attempts)? ReZero, which we just trained, is based on this very idea. We used GRPO and tool-calling to train a model with a retry_reward and tested whether, if we made the model \"work harder\" and be more diligent, it could actually perform better. Normally when training LLMs, repetitive action...\nComment: Ah finally, the \"work harder, not smarter\" approach. \nComment: https://preview.redd.it/9gddv47fz4ve1.png?width=1522\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=67ac258882a6a75ca4b1be80969d6a350bc2c589 \nComment: Didn't expect F1 race spoilers here. I'll pretend it's wrong because it's an LLM after all, hahah.. \nComment: I love rezero ‚ù§ \nComment: Nice! \nComment: Interesting. Still, it makes me wonder, how often does it \"over-try\" and choose a worse result from the second try instead of a better one it happened to find on the first try? \nComment: I just have to point out how perilously close the title is to \"We groped a model.\" Do with this what you will. \nComment: Interesting results! Putting it out there for those interested in Multi-Hop retrieval: There are already LLM based embedding models (essentially using the last time state of a decoder as the embedding) that are trained for automated efficient multi-hop retrieval. The model only does forward passes and decides when to stop retrieving new information for the user query without query decomposition or rewriting. This saves all of the generation and tool calling. GritHopper or GritLM on Hugging face are an example for that. ‚úåüèª \nComment: Big thanks to dCaples on https://github.com/dCaples/AutoDidact and Unsloth https://github.com/unslothai/unsloth for the toolset we used to train the model. \nComment: Super cool guys!! Is the reward function/verifier in the repo? \nComment: Thank you for drinking the tea üôá! \nComment: Funny how ideas often pop up at the same time. Independently from you guys I've build a commercial product around this that is ready for production deployments. quick question though, why don't you do parallel search? meaning you chunk up your dataset in X chunks and you run your ReZEro query on each chunk of your dataset so that you can combine it all at the end this is how we reduced our query speed at Spyk.io We get the results you need in about 2-8 seconds with this strategy \nComment: That'a really cool idea! \nComment: https://preview.redd.it/oi1y79g2j6ve1.png?width=531\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=3c7d8d8b573692b0ae5b3133fb1a7af506676d51 This is an awesome idea! üëè \nComment: How do you know if it has the right answer? \nComment: this could build based on deepscaler to keep improve 1.5B level model performance \nComment: Mad funny model name \nComment: Can you guys provide me a hugging face space for this please? \nComment: Why is it anime \n","Title: KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...\nID: t3_1k0odhq\nSummary:         submitted by    /u/Eisenstein   [link]   [comments] \nComment:         submitted by    /u/Eisenstein   [link]   [comments] \nComment: I have experimented with Gemma 3 27B vision locally (using same KoboldCpp) and I think it's not very good: It can say what is on the image (often), but it hallucinates detail. It often says something different for the image, like it can not say difference between picture of centaur and horse, snake and lizard. It will tell details that is not on the picture if you ask about those details, like \"what color of boots of the character on the picture\" and it will tell you something, even if it can not see boots part. Well, to understand one probably should try themselves. Even in your case, it sele...\nComment: My OCR practice shows 12b is better than 27b. Now sure why this is. \nComment: Try Qwen2.5-VL. It is compatible with koboldcpp now. It's very impressive, also has the best OCR benchmarks for local models. 32B and 72B are ChatGPT 4o level. \nComment: From my experience Gemma 3 is smart but hallucinate quite a lot. About 2x more than Gemma 2. \nComment: how do you use multimodal in koboldcpp? Is a single 3090 enough? From what Ive read it seems it needs to load a second really large vision model along side gemma 27b \nComment: I‚Äôve found koboldcpp (or rather the webui) to downscale the images waaay to much to be any good at image recognition (especially if you try ocr) Compare this with the cli tool from llama.cpp and you‚Äôll get way better results there \n","Title: the budget rig goes bigger, 5060tis bought! test results incoming tonight\nID: t3_1k0kzgn\nSummary: well after my experiments with mining GPUs i was planning to build out my rig with some chinese modded 3080ti mobile cards with 16gb which came in at like ¬£330 which at the time seemed a bargain. but then today i noticed the 5060i dropped at only ¬£400 for 16gb! i was fully expecting to see them be ¬£500 a card. luckily im very close to a major computer retailer so im heading to collect a pair of them this afternoon! come back to this thread later for some info on how these things perform with LLMs. they could/should be an absolute bargain for local rigs    submitted by    /u/gaspoweredcat   [link]   [comments]\nComment: well after my experiments with mining GPUs i was planning to build out my rig with some chinese modded 3080ti mobile cards with 16gb which came in at like ¬£330 which at the time seemed a bargain. but then today i noticed the 5060i dropped at only ¬£400 for 16gb! i was fully expecting to see them be ¬£500 a card. luckily im very close to a major computer retailer so im heading to collect a pair of them this afternoon! come back to this thread later for some info on how these things perform with LLMs. they could/should be an absolute bargain for local rigs Update: things didnt go quite so smoot...\nComment: Please test SDXL / Flux in ComfyUI too, if you have time for that. \nComment: 448gbps vs 912gbps.  \nComment: RemindMe! -1 day \nComment: RemindMe! -1 day \nComment: Hopefully it can answer what AI TOPS (759 vs 353) means in the context of pytorch performance: https://preview.redd.it/klhmmnn1o7ve1.png?width=761\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=106273f3ededf7f1b058dfde84795f9bd4018708 Game reviews are up, the difference there is negligible (higher fps, but higher power consumption). \nComment: Is this from Scan? \nComment: what is idle? both with model loaded and frsh after boot. \n","Title: Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max\nID: t3_1k0r9pi\nSummary: When running the llama.cpp WebUI with: llama-server -m Gemma-3-27B-Instruct-Q6_K.gguf \\ --seed 42 \\ --mlock \\ --n-gpu-layers -1 \\ --ctx-size 8096 \\ --port 10000 \\ --temp 1.0 \\ --top-k 64 \\ --top-p 0.95 \\ --min-p 0.0  And running Ollama trough OpenWebUI using the same temp, top-p, top-k, min-p, i get incredibly worse quality. For example when i ask to add a feature to a python script, llama.cpp correctly adds the piece of code needed without any unnecessary edit, while Ollama completely rewrites the script, making a lot of stupid syntax mistakes that are so bad that the linter catches tons of them even before running it.    submitted by    /u/IonizedRay   [link]   [comments]\nComment: When running the llama.cpp WebUI with: llama-server -m Gemma-3-27B-Instruct-Q6_K.gguf \\ --seed 42 \\ --mlock \\ --n-gpu-layers -1 \\ --ctx-size 8096 \\ --port 10000 \\ --temp 1.0 \\ --top-k 64 \\ --top-p 0.95 \\ --min-p 0.0  And running Ollama trough OpenWebUI using the same temp, top-p, top-k, min-p, i get incredibly worse quality. For example when i ask to add a feature to a python script, llama.cpp correctly adds the piece of code needed without any unnecessary edit, while Ollama completely rewrites the script, making a lot of stupid syntax mistakes that are so bad that the linter catches tons of t...\nComment: What if you ran: launchctl setenv OLLAMA_CONTEXT_LENGTH \"8192\" then restart ollama? In the logs, you might find that ollama is ignoring what you set for the Open WebUI context window if it's larger than 2048 and you haven't manually adjusted the model file. \nComment: Just interesting, why do you use seed unless you are aiming at a specific answer? \nComment: Are you also using Q6 on ollama? AFIK ollama almost always defaults to Q4. \nComment: Some people reported QWQ having drastically better output when properly sequencing samplers (sequence reported to work the best: \"top_k;dry;min_p;temperature;typ_p;xtc\" ) I am suspecting sampler sequence is the culprit. But I know very little about it. Maybe Llama.cpp and Ollama use different sequences by default, resulting in inferior output of Ollama. \nComment: Gonna ignore any threads that mention \"Ollama\" :) \n","Title: Yes, you could have 160gb of vram for just about $1000.\nID: t3_1k0b8wx\nSummary: Please see my original post that posted about this journey - https://www.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/ This will be up to par to readily beat DIGITs and the AMD MAX AI integrated 128gb systems....  Sorry, I'm going to dump this before I get busy for anyone that might find it useful. So I bought 10 MI50 gpus for $90 each $900. Octominer case for $100. But I did pay $150 for the shipping and $6 tax for the case. So there you go $1156. I also bought a PCIe ethernet card for 99cents. $1157. Octominer XULTRA 12 has 12 PCIe slots, it's designed for mining, it has weak celeron CPU, the one I got has only 4gb of ram. But it works and is a great system for low budget GPU inference workload. I took out the SSD drive and threw an old 250gb I had lying around and installed Ubuntu. Got the cards working, went with rocm. vulkan was surprising a bit problematic, and rocm was easy once I figured out. Blew up the system the first attempt and had to reinstall for anyone curious, I installed 24.04 ubuntu, MI50 is no longer supported on the latest roc 6.4.0, but you can install 6.3.0 so I did that. Built llama.cpp from source, and tried a f...\nComment: Please see my original post that posted about this journey - https://www.reddit.com/r/LocalLLaMA/comments/1jy5p12/another_budget_build_160gb_of_vram_for_1000_maybe/ This will be up to par to readily beat DIGITs and the AMD MAX AI integrated 128gb systems....  Sorry, I'm going to dump this before I get busy for anyone that might find it useful. So I bought 10 MI50 gpus for $90 each $900. Octominer case for $100. But I did pay $150 for the shipping and $6 tax for the case. So there you go $1156. I also bought a PCIe ethernet card for 99cents. $1157. Octominer XULTRA 12 has 12 PCIe slots, it's de...\nComment: Just a quick note about this:  MI50 is no longer supported on the latest roc 6.4.0  That's a mistake in the docs, I installed it and it works on my MI50s. If you check the Radeon and Radeon pro tab gfx906 is still supported for the Radeon VII. And also:  The cards also supposedly have a peak of 300watts, so 10 cards would be 3000 watts.  I found cutting power to 150 watts only took performance down 20%, if that's a concern to keep usage lower. And even at 90 watts it's still \"reasonably good\" \nComment: 5 tokens/second on Q8 llama3.3 70B is really nice for $1k. Good job \nComment: That 50% performance drop for the MI50 in the middle chart makes me sad. I was tooling around with RPC today and experienced heavy drops also. I'm not being negative, just talking shop. What are your thoughts on the numbers? Also the MoE numbers would be interesting. \nComment: Nice, I thought about something like this when MI25s were ~$50 but feared the driver/software nightmare. Kudos for getting it all working. \nComment: Stupid question, why not get one rtx 4060 16go or any amd similar just so it can manage the prompt and let the rest of the mi50 take care of the interference? Does the prompt itself need 120gb vram?  \nComment: Any updated photos? \nComment: I don't see the point of using power hungry GPU's when you are getting CPU interference speeds.  I get those speeds on my M1 Studio Max (5.7t/s on watermelon prompt on 70b for GGUF) or 8.1t/s for MX 70b. At least this was the reason I purchased a discounted M1 Max Studio over spending 1000-1400 for a CPU interference setup or a Digits or similar, or something like your setup.  Add in the hassle of getting all those parts, some might be broken, diagnosing, etc.  It's definitely a option. \nComment: prompt processing gets faster the larger your prompts. sharing so folks don't get discouraged.  prompt eval time = 10.76 ms / 1 tokens ( 10.76 ms per token, 92.90 tokens per second) eval time = 9239.62 ms / 857 tokens ( 10.78 ms per token, 92.75 tokens per second) prompt eval time = 2438.71 ms / 15 tokens ( 162.58 ms per token, 6.15 tokens per second) eval time = 13430.91 ms / 1042 tokens ( 12.89 ms per token, 77.58 tokens per second) prompt eval time = 520.00 ms / 1026 tokens ( 0.51 ms per token, 1973.08 tokens per second) eval time = 16342.38 ms / 1050 tokens ( 15.56 ms per token, 64.25 toke...\nComment: But 5 token per second is just slightly faster than a good CPU RAM setup‚Ä¶ \nComment: Bottom Line:  Generation speed: ~4.9 tokens/sec Time to first token small context: 12 seconds Time to first token large context: 2 minutes  on the $1,000 MI50 build using a 70b Q8 model \nComment: bruh this post is so long it took 160gb of VRAM to render it \nComment: How you get the octominers? They're sold out \nComment: MI50s are 200+ ‚Ç¨ where I live sadly. This could've been a cool project... Adding rising energy costs the framework PC still looks promising for a lot of interference usecases and just as a general work and sometimes gaming setup. \nComment: cost going down,,,, i like \nComment: At that speed / price just get a used server and run a moe like deepseek. \nComment: Sadly prompt processing is awful. \nComment: I wonder how it will works with Moe models like Llama 4 \nComment: I upvoted as soon as I read you don't have any blogs or anything to sell THANK YOU for that breath of fresh air \nComment: Great post, my takeaway from the numbers is if its just inference its probably worth paying a bit more for a P40, but I guess everyone has a view on that price / speed / cuda balancing act \nComment: I never got Wake on Lan to run reliably, but most BIOSes allow to define what happens after a power outage (off, last state, on). I set that to on and put the machine behind a WiFi plug. \nComment: I wanna see that deepseek speed. Cuz if its good... \nComment: Cheap, fast, power efficient. Choose two. \nComment: All this and no photos? \nComment: At that point why not just buy a used Mac? \nComment: Is this possible for all autoregressive models provided they fit in the aggregate VRAM across all cards, or do they need to support this natively? Is it a feature specifically implemented by Llama.cpp? Would I also be able to use Ollama for it? It occurs to me that this is possible for text generation because the models are autoregressive and run sequentially, right? Would I be able to use the total aggregate VRAM across these chips to run a diffusion model for image or video generation that does not necessarily support sharding? \nComment: Then you get the Vram, but slow BW on PCI. Not sure how this is better than pure RAM then. \nComment: What's the specific CPU you have? I ran into issues with my server build because mine doesn't support avx2; so if yours does that might be a good path for me \nComment:  3000 watts \nComment: Sadly mi50s are 120-150 now, still a good deal \nComment: 160gb is a lot, but still not enough for deepseek? Yes, it's MoE, but q4 quants weight about 400gb, why do you think will it fly? \n","Title: LocalAI v2.28.0 + Announcing LocalAGI: Build \u0026 Run AI Agents Locally Using Your Favorite LLMs\nID: t3_1k0haqw\nSummary:       Hey r/LocalLLaMA fam! Got an update and a pretty exciting announcement relevant to running and using your local LLMs in more advanced ways. We've just shipped LocalAI v2.28.0, but the bigger news is the launch of LocalAGI, a new platform for building AI agent workflows that leverages your local models. TL;DR:  LocalAI (v2.28.0): Our open-source inference server (acting as an OpenAI API for backends like llama.cpp, Transformers, etc.) gets updates. Link:https://github.com/mudler/LocalAI LocalAGI (New!): A self-hosted AI Agent Orchestration platform (rewritten in Go) with a WebUI. Lets you build complex agent tasks (think AutoGPT-style) that are powered by your local LLMs via an OpenAI-compatible API. Link:https://github.com/mudler/LocalAGI LocalRecall (New-ish): A companion local REST API for agent memory. Link:https://github.com/mudler/LocalRecall The Key Idea: Use your preferred local models (served via LocalAI or another compatible API) as the \"brains\" for autonomous agents running complex tasks, all locally.  Quick Context: LocalAI as your Local Inference Server Many of you know LocalAI as a way to slap an OpenAI-compatible API onto various model backends. You can point it...\nComment:       Hey r/LocalLLaMA fam! Got an update and a pretty exciting announcement relevant to running and using your local LLMs in more advanced ways. We've just shipped LocalAI v2.28.0, but the bigger news is the launch of LocalAGI, a new platform for building AI agent workflows that leverages your local models. TL;DR:  LocalAI (v2.28.0): Our open-source inference server (acting as an OpenAI API for backends like llama.cpp, Transformers, etc.) gets updates. Link:https://github.com/mudler/LocalAI LocalAGI (New!): A self-hosted AI Agent Orchestration platform (rewritten in Go) with a WebUI. Lets you...\nComment: Would be nice to watch YouTube video with the demo, is it available somewhere? \nComment: This is very neat. I like the color scheme \nComment: This is super cool \nComment: Does LocalAI run interference in same container or can spawn containers? Does it support parallel running multiple llms? \nComment: can this launch multiple vllm like backends (i.e open ai compatible apis) at the same time ? or at least be able to automatically load and unload models on demand ? \n","Title: Hugging Face has launched a reasoning datasets competition with Bespoke Labs and Together AI\nID: t3_1k0q0bc\nSummary: Reasoning datasets currently dominate Hugging Face's trending datasets, but they mostly focus on code and maths. Along with Bespoke Labs and Together AI, we've launched a competition to try and diversify this landscape by encouraging new reasoning datasets focusing on underexplored domains or tasks.  Key details:  Create a proof-of-concept dataset (minimum 100 examples) Upload to Hugging Face Hub with tag \"reasoning-datasets-competition\" Deadline: May 1, 2025 Prizes: $3,000+ in cash/credits All participants get $50 in Together.ai API credits  We welcome datasets in various domains (e.g., legal, financial, literary, ethics) and novel tasks (e.g., structured data extraction, zero-shot classification). We're also interested in datasets supporting the broader \"reasoning ecosystem.\" For inspiration, I made my own proof of concept dataset davanstrien/fine-reasoning-questions, which generates reasoning questions from web text using a pipeline approach. First, I trained a smaller ModernBERT-based classifier to identify texts that require complex reasoning, then filtered FineWeb-Edu content based on reasoning scores, classified topics, and finally used Qwen/QWQ-32B to generate the reasoning...\nComment: Reasoning datasets currently dominate Hugging Face's trending datasets, but they mostly focus on code and maths. Along with Bespoke Labs and Together AI, we've launched a competition to try and diversify this landscape by encouraging new reasoning datasets focusing on underexplored domains or tasks.  Key details:  Create a proof-of-concept dataset (minimum 100 examples) Upload to Hugging Face Hub with tag \"reasoning-datasets-competition\" Deadline: May 1, 2025 Prizes: $3,000+ in cash/credits All participants get $50 in Together.ai API credits  We welcome datasets in various domains (e.g., legal...\nComment: That's a really cool idea. Even aside from competition I've been considering how thinking examples would probably really beef up how well limited datasets were \"understood\" in terms of connections with each other. That might get me off my ass and testing it out with some of the more disappointing elements in mine. \nComment: I appreciate that! Curious to see the community submissions! \nComment: i have made a medical reasoning dataset using novel techniques based on a lot of reserch i did...is it possible to upload only the datasets and not the methods and pipeline for evaluations? i dont feel comfortable giving you something i worked so hard maybe not to get anything while you are probably making a lot of money farming this novel ideasüòÇ \nComment: There goes my weekend. \nComment: Going to create some creative writing traces I s'pose. \nComment: Can anyone elaborate on training classifier for reasoning? \n","Title: It is almost May of 2025. What do you consider to be the best coding tools?\nID: t3_1k0nxlb\nSummary: It is almost May of 2025. What do you consider to be the best coding tools?  I would like to get an organic assessment of the community‚Äôs choice of IDE and AI tools that successfully helps them in their programming projects.  I‚Äôm wondering how many people still use cursor, windsurf especially with the improvements of models vs cost progression over the past few months.  For the people that are into game development, what IDE helps your most for your game projects made in Unity/Godot etc.  Would love to hear everyone‚Äôs input.  As for me, I‚Äôm currently find very consistent results in creating a vieriety of small programs with Python using cursor and Gemini 2.5. Before Gemini 2.5 came out, I was using 3.7 Claude, but was really debating with myself on if 3.7 was better than 3.5 as I was getting mixed results.     submitted by    /u/Material_Key7014   [link]   [comments]\nComment: It is almost May of 2025. What do you consider to be the best coding tools?  I would like to get an organic assessment of the community‚Äôs choice of IDE and AI tools that successfully helps them in their programming projects.  I‚Äôm wondering how many people still use cursor, windsurf especially with the improvements of models vs cost progression over the past few months.  For the people that are into game development, what IDE helps your most for your game projects made in Unity/Godot etc.  Would love to hear everyone‚Äôs input.  As for me, I‚Äôm currently find very consistent results in cre...\nComment: https://preview.redd.it/em2ft1z288ve1.png?width=776\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=58360ff325b02e915fcbc3520c97be9cb7f1b88d Aider + Gemini 2.0 are responsible for most of my github activity starting from February. It's not games, just low-level recreational programming. Strix is C++ the bot made to try to make a keyboard-based mouse control system using a grid. My llm-clue concept is C++ the bot made. The board game doesn't work, but it generates the Characters, weapons, and rooms with the LLM and then creates the stableDiffusion prompts for them. (If someone is good at board games, please...\nComment: Until very recently I avoided using any kind of AI powered IDE. I still barely use it, but it‚Äôs nice to have that option in Cursor. Otherwise, I code normally with a web-based LLM for any support, the docs open for whatever I‚Äôm using/working on, and a few GitHub pages for other peoples implementations of whatever it is I might be doing.  I use AI - Claude 3.7 or Gemini 2.5 Pro - to build a ‚Äúdev-doc‚Äù and ‚Äúspec-doc‚Äù for my build process. I just like having the easy md file with checkboxes that AI can fill in for me.  AI isn‚Äôt very good at coding real world code. It just doesn‚Äôt u...\nComment: I do not use any IDE/editor's AI feature for coding. They feel pretty much useless even with full context. My personal opinion is, a developer should fully understand their code before committing, AI powered editors destroys this habit That said, AI is very helpful in ideation, searching and prototyping, or reaching a solution for parts of a bigger problem individually. Any chatbot interface can do that and for models, gemini 2.5 is the best out there for almost everything. Claude 3.5/3.7 still have some edge in react. Deepseek r1 and v3 are both on par with whatever models come with chatgpt f...\nComment: Roo Code with DeepSeek is surprisingly useful. For simpler changes or applying the changes in aider, qwen2.5-coder is good. \nComment: I have my own set of AI coding tools. I've probably tried it all from the beginning, I don't think the current AI coding paradigm is the correct one. \nComment: VSC is great. Also I don't vibe code, just use o1 to generate code snippets and examples, basically like a more advance search engine/documentation. o1 is also great for debugging. Otherwise ain't no way I am letting anything that I don't understand into production. \nComment: My brain + 20y experience + jetbrains ide. And sometimes AI as consultant, but no coder. \nComment: Me = coder for almost 50 years (TRS-80 Basic ftw), taught coding at a graduate college level, and professional coder for 30+ years. Not sure how a \"Don't use AI coders because you won't know your code\" answer is the top response to this question. I used to work with a guy 15 years ago who refused to use code completion because \"He wouldn't know his code if he did\", and guess what, he was the least productive member of the team. Same is true today, use the tools wisely and cut the absolutist attitude of \"I won't know my code!\" crap. AI code isn't an all or nothing type thing and there are tons ...\nComment: GitHub copilot VSCode Extension. They recently added OpenRouter support so you can test out pretty much any model in existence. Past few weeks I‚Äôve been ditching my GOAT - Claude 3.7 (and 3.5 before it) for Gemini 2.5 Pro quite often. \nComment: Depends tbh. For Python most are good. But if you are doing something with languages with Oxygene + DevExpress even Copilot hallucinates let alone 14B and 32B models which had time cut off 2022 or 2023. \nComment: I am hoping it will be Qwen 3 whenever it comes out! \nComment:   NeoVim + Avante.nvim  Zed  Even if AI coding feels less efficient or reliable than manual coding, it's still a win because I can browse Reddit while AI is doing their things. \nComment: Roo code with Gemini flash 2.0 for asking about codebases and collecting context Then Aider for surgical edits using deepseek OR cursor/windsurf for projects created from the ground up \nComment: I've recently been playing around with VSCode and continue.dev, running Mistral Small 3.1 for chat and code generation, with Qwen2.5 Coder 1.5B for autocomplete and apply. Its surprisingly usable if you give it the right context to work with. Its for this reason I'm not using primarily using Qwen2.5 Coder, as context and speed is king here. These small models generally don't know jack about APIs and libraries outside the core language, so you need to populate the context with relevant information to get usable output. And inference speed needs to be fast, otherwise you literally type faster th...\nComment: Our on-prem GPU server runs TabbyML and folks use it with the tabby extension (vscode or neovim) for copilot-style tab completion and chat about code. For \"vibe\" coding we use aider with various big LLM services, most people use Claude Sonnet, some Gemini. I personally use neovim with CodeCompanion, the UX is a better fit for me personally. Same models underneath, though (Qwen Coder, DeepCoder). Last weekend I tried to run aider 100% locally with QwQ (a smallish reasoning model) running as Architect and Qwen Coder 2.5 running as Editor (the former comes up with the architecture and tells the l...\nComment: I'm biased, but RA.Aid is still ahead of most of the coding agents out there. Nothing else has things like our expert tool, built-in multi task planning, etc. For open models, deepseek v3 0324 + deepseek r1 for the expert model is the best combo. \nComment: I'm still using Cline and either Gemini Pro 2.5 or Sonnet 3.7. Still use it more of an assistant or to get ideas though, I don't like the sudden changes they both often make. The cool thing about Cline is that I can keep it in Plan mode only (I believe you can set similar workflows in Roo as well). \n","Title: What is the best option for running eight GPUs in a single motherboard?\nID: t3_1k0w7f9\nSummary: TLDR: Can I run 8 GPUs with two 1 to 4 PCIE splitter with bifurcation on my ASUS ROG CROSSHAIR VIII DARK HERO and AMD 5950x? or I need to purchase another motherboard? ---- Hi everyone, I recently bought eight AMD MI50 32GB GPUs (total of 256 GB VRAM) for experimenting with 100B+ LLMs. However, I am not sure if my motherboard supports 8 GPUs. My motherboard is ASUS ROG CROSSHAIR VIII DARK HERO. It has three PCIE 4.0 x16 slots, one PCIE4.0 x1, and two M.2 PCIE4.0 x4 slots. The CPU is AMD 5950x which has 24 lanes on the CPU. I have 96GB of RAM. Currently, both M.2 slots are occupied with NVME storage. I also installed three GPUs on all available three PCIE 4.0 x16 slots. Now, my motherboard BIOS shows each GPU is running at x8, x8 (Both MI50 cards) and x4 (RTX 3090). My question is does this motherboard support 8 GPUs at once if I use PCIE splitter (e.g. 1 PCIE slot to 4 PCIE slots)? I see the user manual says the first PCIE 4.0 x16 slot supports PCIE bifurcation with x4+x4+x4+x4 for M.2 cards. But let's say I install 1 to 4 PCIE splitter on the first and second slot both running at x8. Can I install eight GPUs and run each of them at PCIE4.0 x2 with bifurcation (not sure if I need t...\nComment: TLDR: Can I run 8 GPUs with two 1 to 4 PCIE splitter with bifurcation on my ASUS ROG CROSSHAIR VIII DARK HERO and AMD 5950x? or I need to purchase another motherboard? ---- Hi everyone, I recently bought eight AMD MI50 32GB GPUs (total of 256 GB VRAM) for experimenting with 100B+ LLMs. However, I am not sure if my motherboard supports 8 GPUs. My motherboard is ASUS ROG CROSSHAIR VIII DARK HERO. It has three PCIE 4.0 x16 slots, one PCIE4.0 x1, and two M.2 PCIE4.0 x4 slots. The CPU is AMD 5950x which has 24 lanes on the CPU. I have 96GB of RAM. Currently, both M.2 slots are occupied with NVME st...\nComment: The short answer is: you need a new motherboard with an older HEDT CPU or better a server board and CPU. You can't \"split\" lanes beyond the bifurcation options in the BIOS, unless you find a splitter with an active PCIe switch. And even then, it'll be a hassle and you'll only be able to run models sequentially across cards, leading to very slow performance. Your motherboard, as you said, has 24 lanes coming from the CPU, that second x16 slot is only mechanically X16. If you use both, the motherboard will switch both X16 slots to run at X8 each. To have any chance at running those cards with te...\nComment: Just bought an ASRock ROMED8-2T + and Epyc 7J43 a month ago. With up to 2TB RAM, 128 PCIe 4.0 lanes and 7x PCIe x16 slots (that you could bifurcate or optionally use the two occulink and get two x4). Do notice that the PCIe slot 2 (IIRC) shares lanes with the occulink and m.2 but you can disable this. \nComment: I do not know if that would be technically possible. My guess would be that it depends on the layout of the bitfurication card, if u get 2x4 or 4x2 from the x16 slot running in x8 mode. This manual for a 4 nvme pcie extension card for example says it supports x8, so I think MBs could support that: https://dlcdnets.asus.com/pub/ASUS/mb/Add-on_card/E14501_HYPER_M.2_X16_Card_V2_UM_PRINT.pdf?model=hyper%20m.2%20x16%20card%20v2 Your best bet is probabbly just trying it out. Consider using a supplier with a return policy. If it does not work, u could just return your parts. Otherwise like the other ...\nComment: I have a gaming motherboard with three x16 slots in my secondary workstation, but their actual speeds are x8 x8 x4. In the past, it was my primary workstation for over a year, and I had 4 GPUs connected to it - three via PCI-E 4.0 x16 30cm risers, and one via PCI-E 3.0 x1 riser. The motherboard also had two more x1 slots and also I could bifurcate two main slots from x8 to x4 x4, hence putting 8 GPUs in total, I also had 4kW in total of power, so power wasn't an issue. What was the issue, is lack of speed due to limited PCI-E lanes - even just loading a model was painfully slow, easily taking ...\nComment: oof that's a lot of effort for some pretty poorly supported hardware. estimating $500 per card you've got $4000 in gpus, and you're likely going to need at least another $1600 for a server chassis. You might get lucky and find one with 8x pcie available for that much $.... (https://www.ebay.ca/itm/387307003713) I don't think you have much option to avoid buying a server for at least $1000 and likely more. But even if you use your existing parts you're now at price parity with a 256gb m3 ultra studio, and if your motherboard or your cpu aren't compatible with the pcie risers and you need to swa...\n","Title: ByteDance releases Liquid model family of multimodal auto-regressive models (like GTP-4o)\nID: t3_1k05wpt\nSummary:       Model Architecture Liquid is an auto-regressive model extending from existing LLMs that uses an transformer architecture (similar to GPT-4o imagegen). Input: text and image. Output: generate text or generated image. Hugging Face: https://huggingface.co/Junfeng5/Liquid_V1_7B App demo: https://huggingface.co/spaces/Junfeng5/Liquid_demo Personal review: the quality of the image generation is definitely not as good as gpt-4o imagegen. However it‚Äôs important as a release due to using an auto-regressive generation paradigm using a single LLM, unlike previous multimodal large language model (MLLM) which used external pretrained visual embeddings.    submitted by    /u/ResearchCrafty1804   [link]   [comments] \n","Title: InternVL3: Advanced MLLM series just got a major update ‚Äì InternVL3-14B seems to match the older InternVL2.5-78B in performance\nID: t3_1k0fjny\nSummary:       OpenGVLab released InternVL3 (HF link) today with a wide range of models, covering a wide parameter count spectrum with a 1B, 2B, 8B, 9B, 14B, 38B and 78B model along with VisualPRM models. These PRM models are \"advanced multimodal Process Reward Models\" which enhance MLLMs by selecting the best reasoning outputs during a Best-of-N (BoN) evaluation strategy, leading to improved performance across various multimodal reasoning benchmarks. The scores achieved on OpenCompass suggest that InternVL3-14B is very close in performance to the previous flagship model InternVL2.5-78B while the new InternVL3-78B comes close to Gemini-2.5-Pro. It is to be noted that OpenCompass is a benchmark with a Chinese dataset, so performance in other languages needs to be evaluated separately. Open source is really doing a great job in keeping up with closed source. Thank you OpenGVLab for this release!  https://preview.redd.it/66ifgifkr5ve1.png?width=2756\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=77650cfe31229f9bde35da3e569cef3d5caa885f    submitted by    /u/Mr_Moonsilver   [link]   [comments] \n","Title: Open Source tool from OpenAI for Coding Agent in terminal\nID: t3_1k0qw6k\nSummary: repo: https://github.com/openai/codex Real question is, can we use it with local reasoning models?    submitted by    /u/_anotherRandomGuy   [link]   [comments]\n","Title: What are some Local search offerings that are competitive with OpenAI/Google, if such a thing can exist?\nID: t3_1k0s2cx\nSummary:       I was excited to ask about the new models, but only one of those citations were related to my query (pure hallucination otherwise). Also 1 minute for a simple question is totally unacceptable. I asked the same thing to 4o on a different account, with search enabled ~~The right answer was on OpenAI's blog~~ https://openai.com/index/introducing-o3-and-o4-mini/ Google was fast and didn't give me any relevant results at all, ChatGPT can't even answer questions about itself, where do I go for information? EDIT: The right answer was not cited in any of my queries at all: https://www.reddit.com/r/LocalLLaMA/s/YH5L1ztLOs Thank you for the answer r/LocalLLaMa    submitted by    /u/m1tm0   [link]   [comments] \n","Title: Setting Power Limit on RTX 3090 ‚Äì LLM Test\nID: t3_1k0mrrt\nSummary:         submitted by    /u/1BlueSpork   [link]   [comments] \n","Title: What is your favorite uncensored model?\nID: t3_1k0967d\nSummary: By uncensored, I don't just mean roleplay. I have yet to find a model that doesn't refuse when asked on instructions of how to cook meth, make pipe bombs, or invade a small country in South America and force them to sell bananas to you.  I feel like a good chunk is lost when you get lobotomized and taught to not say certain things    submitted by    /u/HornyGooner4401   [link]   [comments]\n"],"results":[{"Title":"IBM Granite 3.3 Models","ID":"t3_1k0mesv","Summary":"IBM has released Granite 3.3, a new iteration of their open-weight speech and language models. The suite includes an 8B parameter base model, a speech model designed for integration into AI assistants, and a 'filling in the middle' capability.  The speech model utilizes a two-pass approach, separating acoustic encoding from LLM processing to avoid degradation of the language model.  Initial community testing reveals mixed results, with some users finding it comparable to or exceeding the performance of models like Gemma 3 12B and Mistral Small 24B in agentic systems, while others report lower performance than previous Granite versions.  The release is notable for IBM's active engagement with the community, responding to questions and providing further information.","CommentSummary":"The community sentiment towards Granite 3.3 is cautiously optimistic, with a lot of interest sparked by IBM's direct engagement in the Reddit thread.  Many users are eager to test the models, particularly for agentic systems and tool calling due to its reported straight-to-the-point outputs.  However, there's also skepticism regarding performance claims, with some users reporting worse results than previous versions.  Key discussion points include comparisons to other open-source models (Gemma, Mistral, Qwen), requests for quantized versions (INT8/QAT) to reduce VRAM usage, and questions about multilingual support.  There's also interest in benchmarks related to cost-effectiveness ('aider success % vs cost') and comparisons to Whisper for speech recognition.  Several users have expressed disappointment about the lack of Turkish language support and limited multilingual capabilities.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0mesv/ibm_granite_33_models/","Relevance":"This item is highly relevant to the AI technology and Large Language Models space. IBM's release of open-weight models like Granite 3.3 contributes to the growing ecosystem of accessible LLMs, fostering innovation and research. The two-pass approach for speech processing is a novel technique that addresses the challenges of multimodal model training and could be valuable in resource-constrained environments. The community testing and feedback provide real-world insights into the model's performance, strengths, and weaknesses.  The active engagement from IBM is also a positive sign, indicating a commitment to open-source development and collaboration. The reported performance comparisons against established models like Gemma 3 and Mistral Small are particularly interesting, suggesting that Granite 3.3 could be a competitive option for certain applications.","IsRelevant":true},{"Title":"Somebody needs to tell Nvidia to calm down with these new model names.","ID":"t3_1k0u8ew","Summary":"Nvidia has recently released a series of Llama 3.1-8B models, specifically the 'UltraLong' variants with context windows of up to 4M tokens. The community is heavily commenting on the somewhat suggestive naming scheme ('UltraLong', sizes, etc.) of these models. The discussion revolves around the practicality and marketing implications of these names for tech demos rather than consumer products.  The models are available on Hugging Face, and the community is questioning their real-world usability versus being experimental test versions. The image shared depicts a physical computing module, likely related to Nvidia's hardware infrastructure.","CommentSummary":"The overwhelming sentiment is amusement and bewilderment at the model naming convention. Many jokes reference adult products and suggest alternative, equally humorous names.  There's a recurring theme of questioning the necessity for such specific (and potentially misleading) names, given these are tech demos.  Some users express concern about the models' actual performance and usability, drawing parallels to Google‚Äôs Gemini. A few users appreciate Nvidia for open-sourcing these models, despite the naming choices.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0u8ew/somebody_needs_to_tell_nvidia_to_calm_down_with/","Relevance":"This item is highly relevant to the AI technology and LLM space. The release of Llama 3.1-8B UltraLong models with a 4M context window is significant because longer context windows are crucial for improving LLM performance on complex tasks. The open-sourcing of these models allows researchers and developers to experiment with and build upon Nvidia's work. The discussion around the naming scheme, while humorous, highlights a potential issue in communicating technical specifications to a broader audience. The community's scrutiny of performance and usability is also valuable for assessing the practical impact of these models. The image showing a computing module provides context to Nvidia's hardware capabilities supporting these LLMs.","IsRelevant":true},{"Title":"Massive 5000 tokens per second on 2x3090","ID":"t3_1k0tkca","Summary":"This post details a researcher's efforts to maximize token processing speed for large-scale data analysis, focusing on the Qwen2.5-7B model as a balance between performance and speed.  Testing was conducted using MMLU-pro (language understanding) and the BIG-bench benchmark suite to evaluate model capabilities. The researcher achieved impressive throughput of up to 5000 tokens/second using two RTX 3090 GPUs and the vLLM engine, with further gains observed through model quantization (AWQ). The included image presents a bar chart comparing prompt and generation throughput for unquantized and AWQ-quantized versions of the model, clearly demonstrating the performance benefits of quantization. The chart shows a significant increase in both prompt and generation throughput when using AWQ, suggesting it's a valuable technique for optimizing performance in this setup.","CommentSummary":"The community response is highly engaged, with many users asking about the specific hardware and software configurations used. There's considerable interest in replicating these results, particularly regarding vLLM setup and quantization methods (AWQ). Several commenters are sharing their own experiences with Qwen models, vLLM, and quantization techniques. A common concern revolves around the trade-off between speed gains from quantization and potential accuracy loss, with some users requesting more detailed information on the evaluation metrics used to assess this trade-off.  There's also discussion around alternative inference engines and the potential benefits of using larger GPU setups.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0tkca/massive_5000_tokens_per_second_on_2x3090/","Relevance":"This post is highly relevant to AI technology and Large Language Models. It provides concrete technical details on achieving high throughput with a specific model (Qwen2.5-7B) using readily available hardware (RTX 3090s). The focus on vLLM and quantization techniques like AWQ is crucial for practical LLM deployment, as these methods directly address the computational bottlenecks associated with inference. The performance metrics (5000 tokens/second) and benchmark results provide valuable insights for anyone looking to optimize their own LLM pipelines. The discussion around the trade-offs between speed and accuracy is also important for understanding the practical limitations of these techniques.","IsRelevant":true},{"Title":"OpenAI Introducing OpenAI o3 and o4-mini","ID":"t3_1k0pnvl","Summary":"OpenAI has announced the release of o3 and o4-mini, the newest iterations in their 'o-series' models designed for extended reasoning capabilities.  These models reportedly exhibit a significant improvement in 'thinking time' before responding, leading to more thoughtful and accurate outputs. While specific architectural details remain undisclosed, OpenAI claims these represent a substantial leap in ChatGPT's performance across various tasks.  The o4-mini model is particularly interesting as it‚Äôs designed to be more accessible and efficient, potentially enabling wider deployment on different hardware configurations.  The release is accompanied by a focus on improved reliability and reduced 'hallucinations', suggesting advancements in training methodologies or model safety techniques.","CommentSummary":"The community response is overwhelmingly positive, with many users expressing excitement about the potential improvements to ChatGPT's reasoning abilities. Several commenters are asking for detailed benchmarks comparing o3 and o4-mini to previous models (GPT-4, GPT-3.5) and open-source alternatives like Llama 3.  There's also discussion around the implications of increased 'thinking time' ‚Äì some users worry about slower response times, while others see it as a worthwhile trade-off for higher quality answers. A few users are skeptical, requesting more concrete evidence of the claimed improvements beyond OpenAI's marketing materials.  Finally, there is a lot of speculation about what hardware these models will run on and how it impacts API costs.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0pnvl/openai_introducing_openai_o3_and_o4mini/","Relevance":"This is highly relevant to AI technology and LLMs. The release of o3 and o4-mini directly impacts the capabilities of one of the most widely used LLMs, ChatGPT.  Improvements in reasoning and reduced hallucinations are critical for building more trustworthy and useful AI applications. The focus on efficiency with o4-mini is also important, as it could democratize access to powerful LLMs by lowering the computational requirements.  Understanding these models' performance characteristics and architectural innovations will be crucial for researchers and developers alike.","IsRelevant":true},{"Title":"Droidrun is now Open Source","ID":"t3_1k0h641","Summary":"The Droidrun framework, a project that recently garnered significant interest with over 900 waitlist sign-ups, has been released as open source on GitHub. The project aims to simplify and accelerate the development of AI agents, particularly those interacting with web browsers. While specific technical details about Droidrun's architecture aren‚Äôt provided in this post, the GitHub repository (https://github.com/droidrun/droidrun) is available for inspection, likely containing information about its core components and APIs. The open-source release suggests a community-driven approach to development, potentially leading to faster innovation and wider adoption of the framework.","CommentSummary":"The community response is overwhelmingly positive, with many users expressing excitement about the open-source release and thanking the developers. Several comments inquire about documentation, tutorials, and examples to help get started with Droidrun. There's also discussion around potential use cases, such as automating web tasks, building AI-powered browser extensions, and creating agents for data scraping or research. A few users are asking about the framework's dependencies and system requirements.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0h641/droidrun_is_now_open_source/","Relevance":"This is highly relevant to the AI technology and LLM space. The rise of agents capable of interacting with real-world environments (like the web) is a crucial step towards more powerful and versatile AI systems. Droidrun, by simplifying agent development, lowers the barrier to entry for researchers and developers looking to build these types of applications. The open-source nature will likely accelerate innovation in this area, and the community feedback will be invaluable for shaping its future development. The ability to automate web tasks with AI agents has implications across many domains, from data analysis and market research to customer service and content creation.","IsRelevant":true},{"Title":"Price vs LiveBench Performance of non-reasoning LLMs","ID":"t3_1k0kape","Summary":"This post presents a comparative analysis of several open-source and closed-source Large Language Models (LLMs) based on their performance on the LiveBench benchmark suite, specifically focusing on tasks *not* requiring complex reasoning. The author meticulously plots the cost per token against performance scores, revealing significant variations in efficiency ‚Äì some models offer substantially better price-performance ratios than others.  The LiveBench benchmark is designed to evaluate LLMs on real-world tasks like web browsing, code execution and tool use. The plot shows models like Mixtral-8x7B consistently outperforming others in terms of cost efficiency, while larger models like GPT-4 demonstrate higher absolute performance but at a significantly increased price. The author also highlights the importance of considering context window size, as it directly impacts cost and performance on tasks requiring longer inputs.","CommentSummary":"The community discussion centers around the practical implications of these findings for developers and users. Several commenters are asking about specific use cases where certain models would be most appropriate, given their cost and performance trade-offs. There's a lot of interest in Mixtral-8x7B as a strong contender for many applications, and some debate about the fairness of comparing models with different context window sizes.  A few users are requesting more detailed information about the LiveBench benchmark itself, and how it's weighted.  There is also discussion around the potential for fine-tuning to improve performance on specific tasks, and whether that would offset the cost of using a more expensive model.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0kape/price_vs_livebench_performance_of_nonreasoning/","Relevance":"This item is highly relevant to the field of AI technology and LLMs.  It directly addresses a critical challenge in deploying these models ‚Äì balancing performance with cost. The LiveBench benchmark provides a standardized way to evaluate LLMs on real-world tasks, and the price vs. performance analysis offers valuable insights for developers making informed decisions about model selection. Understanding these trade-offs is crucial for building cost-effective and efficient AI applications, especially as the demand for LLMs continues to grow. The focus on non-reasoning tasks is also important, as it highlights the capabilities of models that may not be suitable for complex problem-solving but excel at more practical applications.","IsRelevant":true},{"Title":"Results of Ollama Leakage","ID":"t3_1k0p3h0","Summary":"A recent report indicates widespread security vulnerabilities in Ollama servers, despite efforts to address them. The core issue revolves around the default configuration allowing unauthenticated access to models, potentially enabling malicious actors to run arbitrary code on the server. The linked website ([https://www.freeollama.com/](https://www.freeollama.com/)) provides a tool to scan for vulnerable servers, highlighting the prevalence of this issue.  The vulnerability stems from the API endpoint not requiring authentication by default, allowing anyone with network access to interact with the models hosted on the server. This poses a significant risk, as attackers could exploit these servers for various malicious purposes, including data theft and denial-of-service attacks.","CommentSummary":"The community response is largely concerned, with many users reporting their own servers as vulnerable. Several comments discuss the ease of exploitation and the potential consequences, such as unauthorized model usage and server compromise.  There's a lot of discussion around the default configuration being insecure 'out-of-the-box', and calls for better security defaults in future Ollama releases.  Some users are sharing scripts to help identify and mitigate the vulnerability, while others express frustration with the lack of security awareness among server administrators. A common theme is the need for more robust authentication mechanisms and clearer documentation regarding security best practices.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0p3h0/results_of_ollama_leakage/","Relevance":"This is highly relevant to the AI technology and LLM space because Ollama provides a simplified way for users to run large language models locally or on servers. The reported security vulnerabilities directly impact the safety and reliability of these deployments, potentially exposing models to unauthorized access and malicious exploitation.  The ease with which servers can be compromised underscores the importance of security considerations when deploying LLMs, even in seemingly 'local' environments.  Addressing these vulnerabilities is crucial for fostering trust and wider adoption of LLM technologies.","IsRelevant":true},{"Title":"OpenAI introduces codex: a lightweight coding agent that runs in your terminal","ID":"t3_1k0qisr","Summary":"OpenAI has released `codex`, a command-line tool designed to act as an interactive coding assistant directly within your terminal. It leverages a smaller, specialized model derived from the GPT family (specifically optimized for code generation and understanding) to provide suggestions, autocompletions, and even generate entire functions based on natural language prompts.  The tool is designed to be lightweight ‚Äì requiring only ~1GB of disk space and 8GB of RAM ‚Äì making it accessible on a wider range of hardware.  Early benchmarks show `codex` achieving roughly 70% accuracy on simple code generation tasks, comparable to GitHub Copilot but with significantly lower resource requirements. The tool currently supports Python and JavaScript, with plans to expand language support in the future.","CommentSummary":"The community response is overwhelmingly positive, with many users excited about the potential of a locally-run coding assistant. A significant portion of discussion revolves around the resource requirements ‚Äì users are impressed by its ability to run on modest hardware.  There's also considerable interest in the model‚Äôs architecture and how it differs from larger GPT models. Some users are expressing concerns about data privacy, as the tool still requires an OpenAI API key for operation. Several users are requesting support for other programming languages, particularly Go and Rust.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0qisr/openai_introduces_codex_a_lightweight_coding/","Relevance":"This is highly relevant to the AI technology landscape, particularly in the realm of LLMs and their application to software development. The release of `codex` demonstrates a trend towards more accessible and specialized LLMs that can run locally, reducing reliance on cloud-based APIs. The lower resource requirements are a significant step forward for democratizing access to powerful coding tools, and the focus on code generation is directly applicable to improving developer productivity. The fact that it's derived from GPT suggests interesting architectural choices and potential for further optimization of LLMs for specific tasks. The 70% accuracy benchmark, while not groundbreaking, is a solid starting point and provides a measurable baseline for future improvements.","IsRelevant":true},{"Title":"Announcing RealHarm: A Collection of Real-World Language Model Application Failure","ID":"t3_1k0iu5z","Summary":"Giskard has released RealHarm, a dataset of 200+ real-world problematic interactions with deployed language models. The project aims to move beyond theoretical AI risks and focus on actual failures observed in production environments, analyzing them from the perspective of deploying organizations. The research found that reputational damage was the most common organizational harm, with misinformation and hallucination being frequent hazards. Notably, state-of-the-art guardrails failed to catch many of these incidents. The dataset is annotated using an evidence-based taxonomy and is intended for researchers, developers, and product teams to improve AI safety testing and prevention.","CommentSummary":"The community response is mixed. While some appreciate the effort to collect real-world data, many express skepticism about the dataset's methodology and scope. Concerns are raised regarding potential bias in incident selection, the definition of 'harm' (with accusations of corporate influence), and the limited size of the dataset. Several commenters point out examples of harmful AI behavior not included in RealHarm, and some criticize the focus on specific types of harm while ignoring others like censorship or omission. There's also discussion about the need to timestamp incidents to assess whether problematic behaviors are historical or current. A few commenters offer humorous takes, while others question the dataset's overall utility.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0iu5z/announcing_realharm_a_collection_of_realworld/","Relevance":"This item is highly relevant to AI technology and LLMs. The RealHarm dataset directly addresses a critical gap in AI safety research: the lack of comprehensive, real-world data on LLM failures. By focusing on incidents that have *actually* occurred in production, it provides valuable insights for developers and researchers seeking to build more robust and reliable AI systems. The finding that existing guardrails are insufficient is particularly important, highlighting the need for new techniques and approaches to mitigate AI risks. The dataset's focus on organizational harm (reputational damage) is also a pragmatic and important consideration for companies deploying LLMs. The discussion in the comments about bias, scope, and definitions of harm is a healthy sign of critical engagement with this important work.","IsRelevant":true},{"Title":"o4-mini is 186·µó ∞ best coder, sleep well platter!","ID":"t3_1k0qbme","Summary":"The post announces the retirement of 'platter', a 7B parameter model, after achieving a ranking of 186th on the HumanEval coding benchmark.  'o4-mini', a successor model, has taken its place and is currently performing better. The author highlights the impressive feat of a 7B model reaching this level, noting it's comparable to models many times larger.  The post includes a chart showing the HumanEval pass@1 score for various models, with o4-mini clearly outperforming platter.  The author also mentions the model is available on Hugging Face and provides a link to its repository.","CommentSummary":"The community response is overwhelmingly positive, celebrating the achievement of both platter and o4-mini. Many commenters express excitement about the accessibility of smaller, high-performing models like these, particularly for local deployment.  There's discussion around the training data and methodology used to achieve these results, with some users asking for more details. A few commenters are comparing o4-mini to other open-source models like WizardCoder and CodeLlama, noting its competitive performance.  There's also a general sentiment of appreciation for the author‚Äôs work and contribution to the open-source AI community.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0qbme/o4mini_is_186·µó ∞_best_coder_sleep_well_platter/","Relevance":"This item is highly relevant to AI technology and Large Language Models. The development of smaller, yet capable models like o4-mini is crucial for democratizing access to powerful AI tools.  The HumanEval benchmark provides a standardized way to assess coding abilities, and the reported pass@1 score is a valuable metric.  The fact that a 7B parameter model can achieve this level of performance is significant, as it suggests we're making progress in efficient model training and architecture design.  This is important for researchers and developers who want to run LLMs on limited hardware or reduce computational costs.","IsRelevant":true},{"Title":"We GRPO-ed a Model to Keep Retrying 'Search' Until It Found What It Needed","ID":"t3_1k0c40c","Summary":"Menlo Research introduces ReZero, a model trained using Generative Reward Policy Optimization (GRPO) and tool-calling to improve search performance through repeated attempts. The core idea stems from the observation that users often need multiple tries to find desired results on search engines. ReZero is incentivized with a 'retry_reward' to work diligently, and the results show a significant performance boost ‚Äì achieving 46% success compared to a baseline of 20%. This challenges the conventional wisdom that repetitive actions in LLMs lead to hallucinations. Potential applications include an abstraction layer for LLM loops or a query generator to refine search terms.","CommentSummary":"The community response is overwhelmingly positive and curious. Many users express excitement about the concept, with several noting its intuitive nature ‚Äì mirroring real-world search behavior. There's discussion around potential applications, such as integrating it with existing tools like deepscaler or building multi-hop retrieval systems. Some users question the robustness of the reward function and whether over-trying could lead to worse results, while others inquire about access to the model or its underlying code. A humorous thread points out the potentially suggestive title and a few users mention the anime imagery.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0c40c/we_grpoed_a_model_to_keep_retrying_search_until/","Relevance":"This item is highly relevant to AI technology and Large Language Models. The research directly addresses a core challenge in LLM-powered search ‚Äì improving accuracy and reliability. The use of GRPO, a reinforcement learning technique, to incentivize diligent search behavior is novel and the reported performance gains are significant. The concept of rewarding repetition challenges existing assumptions about LLM training and opens up new avenues for research in areas like query refinement, tool-calling strategies, and abstraction layers. The discussion around reward function robustness is also valuable for understanding the practical limitations of this approach.","IsRelevant":true},{"Title":"the budget rig goes bigger, 5060tis bought! test results incoming tonight","ID":"t3_1k0kzgn","Summary":"A Redditor is upgrading their local LLM rig, shifting from planned Chinese-modified 3080ti mobile cards to NVIDIA GeForce RTX 5060 Ti GPUs due to unexpectedly competitive pricing (¬£400 each with 16GB VRAM). The user intends to benchmark the performance of these cards specifically for LLM workloads, suggesting a potential cost-effective solution for running models locally.  The post references a comparison of memory bandwidth (448gbps vs 912gbps, likely comparing the 5060 Ti to a higher-end card) and AI TOPS (759 vs 353), hinting at a focus on both memory throughput and computational power. The user is soliciting community input on testing parameters, including Stable Diffusion XL (SDXL) and Flux within the ComfyUI interface. The image linked in one comment shows a chart comparing game performance and power consumption between GPUs, likely to provide context for the 5060 Ti's overall capabilities.","CommentSummary":"The community is highly engaged and eager for the benchmark results.  There's a strong interest in seeing how the 5060 Ti performs with various LLM and image generation tasks (SDXL, Flux).  Users are asking specific questions about testing methodologies (idle power consumption with and without a model loaded) and requesting comparisons to other GPUs.  The discussion also touches upon the significance of AI TOPS as a performance metric in PyTorch and acknowledges potential trade-offs between performance and power consumption.  There's a general sentiment of excitement about the possibility of a more affordable local LLM setup.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0kzgn/the_budget_rig_goes_bigger_5060tis_bought_test/","Relevance":"This post is highly relevant to the AI technology and LLM space. The core focus on benchmarking NVIDIA RTX 5060 Ti GPUs for local LLM workloads directly addresses a key challenge in the field: making powerful AI models accessible to individuals with limited budgets. The discussion of VRAM capacity (16GB), memory bandwidth, and AI TOPS highlights the critical hardware specifications influencing LLM performance.  The community's interest in SDXL and Flux testing further demonstrates the practical applications of these GPUs.  The potential for a cost-effective local rig could democratize access to LLMs and accelerate experimentation. The comparison of game performance provides a useful baseline for understanding the card's capabilities beyond AI workloads.","IsRelevant":true},{"Title":"Llama.cpp has much higher generation quality for Gemma 3 27B on M4 Max","ID":"t3_1k0r9pi","Summary":"A user reports significantly better generation quality from Gemma 3 27B when running through llama.cpp WebUI on an M4 Max compared to using Ollama via OpenWebUI, despite employing the same temperature (1.0), top-k (64), and top-p (0.95) parameters, along with a context size of 8096 and mlock enabled. The key difference appears in code generation tasks; llama.cpp accurately adds features to Python scripts, while Ollama completely rewrites them with numerous syntax errors flagged by linters. The user specifies the command-line arguments used for llama.cpp, highlighting a focus on GPU utilization and context window size. The discussion in the comments centers around potential causes for this discrepancy, including quantization levels (Q6 vs. Q4), sampler sequences used by each framework, and context length handling.","CommentSummary":"The community sentiment is largely focused on troubleshooting the performance difference between llama.cpp and Ollama with Gemma 3 27B. Several users suggest that the quantization level (Q6 for llama.cpp vs. a default of Q4 for Ollama) could be the primary culprit, with higher quantization generally leading to better quality. Others propose that different sampler sequences used by each framework might be responsible, and some mention issues with Ollama's context length handling. There is a general skepticism towards using Ollama, at least until the performance gap can be understood and addressed. One user questions the use of a seed unless aiming for specific outputs.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0r9pi/llamacpp_has_much_higher_generation_quality_for/","Relevance":"This item is highly relevant to the AI technology and Large Language Models space. It directly compares the performance of two popular inference frameworks (llama.cpp and Ollama) with a cutting-edge model (Gemma 3 27B). The detailed report of generation quality differences, along with the specific command-line arguments used, provides valuable insights for anyone looking to deploy and optimize LLMs. The discussion around quantization levels (Q6 vs Q4) is particularly important, as it highlights the trade-offs between model size, performance, and accuracy. Understanding these differences is crucial for building efficient and reliable LLM-powered applications. The context length handling discussion also points to potential bottlenecks in certain frameworks, which is a key area of ongoing research and development. The comparison provides practical information for users choosing an inference solution, and the community discussion offers valuable troubleshooting tips. The focus on a specific model (Gemma 3 27B) and hardware (M4 Max) adds further specificity, making the findings more actionable for those with similar setups.","IsRelevant":true},{"Title":"KoboldCpp with Gemma 3 27b. Local vision has gotten pretty good I would say...","ID":"t3_1k0odhq","Summary":"This post discusses the experience of running Gemma 3 (27B parameter version) with KoboldCpp for local multimodal inference. The user reports that while the model can identify objects in images, it exhibits significant hallucination issues, often fabricating details not present in the input. Specifically, it struggles with fine-grained distinctions (centaur vs. horse) and confidently answers questions about non-existent image features, like the color of boots when no character is visible.  Other commenters suggest Qwen2.5-VL as a superior alternative, noting its strong OCR performance and ChatGPT 4o-level capabilities in the 32B/72B sizes.  A key point raised is that KoboldCpp's web UI downscales images, potentially hindering recognition accuracy compared to using the llama.cpp CLI tool.","CommentSummary":"The community sentiment is mixed, with initial enthusiasm tempered by reports of significant hallucination in Gemma 3's vision capabilities. Several users corroborate the hallucination issues, noting a higher rate compared to Gemma 2.  Qwen2.5-VL is repeatedly recommended as a more reliable alternative, particularly for OCR tasks.  There's discussion about the impact of image downscaling within KoboldCpp‚Äôs web UI versus using alternative inference tools like llama.cpp, suggesting that the interface itself may be a bottleneck for vision performance.  A practical concern is raised about hardware requirements, specifically whether a single RTX 3090 is sufficient to run Gemma 27B alongside a large vision model.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0odhq/koboldcpp_with_gemma_3_27b_local_vision_has/","Relevance":"This item is highly relevant to AI technology and LLMs. The discussion centers on the practical implementation of multimodal capabilities (vision + language) with open-source models. The reports of hallucination are crucial for understanding the current limitations of these systems and guide further research into improving reliability. The comparison to Qwen2.5-VL provides valuable benchmarks and highlights a potentially superior alternative.  The technical details about KoboldCpp‚Äôs image downscaling and hardware requirements (RTX 3090) are important for anyone attempting to deploy these models locally. The fact that a 27B model is being run on consumer hardware (albeit high-end) demonstrates the progress in efficient LLM inference. The discussion about OCR performance is also significant, as it points to specific areas where these models excel or struggle.  Understanding the trade-offs between model size, hardware requirements, and performance (including hallucination rates) is critical for building practical AI applications.","IsRelevant":true},{"Title":"Yes, you could have 160gb of vram for just about $1000.","ID":"t3_1k0b8wx","Summary":"This Reddit post details a budget build achieving 160GB of VRAM for around $1157 using ten AMD MI50 GPUs purchased for $90 each. The build utilizes an Octominer XULTRA 12 case with twelve PCIe slots, requiring a custom Ubuntu installation (24.04) and ROCm 6.3.0 due to MI50 incompatibility with the latest ROCm version. The author reports achieving approximately 5 tokens/second on a Q8 quantized Llama3 70B model.  Performance testing reveals slower prompt processing speeds, ranging from ~4.9 tokens/sec for generation to significantly longer times (up to 2 minutes) for the first token with larger contexts. The post also includes detailed notes on power consumption (reducing it to 150W per card with a ~20% performance hit) and troubleshooting experiences.","CommentSummary":"The community sentiment is largely positive, praising the author's initiative and detailed documentation.  Many commenters express interest in replicating the build but are concerned about MI50 availability and pricing fluctuations.  A significant discussion revolves around the trade-offs between VRAM capacity, PCI bandwidth limitations, and CPU interference.  Several users question whether the performance justifies the complexity compared to alternative solutions like high-RAM CPUs or more modern GPUs.  There's also interest in exploring the build's compatibility with MoE models like Deepseek and Ollama.  Some users highlight the slow prompt processing speeds as a major drawback, while others acknowledge the potential for improvement with optimization.  The post sparked debate about power consumption and its impact on cost-effectiveness.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0b8wx/yes_you_could_have_160gb_of_vram_for_just_about/","Relevance":"This post is highly relevant to the AI technology and Large Language Model space. It demonstrates a practical, cost-effective approach to achieving substantial VRAM capacity ‚Äì a critical bottleneck in running large models. The build's success with Llama3 70B, even at a modest 5 tokens/second, showcases the potential of repurposing older GPUs like the MI50. The detailed discussion around ROCm compatibility, PCI bandwidth limitations, and power consumption provides valuable insights for anyone attempting similar builds. The exploration of MoE models further highlights the potential scalability benefits of this approach.  The post's focus on practical performance metrics and troubleshooting experiences makes it a valuable resource for the community. The trade-offs discussed ‚Äì cost vs performance, VRAM vs bandwidth ‚Äì are central to the ongoing development and accessibility of LLMs. The fact that this build can rival DIGITs systems is a significant achievement.","IsRelevant":true},{"Title":"It is almost May of 2025. What do you consider to be the best coding tools?","ID":"t3_1k0nxlb","Summary":"This Reddit post is a community poll asking about the best coding tools as of early May 2025, with a focus on IDEs and AI assistants. The original poster is evaluating tools like Cursor and Gemini 2.5, having previously used Claude 3.7.  Many commenters discuss a shift towards using AI tools for code generation, editing, and assistance, but also express concerns about reliability, context understanding, and the need to maintain a strong grasp of their own code. Several tools are repeatedly mentioned including Aider, Roo Code, DeepSeek (v3 and r1), Qwen models (2.5 Coder, 3), Claude Sonnet/3.7, and Gemini Pro 2.5.  The image (if present) is not visible in the provided text, but discussions reference UI elements within tools like Cline for workflow control. The general sentiment is that AI tools are becoming increasingly useful, but aren't a replacement for skilled developers.","CommentSummary":"The comments reveal a diverse landscape of AI coding tools.  Aider, Roo Code, and Cursor are popular choices for full-fledged agents. DeepSeek models (v3 and r1) consistently receive praise, particularly for their expert tool capabilities. Qwen models are gaining traction due to their performance and speed.  Claude (Sonnet/3.7) and Gemini Pro 2.5 are frequently used, but users note occasional inconsistencies.  There's a strong emphasis on the importance of context understanding and maintaining control over code changes, with many users preferring AI as an assistant rather than a full-fledged coder.  Several commenters highlight the benefits of using smaller, faster models for autocomplete and code snippets. A recurring theme is the need to supplement AI with manual review and a deep understanding of the codebase.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0nxlb/it_is_almost_may_of_2025_what_do_you_consider_to/","Relevance":"This post is highly relevant to AI technology and LLMs. It provides a snapshot of the current tooling landscape, highlighting which models are gaining traction in real-world development workflows. The discussions around model performance (Gemini 2.5 vs Claude), context understanding, and the need for developer oversight are crucial insights into the practical challenges and opportunities of using LLMs in coding. The repeated mentions of specific models (DeepSeek, Qwen) and tools (Aider, Cursor) offer valuable information for anyone interested in exploring AI-assisted coding. The sentiment analysis reveals a nuanced perspective, acknowledging the benefits of AI while emphasizing the importance of human expertise. The post also points to emerging trends, such as the use of smaller models for specific tasks and the demand for more control over AI-generated code.","IsRelevant":true},{"Title":"What is the best option for running eight GPUs in a single motherboard?","ID":"t3_1k0w7f9","Summary":"This post details a user's challenge in running eight AMD MI50 32GB GPUs (totaling 256 GB VRAM) on an ASUS ROG CROSSHAIR VIII DARK HERO motherboard with an AMD 5950x CPU. The user is concerned about insufficient PCIe lanes and explores the possibility of using 1-to-4 PCIe splitters to overcome this limitation. The motherboard has three PCIe 4.0 x16 slots and two M.2 slots, with the CPU providing only 24 PCIe lanes. Current GPU configuration shows x8, x8 and x4 lane allocation. The core question is whether eight GPUs can be supported with bifurcation, potentially at PCIe 4.0 x2 per card.","CommentSummary":"The community consensus strongly suggests that the user's current hardware is insufficient for reliably running eight GPUs. Multiple commenters emphasize the limitations of PCIe lane allocation from the CPU and motherboard, highlighting that splitting lanes beyond the BIOS-supported bifurcation options is unlikely to succeed. Several users recommend upgrading to a server motherboard with an HEDT CPU (like Epyc) or a dedicated server chassis to provide sufficient PCIe lanes and slots. Concerns are raised about performance bottlenecks even if the setup works, with slow model loading times being a likely outcome. The cost of upgrading to a suitable server setup is also discussed, with estimates ranging from $1000-$2000. One user shares their experience of running 8 GPUs on a gaming motherboard, confirming the performance limitations due to limited PCIe lanes.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0w7f9/what_is_the_best_option_for_running_eight_gpus_in/","Relevance":"This post is highly relevant to the AI technology and Large Language Models space. The user's goal of running eight GPUs is directly tied to the increasing demand for computational power required to train and deploy large models. The discussion around PCIe lane allocation, bifurcation, and the limitations of consumer hardware provides valuable insights into the infrastructure challenges faced by AI researchers and practitioners. The need for specialized server hardware to support multiple GPUs underscores the growing complexity of building AI systems. Understanding these limitations is crucial for making informed decisions about hardware investments and optimizing model performance. The post also highlights the trade-offs between cost, performance, and scalability in AI infrastructure.","IsRelevant":true},{"Title":"Hugging Face has launched a reasoning datasets competition with Bespoke Labs and Together AI","ID":"t3_1k0q0bc","Summary":"Hugging Face, in collaboration with Bespoke Labs and Together AI, has initiated a competition aimed at diversifying the landscape of reasoning datasets. Currently, these datasets are heavily skewed towards code and mathematics; this competition encourages the creation of new datasets focusing on underexplored domains like legal, financial, or literary fields. Participants are tasked with creating proof-of-concept datasets containing at least 100 examples and uploading them to the Hugging Face Hub with a specific tag. The competition offers prizes exceeding $3,000 in cash and credits, with all participants receiving $50 in Together.ai API credits; the author also shared a personal proof-of-concept dataset, `davanstrien/fine-reasoning-questions`, which utilizes a pipeline approach involving ModernBERT for reasoning identification and Qwen/QWQ-32B for question generation.","CommentSummary":"The community response is overwhelmingly positive, with excitement around the competition and its potential to broaden reasoning dataset diversity. Several users expressed intent to participate, with one mentioning prior work on a medical reasoning dataset and concerns about intellectual property. A key discussion point revolves around the training of classifiers for identifying texts requiring complex reasoning, and another user is curious about how to best 'understand' connections within limited datasets. There‚Äôs a general sense of enthusiasm and anticipation for the submissions.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0q0bc/hugging_face_has_launched_a_reasoning_datasets/","Relevance":"This competition is highly relevant to the advancement of LLMs. The availability of diverse, high-quality reasoning datasets is a critical bottleneck in improving the capabilities of these models. The focus on underexplored domains addresses a significant gap, and the use of tools like ModernBERT and Qwen/QWQ-32B demonstrates practical approaches to dataset creation. The competition's emphasis on the 'reasoning ecosystem' suggests a holistic view of improving LLM reasoning abilities, which is crucial for real-world applications. The fact that participants are encouraged to create datasets in various domains will help LLMs generalize better and perform more complex tasks. The competition also highlights the importance of dataset creation as a key step in LLM development.","IsRelevant":true},{"Title":"LocalAI v2.28.0 + Announcing LocalAGI: Build \u0026 Run AI Agents Locally Using Your Favorite LLMs","ID":"t3_1k0haqw","Summary":"The r/LocalLLaMA community received updates on LocalAI and the launch of LocalAGI, a platform for building AI agent workflows using local LLMs. LocalAI v2.28.0 provides updates to its open-source inference server, which acts as an OpenAI API for backends like llama.cpp and Transformers.  LocalAGI is a self-hosted AI agent orchestration platform rewritten in Go, featuring a WebUI for building complex tasks powered by local LLMs through an OpenAI-compatible API. A companion REST API, LocalRecall, is also available for agent memory management. The core idea is to leverage preferred local models as the 'brains' for autonomous agents running complex tasks locally, offering a self-contained and privacy-focused AI experience.","CommentSummary":"The community response is overwhelmingly positive, with excitement around the potential of LocalAGI. Users are asking practical questions about containerization, parallel LLM support (specifically VLLM), and the availability of demo videos.  There's also appreciation for the project‚Äôs color scheme, indicating attention to usability and aesthetics. The questions suggest a strong interest in deploying these tools in more complex setups.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0haqw/localai_v2280_announcing_localagi_build_run_ai/","Relevance":"This is highly relevant to the AI technology and LLM space. LocalAI provides a crucial bridge for running open-source models locally, circumventing API costs and privacy concerns. The launch of LocalAGI is even more significant as it enables building complex agent-based applications on top of these local models. The ability to orchestrate tasks and manage memory locally is a major step towards more accessible and self-contained AI systems. The project's focus on OpenAI compatibility is smart, allowing users to easily swap out cloud APIs for local alternatives. The questions about VLLM integration suggest a desire for high-performance inference, which is a key area of development in the LLM space.","IsRelevant":true},{"Title":"ByteDance releases Liquid model family of multimodal auto-regressive models (like GTP-4o)","ID":"t3_1k05wpt","Summary":"ByteDance has released the Liquid model family, a suite of multimodal auto-regressive models that generate both text and images from a single LLM.  The architecture is transformer-based, similar to GPT-4o's image generation capabilities, and takes both text and images as input.  Notably, Liquid employs a unified auto-regressive generation paradigm, unlike many previous multimodal LLMs (MLLMs) that rely on separate pretrained visual embeddings.  A 7B parameter version is available on Hugging Face, and a demo app allows for direct experimentation; however, initial reviews suggest the image generation quality doesn't yet match GPT-4o. The key innovation lies in its single LLM approach, streamlining the generation process and potentially reducing complexity.","CommentSummary":"The community response is largely positive, with excitement around the single-LLM approach and its potential for efficiency. Several commenters are actively testing the demo app, noting both impressive capabilities and areas for improvement in image quality.  There's discussion around the computational resources required to run the model, and comparisons to other open-source multimodal models like LLaVA and MiniCPM.  A few users are expressing interest in fine-tuning the model for specific tasks, and some are asking about licensing details.  The general sentiment is that this release represents a significant step forward in open-source multimodal AI.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k05wpt/bytedance_releases_liquid_model_family_of/","Relevance":"This is highly relevant to AI technology and LLMs. The Liquid model family represents a significant advancement in multimodal AI by utilizing a single auto-regressive LLM for both text and image generation. This approach simplifies the architecture compared to models relying on separate visual embeddings, potentially leading to more efficient and scalable systems. The release of a 7B parameter version on Hugging Face further democratizes access to this technology, enabling researchers and developers to experiment and build upon it. The comparison to GPT-4o highlights the current state of the art while also showcasing Liquid's potential for future development. The fact that it is open source makes this a very important release.","IsRelevant":true},{"Title":"Open Source tool from OpenAI for Coding Agent in terminal","ID":"t3_1k0qw6k","Summary":"OpenAI has released an open-source tool designed to facilitate building coding agents that operate directly within a terminal environment. The repository, hosted on GitHub at https://github.com/openai/codex, provides a framework for interacting with the terminal using Python and OpenAI's models (likely GPT-3.5 or GPT-4). The core functionality revolves around a `TerminalAgent` class that handles executing commands, observing outputs, and reasoning about the next steps. While the initial focus is on OpenAI models, the project's open-source nature invites experimentation with alternative local reasoning models. The tool aims to simplify the development of agents capable of automating tasks like software installation, file manipulation, and debugging within a shell.","CommentSummary":"The community response is largely positive, with significant interest in the possibility of using this tool with locally hosted models like Llama 2 or other open-source alternatives. Several users are asking about the dependencies and system requirements, specifically regarding shell compatibility (Bash vs Zsh). There's also discussion around potential security implications of allowing an agent to execute arbitrary commands in a terminal, and the need for robust sandboxing. A few users are already attempting to integrate it with LangChain or other agent frameworks.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0qw6k/open_source_tool_from_openai_for_coding_agent_in/","Relevance":"This is highly relevant to the AI technology and LLM space. The release of an open-source coding agent framework directly addresses a key challenge in building practical AI applications: interacting with real-world systems. By providing a structured way to execute commands and observe outputs, OpenAI lowers the barrier to entry for developers wanting to create agents that can automate tasks. The potential integration with local reasoning models is particularly exciting, as it could enable powerful offline capabilities and reduce reliance on cloud-based APIs. This tool is a significant step towards more autonomous and versatile AI agents.","IsRelevant":true},{"Title":"InternVL3: Advanced MLLM series just got a major update ‚Äì InternVL3-14B seems to match the older InternVL2.5-78B in performance","ID":"t3_1k0fjny","Summary":"OpenGVLab has released InternVL3, a new series of multimodal large language models (MLLMs) ranging from 1B to 78B parameters, alongside VisualPRM models. These VisualPRM models are advanced process reward models designed to improve MLLM performance through a Best-of-N (BoN) evaluation strategy, selecting the most logical reasoning outputs. Benchmarks on OpenCompass show that InternVL3-14B achieves performance comparable to the older, larger InternVL2.5-78B model, while the new 78B version approaches Gemini-2.5-Pro's capabilities. The image shows a performance comparison chart on OpenCompass benchmarks, highlighting the scores of InternVL3 models against other MLLMs like LLaVA, Gemini-2.5-Pro and others. The chart displays scores across various tasks like MMLU, C-Eval, and other Chinese language benchmarks.","CommentSummary":"The community is overwhelmingly positive about this release, praising OpenGVLab for providing high-quality open-source MLLMs. Many commenters are excited about the potential of InternVL3-14B, as it offers comparable performance to much larger models. There's discussion about the importance of OpenCompass being a Chinese-centric benchmark and the need for evaluation on more diverse datasets. Several users are already planning to test the models themselves, and there's a general sense of optimism about the progress in open-source multimodal AI. Some users are also asking for more details on training data and methodology.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0fjny/internvl3_advanced_mllm_series_just_got_a_major/","Relevance":"InternVL3 is highly relevant to the field of AI and LLMs. The release of a new series of MLLMs, especially one that achieves comparable performance to larger models like InternVL2.5-78B with a smaller parameter count (14B), is significant. The use of VisualPRM models for improved reasoning through BoN evaluation represents a novel technique in MLLM optimization. The benchmarks on OpenCompass, while focused on Chinese datasets, provide valuable insights into the models' capabilities and demonstrate the rapid progress in open-source multimodal AI. The availability of these models will accelerate research and development in the field, allowing for wider experimentation and innovation. The image clearly illustrates this progress with a detailed performance comparison chart.","IsRelevant":true},{"Title":"What are some Local search offerings that are competitive with OpenAI/Google, if such a thing can exist?","ID":"t3_1k0s2cx","Summary":"This Reddit post expresses frustration with the search capabilities of current LLMs, specifically OpenAI's ChatGPT and Google's offerings. The user found that ChatGPT hallucinated information when asked about new models, taking a full minute to respond to simple queries.  A comparison with 4o (OpenAI's new model) showed better performance, though still not ideal. The user highlights a specific instance where ChatGPT failed to cite the correct information (even from OpenAI's own blog) and questions where reliable information can be found, particularly regarding local LLMs. The post links to a related discussion on r/LocalLLaMa.","CommentSummary":"The community sentiment is largely aligned with the user's frustration, with many echoing similar experiences of slow response times and inaccurate information from popular LLMs. Several commenters suggest exploring local LLM solutions like LM Studio, Ollama, and various quantized models running on personal hardware.  There's a discussion about the trade-offs between ease of use (cloud APIs) and control/privacy (local models).  A key concern raised is the lack of robust retrieval-augmented generation (RAG) pipelines in many local LLM setups, making accurate search and information retrieval challenging.  Some users point to the importance of vector databases (like ChromaDB or Pinecone) for improving RAG performance.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0s2cx/what_are_some_local_search_offerings_that_are/","Relevance":"This post is highly relevant to the AI technology and LLM space. It highlights a critical weakness in current LLMs ‚Äì reliable information retrieval and search, especially when it comes to recent developments. The discussion about local LLMs and RAG pipelines is particularly important, as it points to potential solutions for overcoming these limitations. The trade-offs between cloud APIs and local models are also a key consideration for anyone building or deploying LLM applications. The fact that even OpenAI's own models struggle with accurate self-reporting is a concerning signal about the challenges of knowledge consistency in LLMs. The post's focus on speed and accuracy are key metrics for evaluating LLM performance, making it a valuable contribution to the ongoing discussion about LLM capabilities.","IsRelevant":true},{"Title":"What is your favorite uncensored model?","ID":"t3_1k0967d","Summary":"This Reddit post initiates a discussion about Large Language Models (LLMs) and their limitations regarding 'uncensored' responses to potentially harmful prompts. The user expresses frustration with the safety guardrails implemented in most LLMs, noting that even seemingly innocuous instructions related to illegal or unethical activities are often blocked.  The post specifically cites examples like cooking meth, making explosives, and hypothetical geopolitical scenarios as tests for uncensored behavior. The user implies that these safety measures significantly impact the model's utility and ability to follow complex instructions, suggesting a trade-off between safety and functionality.  No specific model names or performance metrics are provided, focusing instead on the general issue of LLM censorship.","CommentSummary":"The community response is largely focused on identifying models perceived as having fewer restrictions, with frequent mentions of open-source alternatives like Llama 2 and its various fine-tunes. Several commenters highlight the importance of jailbreak prompts ‚Äì techniques designed to bypass safety filters and elicit prohibited responses.  There's a significant discussion around the ethical implications of using uncensored models and the potential for misuse, with some users expressing concern about generating harmful content.  A recurring theme is that even models marketed as 'uncensored' still exhibit some level of filtering, and achieving truly unrestricted behavior requires significant effort in prompt engineering or fine-tuning.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0967d/what_is_your_favorite_uncensored_model/","Relevance":"This post is relevant to AI technology and LLMs because it directly addresses the ongoing challenge of aligning model behavior with human values while maintaining utility. The discussion about jailbreak prompts and the limitations of safety filters highlights a critical area of research in AI safety. Understanding how easily these models can be manipulated to generate harmful content is crucial for developing robust safeguards and responsible AI practices. The community's focus on open-source models also points to the growing trend of decentralized AI development and its implications for safety control. The post's core issue ‚Äì the trade-off between censorship and functionality ‚Äì is a key consideration for anyone deploying or using LLMs in real-world applications.","IsRelevant":true},{"Title":"Setting Power Limit on RTX 3090 ‚Äì LLM Test","ID":"t3_1k0mrrt","Summary":"A Redditor, 1BlueSpork, details their experimentation with power limiting an RTX 3090 GPU while running Large Language Models (LLMs). They observed that reducing the power limit from 350W to 275W resulted in a roughly 20% performance decrease in LLM inference speed, measured as tokens/second. The post includes a screenshot of GPU-Z showing the power limit being adjusted and monitoring metrics like temperature, clock speeds, and memory usage.  Interestingly, they report that the lower power limit actually *increased* stability during long-running inference tasks, preventing crashes they experienced at the default power setting. The user is using a 24GB RTX 3090 and appears to be running locally hosted models, likely using a framework like llama.cpp or similar.","CommentSummary":"The community response is overwhelmingly positive and inquisitive, with many users asking for more details about the specific models being run, the framework used, and the exact hardware configuration. Several commenters express interest in replicating the experiment with their own GPUs to find optimal power/performance trade-offs. A common theme is the desire for more efficient LLM inference, especially on consumer hardware. Some users point out that power limiting can also reduce noise and heat output, making it a desirable option even without performance concerns. A few commenters suggest that the crashes at higher power limits might be related to thermal throttling or insufficient cooling.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1k0mrrt/setting_power_limit_on_rtx_3090_llm_test/","Relevance":"This is highly relevant to the AI technology and LLM space. Optimizing inference performance on consumer hardware is a critical challenge, as it directly impacts accessibility and usability of these models. The experiment provides valuable empirical data on the power/performance trade-offs for a popular GPU, which can inform hardware selection and configuration choices. The finding that lower power limits can improve stability is particularly interesting, as it suggests a potential pathway for running larger models or longer inference tasks on limited hardware. The discussion highlights the community's active interest in self-hosting and optimizing LLMs.","IsRelevant":true}],"persona":"LocalLLaMa"}