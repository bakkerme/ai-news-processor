{"raw_input":["Title: What do I test out / run first?\nID: t3_1kexdgy\nSummary:       Just got her in the mail. Haven't had a chance to put her in yet.    submitted by    /u/Recurrents   [link]   [comments] \nImageDescription: \nComment:       Just got her in the mail. Haven't had a chance to put her in yet.    submitted by    /u/Recurrents   [link]   [comments] \nComment: First run home. Preferably safely. \nComment: llama 3.2 1b \nComment: Bro is loaded. How many kidneys did you sell for that?! \nComment: LLAMA 405B Q.000016 \nComment: all the new qwen 3 models \nComment: sexy ass card \nComment: Old School Runescape \nComment: Are they selling those already? \nComment: Download cuda and make sure your pytorch is the cuda version \nComment: Can it run Crysis? \nComment: You bought it just to benchmark it, didn't you? \nComment: Hello Kitty Island Adventures, butters would be proud of you. \nComment: Would you mind sharing or DMing retailer info? I don‚Äôt have a preferred vendor and am curious on your experience. \nComment: Llama 3.3 70b at 8-bit. Would be interesting to see how many tokens per second gives. \nComment: Try Super Mario Bros ü•∏ \nComment: you don't need it. gimme that. \nComment: Rtx pro 6000 is 96Gb it is beast. Without pro is 48gb. I really want to know how many FOPS it is. Or the t/s for a deepseek 70B or largest model it can fit. \nComment: Get some silly concurrency going on qwen 3 32b awq and run the aider benchmark. \nComment: Flux to generate pics of your dream Audi. Find out your use case and try some models that fit. I was first impressed by GLM 4 in one shot coding, but it fails to use other tools. Mistral small is my daily driver currently. It's even fluent in most languages. \nComment: That‚Äôs some expensive computer hardware. Congratulations. \nComment: That‚Äôs our serial number now \nComment: Your power connectors. \nComment: i cant imagine spending that much money on a gpu with that power connector \nComment: Houston we have lift off  https://preview.redd.it/v3z4prno2wye1.png?width=780\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=6a6156b3fc0818b93b0459a14c86a0e0dd1d70d7 \nComment: https://preview.redd.it/5bnvabxayvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=9516acddbdda888267887c823c70c25db1ba8c6e New card installed! \nComment: Is it better then a h100 performance wise? i know the vram is slightly bigger. \nComment: Quake I \nComment: Everything. In all seriousness, I would reaaally like to see the benchmarks on that thing \nComment: Old school runescape \nComment: Cancer research. \nComment: OT, but run 3Dmark and confirm if it really is faster in games than the 5090 (for once in the history of workstation cards). \nComment: dude you are so lucky congrats!! run every qwen 3 model and make videos! i hear you stream, how about a live stream using llama.cpp and testing out models, or lm studio. this card is so awesome üòç \nComment: Something like Gemma 3 27B/Mistral small-3/Qwen 3 32B with maximum context size? \nComment: Qwen3 and don‚Äôt look back \nComment: Qwen 30B A3B q8 has something around 30 GB file size. Should run very fast and have plent of room for context. \nComment: Bios \nComment: Where did you buy it from? \nComment: About $12,000 to $16,000 for the 48gb vram editions here .. not sure we can get the 96gb \nComment: What CPU are you pairing with? Linux? \nComment: https://preview.redd.it/glj9rjmk9vye1.jpeg?width=1280\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=64d6eac13d0a6aaed4b500953bfd300dcea46322 \nComment: ü•∫ü•πüò≠ \nComment: Wan 2.1 fp 32 model \nComment: Crysis. \nComment: Haha I thought it had a plaid pattern printed on it üòÖ \nComment: Dude so cheesed, could've even wait to get home \nComment: Hey, I was looking to buy one as well, how much did you pay and how long did it take to arrive. They are releasing so many cards these days I get confused. \nComment: How much \nComment: what version is it? Max‚ÄìQ? Workstation edition? Etc‚Ä¶ \nComment: GTA V \nComment: Grounding strap. \nComment: Crysis \nComment: Plex Media Server. But make sure to hack your drivers. \nComment: Mate, share some benchmarks!  I‚Äôm about ready to pull the trigger on one too, but the price gouging here is insane. They‚Äôre still selling Ampere A6000s for 6‚Äì7K AUD, and the Ada version is going for as much as 12K.  Instead of dropping prices on the older cards, they‚Äôre just marking up the new Blackwell ones way above MSRP. The server variant of this exact card is already sitting at 17K AUD (~11K USD)‚Äîabsolute piss take tbh. \nComment: Image and clip generation \nComment: I think I'll stream getting some LLMs and comfyui up tomorrow and the next few days. give a follow if you want to be notified https://twitch.tv/faustcircuits \nComment: Get that unsloth 235B Qwen3 model at Q2K_XL. It should fit. Q2 is the most efficient size when it comes to benchmark score to size ratio according to unsloths documentation. It should be fast AF too since only 22B active parameters.  \nComment: Nice! Still waiting for mine. Can you let me know if you are able to disable ECC or not? \nComment: what Audi is that? S4? \nComment: Nice. Run stuff and share stats! Would be cool to see. \nComment: https://preview.redd.it/ems9w2z6yvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=76b13f186be7cb783727c000bda533c92c1e8c56 here is the old card lol \n","Title: Visa is looking for vibe coders - thoughts?\nID: t3_1keolh9\nSummary:         submitted by    /u/eastwindtoday   [link]   [comments] \nImageDescription: ## Image Analysis: Visa \"Vibe Coder\" Job Requirements\n\nThe image presents a snippet of text outlining requirements for a position at Visa, seemingly related to \"Vibe Coding\" as indicated by the post title. Here's a detailed breakdown:\n\n**What is shown:**\n\n*   **Textual List:** The core content is a bullet-point list of desired skills and experience.\n*   **Logo:** A small, circular logo with the word \"technical\" is present in the bottom right corner. This likely indicates the source or department within Visa responsible for this role (e.g., a technical team).\n*   **Repetitive Text:** A large block of seemingly random characters (Unicode symbols) is repeated multiple times at the bottom. This appears to be filler or obfuscated text, potentially used for visual effect or as a placeholder.\n\n**Technical Details:**\n\n*   **Vector Databases:** Explicit mention of experience with vector databases ‚Äì specifically **ChromaDB and Conecone**. This is a key indicator. Vector databases are used for similarity search on embeddings, crucial in modern AI applications like recommendation systems, semantic search, and RAG (Retrieval-Augmented Generation).\n*   **Embeddings:** The requirement for experience with embedding models directly links to the vector database need.  These models transform data (text, images, etc.) into numerical vectors for efficient similarity comparisons.\n*   **Containerization:**  Experience with **Docker** is requested, signifying a need for reproducible and scalable deployments of AI/ML models.\n*   **Kubernetes:**  The inclusion of **Kubernetes** points to a production-level environment requiring orchestration and management of containerized applications.\n*   **Frontend Design Systems:**  Experience with **Lovable and V0** (likely referring to V0.dev) suggests a need for building user interfaces around AI-powered features, potentially involving component libraries and design systems.\n*   **Data Science Background:**  A \"big plus\" indicates that foundational data science knowledge is valuable.\n\n**Relation to Post Title (\"Visa is looking for vibe coders - thoughts?\"):**\n\nThe term \"vibe coding\" appears to be a colloquialism for building AI applications with a focus on user experience and rapid prototyping. The requirements strongly suggest this is *not* traditional software engineering.  The emphasis on frontend design systems, vector databases (for quick semantic search/response), and rapid prototyping tools like V0.dev supports this interpretation.  It's likely Visa is looking for engineers who can quickly build and iterate on AI-powered features, focusing on the *feel* of the application as much as its functionality.\n\n**Key Insights:**\n\n*   **RAG Focus:** The combination of vector databases and embedding models strongly suggests Visa is building applications leveraging Retrieval-Augmented Generation (RAG). This involves retrieving relevant information from a knowledge base to enhance the responses of large language models.\n*   **User-Centric AI:** The emphasis on frontend design systems and \"vibe coding\" indicates a focus on creating intuitive and engaging AI-powered user experiences.\n*   **Modern AI Stack:** The requirements showcase a modern AI stack including vector databases, containerization (Docker/Kubernetes), and rapid prototyping tools.\n*   **Potential Application Areas:**  Given Visa's business, potential applications could include fraud detection with explainable AI (using RAG to provide context), personalized financial recommendations, or intelligent customer support.\n\n\n\n\nComment:         submitted by    /u/eastwindtoday   [link]   [comments] \nComment: The way it's written with the other requirements makes me think that they want a programmer able to write vibe coding tools, not specifically hiring a vibe coder for programming. \nComment: \"Background in Data Science is a big plus\" - Be prepared to be payed less than a proper data scientist. \nComment: Dumb. Vibe coding is a bit ill-defined but what I see is a niche where people like product managers can use AI to build tiny tools that would be hard to get into a product team's priority list. In particular what I saw was using Zapier which is a user-friendly automation tool that lets you run snippets of Python or js inside workflows and now lets you generate them with AI. Great way to build little utilities without a developer but also very narrow, low stakes applications that aren't critical path. This job req wants familiarity with vector DBs and containers which means they want an actual ...\nComment: Vibe coding 2025 is same as slapping an untested, ill designed, 700-lines-a-function Python script on anything and calling it a day in the 2015s Edit: You scrolled down on some requirements tho https://www.visa.ca/fr_ca/jobs/REF061638W \u0026gt; Strong proficiency with Python, FastAPI and PostgreSQL for backend development. \nComment: These days, stupid HRs are stuffing the job requirements sections with as many tech buzzwords as possible, regardless of whether these buzzwords actually reflect the technologies used in the company. I feel like \"vibe coding\" may be one of those buzzwords. \nComment: now exactly nobody will know why credit card applications failed \nComment: Maybe we should all \"vibe-pay\" our Visa card bills. \nComment: Click click, sold all visa stocks instantly \nComment: I for one am happy my CC transactions are handled by vibed up systems. \nComment: they are starting up an ml intership for students next summer in poland too \nComment: I don't even know what to think. My general reaction of most job posts is to assume the companily already has someone they're gonna hire, and the position is only being posted as a formality and to make it appear that the company has fair hiring practices (even though requirements weed out people that are perfectly capable of being trained). \nComment: I know some folks working with such teams. They have ambitions to build vibe coding tools (like cursor) but without the dependency on letting data go out of the org (from a financial reg pov). Visa‚Äôs CTO is one of the highest paid CTOs in the US, and they are building agents replicating (gamma.ai, OpenAI deep research, Agentic Commerce) to start with. \nComment: They are even Vibe-using Caps. That's so Vibe. It gives me Bad Vibes. \nComment: That job posting is clear as mud. Title is an obvious mismatch for the skillset. \nComment: well I'm glad none of my credit cards are visa...  \nComment: what could possibly go wrong? \nComment: If they are listing the tech stacks that should be enough. \nComment: At least they're not requiring minimum 15 years of experience \nComment: Lmao \nComment: AI generated job posting ? :p \nComment: Vibe coding doesn't work. It takes me 10-20 prompts to generate anything useful with gemini 2.5.  Sometimes 30+ prompts even with explicit instructions. \nComment: And I thought it was a big deal 15 years ago when I talked NASA into letting me put ‚Äúauto-generated‚Äù guidance and control code into a satellite. That particular NASA center had never done such a thing, and I had to pop their cherry on it (as a contractor). Partly because a colleague of mine was doing the same thing on a bigger project, and he really wanted to be able to say his project wasn‚Äôt the first. The ‚Äúauto-generation‚Äù step is just compiling MatLab down into C code, and from there is just standard C compilation on the target environment. But the MatLab becomes the source code, ...\nComment: As someone already posted, these are not the full requirements, but another thing also:  Vibe-coding is expensive. Requests can quickly become $1-2 a piece. Over the course of a month, you could easily rack up $500+ if you're using it liberally. Now consider that you're already paying a regular salary, put this on top, and it becomes kind of unsustainable.  The money has to come from somewhere, and it's most probably the base pay. \nComment: I don't know much about developers as vibe coders. But, a lot of project managers \u0026amp; UI UX folks and product managers are starting to learn these tools (lovable, cursor, etc.,) and will roll out the basic screens for evaluation.  Also, this will start getting back to actual developers on justifying their timelines, complexity justifications, etc., This is mostly as nobody gives a fuck about code maintainability, reusability, tech debt, security, etc,.  Other side to this coin is 90% of code written by \u0026lt;5 years experienced devs are not following these standards anyways ü§∑‚Äç‚ôÇÔ∏è 90% o...\n","Title: UI-Tars-1.5 reasoning never fails to entertain me.\nID: t3_1keo3te\nSummary:       7B parameter computer use agent.    submitted by    /u/Impressive_Half_2819   [link]   [comments] \nImageDescription: ## Image Analysis: UI-Tars-1.5 Reasoning \u0026 LLM Interaction\n\nThe image depicts a screenshot of a user interface (UI) seemingly built to interact with a Large Language Model (LLM), likely within a macOS application environment. The UI is styled to resemble a pop-up window, possibly an extension or plugin.\n\n**Key Elements \u0026 Technical Details:**\n\n* **LLM Provider and Model Selection:** A prominent section allows the user to specify an LLM provider.  A dropdown menu is present, with \"Custom model...\" selected. Below this is a text input field labeled ‚ÄúCustom Model Name‚Äù where the string \"ByteDance-Seed/UI-Tars-1.5-7B\" is entered. This strongly suggests the user is attempting to utilize a custom LLM hosted or accessible via an API. The \"7B\" likely refers to the model size ‚Äì a 7 billion parameter LLM.\n* **UI-Tars Thoughts:** A text area displays the LLM's \"thoughts.\" The content reveals a conversational, almost stream-of-consciousness reasoning process.  The LLM is responding to an implicit prompt (likely related to Google) and expresses a desire to search for information (\"search for a repository\").  The use of the downward arrow emoji (‚Üì) suggests an intention to perform an action, possibly triggering a web search. This is indicative of the LLM having agency or access to tools.\n* **Text Input Box:** A text box labeled \"Ask me to perform tasks in a virtual macOS environment\" is present. This is crucial; it implies the LLM isn't just generating text but can *execute* commands within a simulated macOS environment. This is a significant capability, suggesting the LLM has access to APIs or tools that allow it to interact with an operating system.\n* **Provider Base URL:** A large, seemingly empty text field labeled \"Provider Base URL\" is visible. This likely specifies the API endpoint for accessing the LLM provider's services, crucial for authentication and communication. The field is filled with a long string of seemingly random characters ‚Äì potentially obfuscated or placeholder data.\n* **UI Style:** The UI is minimalist, with a clear focus on text interaction and model selection.  The use of rounded corners and a light background is typical of modern macOS applications.\n\n**Relation to Post Title (\"UI-Tars-1.5 reasoning never fails to entertain me\"):**\n\nThe post title directly references \"UI-Tars-1.5.\" The image confirms this is the name of the LLM being used.  The \"reasoning never fails to entertain me\" suggests the user is amused by the LLM's conversational style, its attempts at problem-solving (searching for a repository), and potentially its unexpected or quirky responses. The LLM's \"thoughts\" displayed in the text area support this ‚Äì it‚Äôs a verbose, self-aware reasoning process.\n\n**Key Insights:**\n\n* **Agentic LLM:** This is not a simple text completion model. The LLM exhibits agentic behavior ‚Äì it *plans* actions (searching), expresses intentions, and interacts with a virtual environment.\n* **Tool Use:** The LLM has access to tools (likely web search) and can utilize them to achieve its goals.\n* **Custom Model:** The user is leveraging a custom LLM, potentially fine-tuned for specific tasks or with unique capabilities.\n* **Virtual Environment Interaction:** The ability to perform tasks within a virtual macOS environment is a powerful feature, suggesting applications in automation, testing, or even security research.\n* **Potential for Complex Workflows:** The combination of reasoning, tool use, and environment interaction suggests this LLM can handle complex workflows beyond simple text generation.\n\n\n\n\nComment:       7B parameter computer use agent.    submitted by    /u/Impressive_Half_2819   [link]   [comments] \nComment: What's more important here is the model used - ByteDance-Seed/UI-TARS-1.5-7B the model which it is meant to be used with, so how did you make it work? Because last time I checked I haven't seen that model being converted to GGUF format, nor having vision support added into llama.cpp for it. \nComment: I guess: https://github.com/trycua/cua \nComment: When you train a model to use computers for humans and do the tiresome ToS reading, but it can't be bothered to do it either \nComment: Most probably trained on Gen-Z data. \nComment: https://preview.redd.it/4ignwtxrhuye1.jpeg?width=1079\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=e5d30270b51854c061aebdd502448897c63fed18 \nComment: On one hand, I guess I'd like the language model to read language on my behalf - on the other hand I wouldn't want the model to decide the cookies policy warrants user review or some other distraction so maybe skipping it is for the best after all. It does seem reading the pop-up falls within the scope of accessing the site to search for a repository \nComment: Try out yourself using cu/a! \nComment: I mean, fair \nComment: tiktok ai getting lazy \nComment: TARS, would you set your attention span setting to 8 for me? \nComment: It‚Äôs the defaut personality? \n","Title: Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)\nID: t3_1kepuli\nSummary:       Hey LocalLlama! We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon. We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device). Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist. We figured we'd kickstart this since we already maintain this benchmarking infra/tooling at RunLocal for our enterprise customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and TFLite to date, so a few things are still WIP for GGUF support.  Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill Tokens=512, Num Generation Tokens=128). GGUFs are from Unsloth üêê Qwen3 GGUF benchmarks on laptops Qwen3 GGUF benchmarks on phones You can see more of the benchmark data for Qwen3 here. We realize there are so many variables (devices, backends, etc.) that interpreting the data is currently harder than it should be. We'll work on that! You can also see benchmarks for a few other models here. If you want to see benchmarks for any others, feel free to request them and we‚Äôll try to publish ASAP! Lastly, you can run your own benchmarks on our devices for free (limited to some degree to avoid our devices melting!). This free/public version is a bit of a frankenstein fork of our enterprise product, so any benchmarks you run would be private to your account. But if there's interest, we can add a way for you to also publish them so that the public benchmarks aren‚Äôt bottlenecked by us.  It‚Äôs still very early days for us with this, so please let us know what would make it better/cooler for the community: https://edgemeter.runlocal.ai/public/pipelines To more on-device AI in production! üí™ https://i.redd.it/aev5rgjazsye1.gif    submitted by    /u/intofuture   [link]   [comments] \nImageDescription: ## Image Analysis: Qwen3 Performance Benchmarks\n\nThe image displays a screenshot of what appears to be a user interface (UI) for running and benchmarking the Qwen3 large language model (LLM) across a diverse set of devices. The UI is dark-themed with a card-based layout, likely from an application or web service designed for LLM inference.\n\n**Key UI Elements \u0026 Technical Details:**\n\n* **Model Selection:** Several Qwen3 model variants are listed, including:\n    * `DepthAnythingV2`: Suggests integration with a depth estimation model alongside Qwen3, potentially for multimodal applications.\n    * `Depth2Img-7B-08_0_gpu`: A 7 billion parameter Qwen3 variant optimized for image generation, running on a GPU. The \"08_0\" likely refers to a specific revision or training checkpoint.\n    * `Llama-3_2-1B-Instruct-08_0_gpu`: A 1 billion parameter Llama-3 variant, also GPU accelerated.  The \"Instruct\" suffix indicates fine-tuning for instruction following tasks.\n    * `Kokoro-82M`: A smaller 82 million parameter model, likely for faster inference on resource-constrained devices.\n* **Benchmark Metrics:** Each model card displays several key metrics:\n    * `toks/s`: Tokens per second ‚Äì a primary measure of inference speed.  Values are visible (e.g., 1.08 for Llama-3_2-1B).\n    * `RAM`: RAM utilization in MB.  Values are visible (e.g., 62MB for Llama-3_2-1B).\n    * `Text Generation`: Indicates the model is capable of text generation.\n    * `Depth Estimation` (for DepthAnythingV2): Indicates capability for depth estimation tasks.\n    * `View Pipeline`: A button to access detailed pipeline information, likely including hardware utilization and other performance data.\n* **Device Coverage:** The post title mentions ~50 devices (iOS, Android, Mac, Windows). While not directly visible in the screenshot, the UI is designed to support a wide range of platforms.\n* **Model Versions:** The inclusion of version numbers (e.g., \"08_0\") suggests the UI tracks and allows benchmarking of different model revisions, facilitating performance comparisons.\n\n**Relation to Post Title:**\n\nThe image directly illustrates the core content of the post: performance benchmarks for Qwen3 models. The UI provides a clear and organized way to compare the speed (toks/s) and resource usage (RAM) of different Qwen3 variants across a variety of devices. The mention of iOS, Android, Mac and Windows in the title is reflected by the UI's likely cross-platform nature.\n\n**Key Insights:**\n\n* **Scalability:** The availability of models ranging from 82M to 7B parameters suggests a focus on scalability, allowing users to choose models based on their hardware constraints and performance requirements.\n* **GPU Acceleration:** The inclusion of \"_gpu\" in some model names indicates that GPU acceleration is a key optimization strategy.\n* **Performance Trade-offs:** The UI allows for direct comparison of the trade-off between model size, inference speed (toks/s), and RAM usage.  Smaller models like Kokoro-82M likely offer faster inference but lower quality, while larger models provide better results at the cost of increased resource consumption.\n* **Ongoing Development:** The version numbers suggest active development and refinement of the Qwen3 models.\n\n\n\n\nComment:       Hey LocalLlama! We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon. We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device). Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist. We figured we'd kickstart this since we alr...\nComment: Iphone 16's Metal performance is pretty impressive for 1.6b-q8. But I do wonder why q8's performance is faster than q4 in that particular setup. \nComment: It‚Äôs interesting to see that performance in m4 is pretty similar in both cpu and gpu \nComment: There's one edge factor you missed - on Metal backend when you get OOM you get completely wrong results. For example on Qwen3 8B Q4 your results are like this: - MacBook Pro M1, 8GB = 99232.83tok/s prefill, 2133.70tok/s generation - MacBook Pro M3, 8GB = 90508.66tok/s prefill, 2507.50tok/s generation If you wouldn't get OOM the correct results for that model should be around ~100-150tok/s prefill and ~10tok/s generation. Additionally, all results for RAM usage on Apple silicon \u0026amp; Metal are not correct. In terms of your UX/UI there's tons of stuff that should be improved. but to not make thi...\nComment: How to run on metal on iphone 16 pro? I have pocketpal app and how to switch from cpu to metal? \nComment: if i'm reading this correctly the load time on cpu is better than gpu/metal for macbook pro but the gpu/metal is less memory intensive? also metal perf on iphone 16 is pretty impressive. \nComment: How do I run this on Android? Rn it just crashes \nComment: Why is Q8 faster than Q4??? \nComment: For laptops, is vulkan using the igpu ? \nComment: according to this data on iphone 16 you have 24 t/s on Q8 and 22 t/s on Q4 why so tiny models? \nComment: The iPhone 16e is listed to have the A18 Pro SoC but it actually has the A18.  https://preview.redd.it/h1dx2hgphvye1.png?width=623\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c2f8794bf27c6a9042b81040ebefa09873eae989 \n","Title: Qwen 30B A3B performance degradation with KV quantization\nID: t3_1kewkno\nSummary: I came across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4 that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization. I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens. I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens without finding any solution. Then I saw that sunpazed didn't used KV quantization and tried the same: boom! First time right. It worked with Q5_K_S and also with Q4_K_XL For who wants more details I leave here a gist https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef Do you have any report of performance degradation with long generations on Qwen3 30B A3B and KV quantization?    submitted by    /u/fakezeta   [link]   [comments]\nImageDescription: \nComment: I came across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4 that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization. I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens. I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens wit...\nComment: What KV quant level were you using? IMO on llama.cpp you shouldn‚Äôt push it past Q8_0. Q4_0 cache quant tanks quality in any model and especially models that heavily leverage GQA.  \nComment: I have one rule: I always test ALL new models without flash attention and with full 16bit KV cache. \nComment: Which KV quantization are you using? Don't have time to run this test right now, but I usually use -ctk q8_0 -ctv q5_1 (requires -DGGML_CUDA_FA_ALL_QUANTS=on) \nComment: Ive only tried KV quantization once and saw that any amount of it makes models super dumb. Not sure why anybody uses it tbh \nComment: Which quantization did you use initially? \nComment: Interested here since I'm running a q6 \nComment: Use these parameters: Thinking Mode Settings: Temperature = 0.6 Min_P = 0.0 Top_P = 0.95 TopK = 20 Non-Thinking Mode Settings: Temperature = 0.7  Min_P = 0.0 Top_P = 0.8 TopK = 20 \nComment: Could you please tell us how to disable KV cache quantisation? I'd also like to check the difference. What is the difference in the amount of memory used with KV running at fp16 in comparison with regular q4? \nComment: I‚Äôm confused. Isn‚Äôt K_M KV quantization? And yet you said Qwen 30b solved the rest with Q4 K_M? \nComment: Of course  Cache should always be fp16 even Q8 has degradation. Only flash attention is ok...ish ( as is fp16 ) \n","Title: QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.\nID: t3_1kenk4f\nSummary:       All models are from Bartowski - q4km version Test only HTML frontend. My assessment lauout quality from 0 to 10 Prompt \"Generate a beautiful website for Steve's pc repair using a single html script.\" QwQ 32b - 3/10 - poor layout but ..works , very basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b Qwen 3 32b - 6/10 - much better looks but still not too complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b272399ba8140785feb429a196ecc5173 GLM-4-32b 9/10 - looks insanely good , quality layout like sonnet 3.7 easily - 1500+ code lines https://preview.redd.it/3zj2lr2ahsye1.png?width=2469\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964 GLM-4-32b is insanely good for html code frontend. I say that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most. Other coding language like python , c , c++ or any other quality of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on the seme level but for html and JavaScript ... is GREAT.    submitted by    /u/Healthy-Nebula-3603   [link]   [comments] \nImageDescription: \nComment:       All models are from Bartowski - q4km version Test only HTML frontend. My assessment lauout quality from 0 to 10 Prompt \"Generate a beautiful website for Steve's pc repair using a single html script.\" QwQ 32b - 3/10 - poor layout but ..works , very basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b Qwen 3 32b - 6/10 - much better looks but still not too complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b2...\nComment: Yeah, I have also tried to generate webpages with a couple of models, like GLM-4, Qwen3, Phi-4 Reasoning, etc. GLM-4 is so far the clear winner at these tasks. It's a gem in my model collection. \nComment: I've created a list of one-shot generated HTML pages using different models, big and small. https://blog.kekepower.com/ai/ \nComment: Ironically, so far userscript (javascript): Qwen 3 32B \u0026gt; GLM-4-0414. Don't get me wrong. I love GLM-4-0414, but it feels like it lacked the required understanding for my particular requests that Qwen 3 32B understood well. \nComment: Whats the temp? Did you rerun multiple times? \nComment: Why not try a slightly more complex task? E.g. a mini-game?  Create a single-HTML-page game using Babylon.js where the player controls a ship and moves about the open sea exploring islands to find treasure. A single, small map with 3 islands of which only one has the treasure is enough  In the first reply, the camera works, but WASD didn't. I copied-pasted the errors from the console a couple times and WASD works now. It looks terrible, but I guess that's expected without external assets. https://preview.redd.it/lf5foomq1tye1.png?width=1288\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b02ed407b3fbef31bf...\nComment: GLM falls flat on its face when I try to continue developing after the first prompt. It feels like a model trained (very well) for one-shots \nComment: Try UIGEN-T2 for html generation! There's also a react model.  https://huggingface.co/Tesslate/UIGEN-T2 \nComment: Would be interesting to see how these results compare to the recent Tesslate/UIGEN-T2-7B. It's a tuned version of Qwen 2.5 Coder 7B specifically for UI generation. \nComment: What quants what context window size? Ollama default size will kill QWQ reasoning if you don‚Äôt know how to set it up properly. \nComment: Still kind of freaked out that smaller Qwen 3 models are probably as good at website development as I was as a teenager. And a damn sight quicker too. \nComment: Thanks for posting the eval. Would be curious to see the prompt used as well. \nComment: Interesting. I ran the same input on QwQ with these settings: Temp: 0.6 Top p: 0.95 Min p: 0.0 Top k: 40 And quite a bit different output. Output: https://pastebin.com/Ntc8QQfH \nComment: GPT 4.1 is clearly the winner here in my opinion as well as claude sonnet 3.7 \n","Title: LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!\nID: t3_1keoint\nSummary:       You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B) mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert) tl;dr; I highly recommend doing a git pull and re-building your ik_llama.cpp or llama.cpp repo to take advantage of recent major performance improvements just released. The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving r/LocalLLaMA community! If you have enough VRAM to fully offload and already have an existing \"normal\" quant of Qwen3 MoE then you'll get a little more speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork! Details I spent yesterday compiling and running benhmarks on the newest versions of both ik_llama.cpp and mainline llama.cpp. For those that don't know, ikawrakow was an early contributor to mainline llama.cpp working on important features that have since trickled down into ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my understanding) the ik_llama.cpp fork was built and has a number of interesting features including SotA iqN_k quantizations that pack in a lot of quality for the size while retaining good speed performance. (These new quants are not available in ollma, lmstudio, koboldcpp, etc.) A few recent PRs made by ikawrakow to ik_llama.cpp and by JohannesGaessler to mainline have boosted performance across the board and especially on CUDA with Flash Attention implementations for Grouped Query Attention (GQA) models and also Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B releases! References  ikawrakow/ik_llama.cpp/pull/370     submitted by    /u/VoidAlchemy   [link]   [comments] \nImageDescription: \nComment:       You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B) mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert) tl;dr; I highly recommend doing a git pull and re-building your ik_llama.cpp or llama.cpp repo to take advantage of recent major performance improvements just released. The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving r/LocalLLaMA community! If you have enough VRAM to fully offload and already have an existing \"normal\" quant of Qwen3 MoE then you'll get ...\nComment: I'm currently running ik_llama.cpp with Qwen3-235B-A22 on a Xeon E5-2680v4, that's a 10 year old CPU with 128GB ddr4 memory, and a single RTX3090. I'm getting 7 tok/s generation, very usable if you don't use reasoning. BTW the server is multi-GPU but ik_llama.cpp just crash trying to use multiple-gpus, but I don't think it would improve speed a lot, as the CPU is always the bottleneck. \nComment: Could you explain how to read your pictures? I see orange plot below red plot, so ik_llama.cpp is slower than llama.cpp? \nComment: Can you post some of the commands you use for the benchmarks? I want to tinker to see what is best for my use case \nComment: Oh, just updated. My rig is busy for running deepseek \u0026amp; ik_llama (1 week jobs). I will update after that :) \nComment: Maybe GGUF will now give same speed as MLX on Mac devices \nComment: I have a 3090. Doesn't this say it's slower, not faster? \nComment: https://preview.redd.it/0zroyhg1qsye1.png?width=3404\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b3e55128b1aac3f6d2ddfbd22597b9cd6d7dd02c In my limited testing you probably want to go with ik_llama.cpp for fully offloaded non-MoE models like the recent GLM-4 which is crazy efficient on kv-cache VRAM usage due to its GQA design. \nComment: I just pulled and rebuilt and I'm now actually going about 15 tps slower. My previous build was from about a week ago, and I was getting an eval time of about 54 tps. Now I'm only getting 39 tokens per second, so pretty significant drop. I just downloaded the latest unsloth model I'm running on 2 3090s, using this command: ``` .\\bin\\Release\\llama-server.exe -m C:\\shared-drive\\llm_models\\unsloth-2-Qwen3-30B-A3B-128K-Q8_0.gguf --host 0.0.0.0 --ctx-size 50000 --n-predict 10000 --jinja --tensor-split 14,14 --top_k 20 --min_p 0.0 --top_p 0.8 --flash-attn --n-gpu-layers 9999 --threads 24 ``` Prompt:...\nComment: How close is llamacpp to vLLM and exllama now? \nComment: Seems like it is related to CUDA only, so I guess only for people with Nvidia cards and not folks on Apple Silicon and others. \n","Title: Apparently shipping AI platforms is a thing now as per this post from the Qwen X account\nID: t3_1kebauw\nSummary:         submitted by    /u/MushroomGecko   [link]   [comments] \nImageDescription: ## Image Analysis: Qwen AI Platform Shipping Announcement\n\nThe image is a promotional graphic, likely intended for social media (given the cartoon style and celebratory elements), announcing the shipping of a Qwen AI platform. It features two anthropomorphic animal characters: a sloth and a bear, both appearing to be mascots. The bear is presenting a flower to the sloth.\n\n**Key Visual Elements \u0026 Technical Implications:**\n\n* **Mascots:** The use of mascots suggests a focus on accessibility and user-friendliness.  This is common in platforms aiming for broader adoption, particularly those targeting developers or non-technical users.\n* **\"Qwen\" Branding:** While not explicitly stated in the image, the post title confirms this is related to Qwen AI.  Qwen (ÈòøÈáåÂ∑¥Â∑¥ÈÄö‰πâÂçÉÈóÆ) is a large language model developed by Alibaba, known for its open-source releases and strong performance in Chinese NLP tasks.\n* **Shipping Implication:** The celebratory nature (flower presentation, bright colors) strongly implies a product release or significant update is being shipped.  This could refer to:\n    * **Model Weights:** The shipping of updated model weights for Qwen, allowing users to download and deploy the latest version.\n    * **Deployment Infrastructure:**  A packaged deployment solution (e.g., Docker images, cloud integrations) making it easier to run Qwen models.\n    * **SDK/API Release:**  New or updated Software Development Kits (SDKs) and Application Programming Interfaces (APIs) for interacting with Qwen models.\n    * **Platform/Tooling:** A complete platform or suite of tools built around Qwen, simplifying model fine-tuning, evaluation, and serving.\n\n**Insights:**\n\n* **Focus on User Experience:** The cartoon mascots suggest a deliberate effort to make Qwen approachable and easy to use.  This is crucial for attracting developers and fostering a community around the model.\n* **Commercialization/Accessibility:**  \"Shipping\" implies more than just open-source availability. It points towards a commercially viable product or easily deployable solution, potentially through cloud services or packaged software.\n* **Competitive Landscape:**  The announcement suggests Qwen is actively competing in the rapidly evolving LLM space, alongside models like Llama 2, Mistral, and others.  Shipping a platform is a key step in gaining market share.\n\n\n\nWithout further context (e.g., accompanying text, links), it's difficult to pinpoint the exact nature of what is being shipped. However, the image clearly communicates a positive update related to Qwen AI and its accessibility for users.\nComment:         submitted by    /u/MushroomGecko   [link]   [comments] \nComment: Ah yes, the AI romcom. \nComment: theres lore now? \nComment: -Ok, lets see, i want to make love to you-. -But...wait. no no \nComment: What do you mean by \"shipping AI platforms\"? \nComment: Can't wait for the competitions to see which model writes the best fan fiction \nComment: Should AI companies also create their own VTuber characters that use their models? \nComment: Qwen guy looks mean. His expression is like he's never felt love. \nComment: I ship it! \nComment: And those two ship often \nComment: Hell yeah \nComment: Has anyone noticed better performance for similar quants of the same model by different creators? \nComment: Oh no \nComment: Love the contribution to the OSS these guys make!! Pure awesomeness \nComment: ü§¶üèª‚Äç‚ôÇÔ∏è \nComment: Anyone feeding into this just entirely lacks a social life, why are some of you like this? \n","Title: IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models\nID: t3_1kedu0d\nSummary:         submitted by    /u/ab2377   [link]   [comments] \nImageDescription: \nComment:   \n                \n            submitted by   \n            /u/ab2377   [link]\n              [comments]\n            \nComment: Hope they\n            can release a larger one like 30b-a3b  \nComment: so a new\n            architecture, more moe goodness  \"Whereas prior generations\n            of Granite LLMs utilized a conventional transformer architecture, all models in the\n            Granite 4.0 family utilize a new hybrid Mamba-2/Transformer architecture, marrying the\n            speed and efficiency of Mamba with the precision of transformer-based self-attention.\n            Granite 4.0 Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts\n            (MoE) model, with 7B total parameters and only 1B active parameters at inference\n            time. Many of the innovat...\nComment: Please\n            look here: https://huggingface.co/ibm-granite/granite-4.0-tiny-preview/discussions/2\n            gabegoodhart IBM\n            Granite org 1\n            day ago Since this model is hot-off-the-press, we\n            don't have inference support in llama.cpp yet.\n            I'm actively working on it, but since this is one of the first major models\n            using a hybrid-recurrent architecture, there are a number of in-flight architectural\n            changes in the codebase that need to all meet up to get this supported. We'll\n            keep you posted! gabegoodhart IBM...\nComment: i hope we\n            can see some larger models too! I really want them to scale those more experimental\n            architectures and see where it leads. I think there is huge potential in combining\n            attention with hidden state models. attention to understand context, hidden state to\n            think ahead, remember key information etc. \nComment: Read the\n            full thing. It‚Äôs worth it. \nComment: Holy,\n            this actually looks really good. IBM might actually be able to catch up with Alibaba\n            with this one. \nComment: Neat but\n            unless folks really start working to help add support for mamba architectures to\n            llama.cpp it'll be dead on arrival. It would be great to see\n            the folks at /u/IBM step up and help out\n            llama.cpp to support things like this. \nComment: \n            The Granite 4.0 architecture uses no positional encoding (NoPE). Our testing\n            demonstrates convincingly that this has had no adverse effect on long-context\n            performance.   This is interesting. Are there any\n            papers that explain why this still works? \nComment: Looking\n            very promising... \nComment: Is IBM\n            going to be the silent winner? It‚Äôs impressive that their tiny model is 8b MOE and\n            likely to perform at the same level as their previous dense 8b: Granite 4.0\n            Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts (MoE) model,\n            with 7B total parameters and only 1B active parameters at inference time.\n            I hope their efforts attempt to improve in https://fiction.live/stories/Fiction-liveBench-April-6-2025/oQdzQvKHw8JyXbN87\n            and not just passkey testing. \nComment: I'm just a dreamer without much background in\n            ML stuff. Can anyone with sense comment on how likely it is we'll ever see\n            something that might be so efficient it'll run well on CPU? I mean, this model\n            already sounds pretty exciting from an efficiency perspective. Wondering if\n            we've exhausted architectural changes that would e.g. reduce memory bandwidth\n            requirements \nComment: ‚ÄúWe‚Äôre\n            excited to continue pre-training Granite 4.0 Tiny, given such promising results so early\n            in the process. We‚Äôre also excited to apply our learnings from post-training Granite\n            3.3, particularly with regard to reasoning capabilities and complex instruction\n            following, to the new models. Like its predecessors in Granite 3.2 and Granite 3.3,\n            Granite 4.0 Tiny Preview offers toggleablethinking on andthinking off functionality\n            (though its reasoning-focused post-training is very much incomplete).‚Äù\n            I hope s...\nComment: Now if\n            only we could get IBM to sell a version of their AI card to the public\n            \nComment: ibm doing\n            better work than meta theyre surprisingly becoming a big player in open source (for\n            small models) \nComment: I wonder\n            what is prompt processing speed for semi-recurrent stuff compared to transformers.\n            Transformers have fantastic prompt processing speed like 1000t/s easy even on crap like\n            3060, but slow down during token generation as context grows. This seems to be the other\n            way around, slow PP but fast TG. I might be completely\n            wrong. \nComment: Large\n            datasets: all of Harry Potter series asking questions like, what would have to change in\n            the series for Harry to end up with Hermione or for Voldemort to win. It‚Äôs a series\n            everyone knows fairly well and requires details in the story and the story\n            whole. \nComment: I\n            remember seeing this model a few days ago. There's no gguf so i cant try it out.\n            I guess there's not a lot of interest in this moe or it's not currently\n            possibly to make ggufs for it at the moment.  Webui stopped working\n            for me last year after i updated it and I've never been able to get it working\n            right since then, so been using lm studio appimages. That program runs everything good\n            for me but only runs ggufs. \nComment: Yall need\n            to learn Transformers and stop hating on llama.cpp \n"],"results":[{"Title":"Qwen3 Performance Benchmarks Across 50+ Devices (iOS, Android, Mac, Windows)","ID":"t3_1kepuli","Summary":"LocalLlama has launched an open-source initiative to benchmark Qwen3's performance across ~50 devices (iOS, Android, Mac, Windows), focusing on metrics like tokens per second (toks/s) and RAM utilization. The project aims to address the critical need for standardized performance data to ensure viable on-device AI deployment. Key technical details include benchmarks for Qwen3 variants like 'DepthAnythingV2' (multimodal), 'Llama-3_2-1B-Instruct' (instruction-tuned), and 'Kokoro-82M' (lightweight). For example, the 7B-parameter 'Depth2Img-7B-08_0_gpu' achieves ~1.08 toks/s on GPU-accelerated devices, while the 82M-parameter 'Kokoro-82M' uses only 62MB RAM. The initiative highlights trade-offs between model size, speed, and resource usage, with plans to expand to 100+ devices. Notably, the UI allows comparing GPU vs CPU performance (e.g., similar M4 results) and reveals anomalies like Q8 quantization outperforming Q4 on iPhones, though community feedback points to potential OOM (out-of-memory) issues affecting accuracy. The project also enables users to run free benchmarks on supported devices, with future plans for public contribution of results.","CommentSummary":"The community is actively engaging with the benchmarks, raising technical questions about model quantization (e.g., why Q8 outperforms Q4 on iPhones) and hardware-specific quirks like Metal backend OOM errors causing incorrect results. Users report mixed experiences: some praise the iPhone 16 Pro's Metal performance, while others note crashes on Android. Discussions highlight practical concerns, such as switching to Metal backend on iPhones via PocketPal and Vulkan's iGPU usage on laptops. Criticisms include inaccurate RAM metrics on Apple silicon and requests for UI improvements. Despite this, the project is seen as a valuable step toward standardized on-device AI metrics, with users eager for more model support and transparency in benchmarking methodologies.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/","Relevance":"","IsRelevant":true},{"Title":"IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models","ID":"t3_1kedu0d","Summary":"IBM's Granite 4.0 Tiny Preview introduces a groundbreaking hybrid Mamba-2/Transformer architecture, combining the efficiency of Mamba's state-space models with transformer-based self-attention. The model is a fine-grained mixture of experts (MoE) with 7B total parameters, but only 1B active during inference, achieving significant computational efficiency. Notably, it employs 'No Positional Encoding' (NoPE), which IBM claims maintains long-context performance without traditional positional embeddings‚Äîa departure from standard transformer designs. The architecture's hybrid nature aims to balance speed and precision, with early tests suggesting it could match or exceed previous dense 8B models like Granite 3.3 while reducing resource demands. This represents a major shift in LLM design, potentially enabling more efficient deployment on edge devices or resource-constrained systems.","CommentSummary":"The community is energized by the technical innovation but cautious about practical adoption. Many users highlight the potential of combining Mamba's efficiency with transformers' precision, with one noting, 'This could be a game-changer for real-time applications.' However, concerns about tooling support dominate discussions‚Äîseveral commenters stress that without llama.cpp compatibility, the model's utility is limited. Others question the lack of public benchmarks and ask for clarification on NoPE's implementation details. While some speculate IBM could 'catch up with Alibaba,' others urge the team to prioritize open-source collaboration and provide clearer performance metrics. The conversation underscores a tension between cutting-edge research and real-world usability.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/","Relevance":"","IsRelevant":true},{"Title":"LLaMA Gotta Go Fast! Llama.cpp and ik_llama.cpp Performance Boosts for Qwen3 MoE","ID":"t3_1keoint","Summary":"A major performance overhaul has hit both mainline llama.cpp and the ik_llama.cpp fork, particularly benefiting Qwen3 MoE models (235B and 30B) through CUDA optimizations, Flash Attention implementations, and novel quantization techniques. The updates enable faster inference for fully offloaded MoE models and hybrid CPU+GPU setups, with ik_llama.cpp introducing SotA 'iqN_k' quantizations not available in mainstream tools like Ollama or LMStudio. Key technical advancements include Grouped Query Attention (GQA) optimizations, reduced VRAM usage for GQA-designed models like GLM-4, and improved speed for single-expert MoE configurations. Users with sufficient VRAM report measurable gains, while hybrid setups gain access to more efficient quantization schemes. The competition between forks is driving rapid innovation for GGUF-format model enthusiasts.","CommentSummary":"The community is split between excitement over performance gains and confusion about conflicting benchmark results. Some users report significant speed improvements, while others note regressions (e.g., 15 TPS drop after updates). Discussions highlight practical challenges like multi-GPU crashes in ik_llama.cpp and CUDA dependency concerns for Apple Silicon users. Technical curiosity drives requests for benchmark commands and clarification on quantization tradeoffs. A recurring theme is the value of open-source competition, with users praising the 'delicious fruit' of collaborative innovation. Concerns about documentation gaps and compatibility remain, but overall enthusiasm for GGUF ecosystem advancements is strong.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/","Relevance":"","IsRelevant":true},{"Title":"Qwen AI Platform Shipping Announcement Sparks Community Speculation","ID":"t3_1kebauw","Summary":"The image analysis reveals a promotional graphic from the Qwen X account announcing the 'shipping' of an AI platform, featuring anthropomorphic mascots and celebratory visuals. While the exact nature of the 'shipping' remains unclear, possibilities include model weight releases, deployment tools, SDKs, or a full platform. Qwen, Alibaba's large language model, is known for its open-source contributions and strong Chinese NLP performance. The use of mascots suggests a focus on accessibility, while the 'shipping' terminology implies a commercial or user-friendly product release. However, without additional context, the technical specifics of this announcement remain speculative. The post highlights Qwen's competitive positioning in the LLM space but lacks concrete details about model architecture, performance metrics, or novel techniques.","CommentSummary":"The comments reflect a mix of humor and confusion about the 'shipping' terminology, with users joking about AI romcoms, VTubers, and fan fiction competitions. Some questioned the literal meaning of 'shipping AI platforms,' while others praised Qwen's open-source contributions. The community's focus on playful speculation rather than technical analysis suggests the post served more as a promotional tease than a substantive technical update. A few users noted the lack of concrete details, with one sarcastically questioning the social life of those engaged in the discussion.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kebauw/apparently_shipping_ai_platforms_is_a_thing_now/","Relevance":"","IsRelevant":false},{"Title":"Qwen 30B A3B Performance Degradation with KV Quantization","ID":"t3_1kewkno","Summary":"A user reported significant performance issues with Qwen 30B A3B when using KV quantization during long-generation tasks, particularly in solving the OpenAI cypher test. The post details attempts to replicate results from a gist where Qwen 30B succeeded with Q4_K_M quantization, but the user encountered repetition loops and incorrect conclusions when using Unsloth's Q4_K_XL and Bartowski's Q5_K_S quantizations. Disabling KV quantization (by using fp16 cache) resolved the issues, suggesting a critical dependency on KV cache precision. The discussion highlights technical challenges in quantization strategies for large language models, particularly with architectures like Qwen that may rely heavily on GQA (Grouped Query Attention). The user's findings align with community concerns about KV quantization degrading model quality, especially in complex reasoning tasks.","CommentSummary":"The community emphasized the risks of aggressive KV quantization, with users warning that Q4_0 cache quantization severely impacts quality, particularly for models using GQA. Several commenters recommended avoiding KV quantization altogether or limiting it to Q8_0, while others stressed the importance of fp16 KV caches for stability. A debate emerged about the necessity of KV quantization, with some calling it 'useless' and others noting its memory-saving benefits. The discussion also clarified terminology, as some users confused Q4_K_M with KV quantization, despite the original gist explicitly avoiding it. Practical advice included using specific llama.cpp parameters (-ctk q8_0 -ctv q5_1) and adjusting sampling settings (temperature, top_p) to mitigate issues.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/","Relevance":"","IsRelevant":true},{"Title":"QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML Coding Benchmark","ID":"t3_1kenk4f","Summary":"This post presents a technical comparison of three large language models (LLMs) ‚Äî QwQ 32b, Qwen 3 32b, and GLM-4-32B ‚Äî focused on their ability to generate HTML code for a 'Steve's PC Repair' website. The evaluation uses a standardized prompt and assesses output quality on a 0-10 scale, along with code complexity metrics. GLM-4-32B achieves the highest score (9/10) with a 1,500+ line output featuring 'sonnet 3.7-level' aesthetics, while QwQ 32b scores poorly (3/10) with minimalistic code. The analysis highlights significant differences in frontend development capabilities, with GLM-4-32B demonstrating superior HTML/CSS generation but potential limitations in other domains. Technical details include model quantization (q4km), code line counts, and visual comparisons via PNG screenshots.","CommentSummary":"The community emphasizes GLM-4-32B's exceptional HTML generation skills, with users calling it a 'gem' for frontend tasks. However, discussions reveal mixed experiences: some note GLM-4's struggles with iterative development and complex tasks like Babylon.js games, while others praise Qwen 3's consistency. Technical debates focus on quantization impacts (q4km), context window settings, and model-specific strengths. A Hugging Face repository for UI generation models is shared, and users speculate about the role of specialized training data. Several commenters question the absence of temperature/parameter details and request comparisons with GPT-4 and Claude 3.7.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/","Relevance":"","IsRelevant":true},{"Title":"UI-Tars-1.5 Reasoning Never Fails to Entertain Me","ID":"t3_1keo3te","Summary":"The post showcases a custom UI for interacting with the 'ByteDance-Seed/UI-Tars-1.5-7B' model, a 7-billion-parameter language model capable of agentic behavior. The interface allows users to specify LLM providers, execute tasks in a virtual macOS environment, and leverage tools like web search. Key technical details include the model's ability to generate reasoning workflows (e.g., 'search for a repository') and its integration with a simulated OS. The UI's 'Provider Base URL' field suggests API-level access, while the 'Custom Model Name' input confirms the use of a non-standard LLM. This setup implies advanced capabilities beyond traditional text generation, such as automation and environment interaction. The model's 7B parameter count places it in the mid-sized LLM category, balancing performance and resource efficiency. The community's fascination stems from its apparent agency and the potential for complex workflows, though questions remain about compatibility with tools like llama.cpp.","CommentSummary":"The comments highlight curiosity about the model's implementation, with users questioning how it works given the lack of GGUF format support or vision capabilities in llama.cpp. Some reference a GitHub project (cua) for interaction, while others joke about the model's laziness or 'Gen-Z data' training. Discussions oscillate between technical skepticism and enthusiasm for its agentic features. A recurring theme is the tension between automation benefits (e.g., reading ToS) and risks (e.g., overstepping user intent). The mention of 'TARS' evokes both admiration for its capabilities and humor about its personality quirks.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/","Relevance":"","IsRelevant":true},{"Title":"Visa's 'Vibe Coder' Job Postings Spark Debate Over AI-Driven Development Trends","ID":"t3_1keolh9","Summary":"Visa's job posting for 'vibe coders' has sparked curiosity and skepticism, blending traditional software engineering requirements with modern AI/ML tools. The role explicitly demands expertise in vector databases like ChromaDB and Conecone, embedding models, Docker/Kubernetes containerization, and frontend design systems (Lovable/V0.dev). This suggests Visa is targeting engineers capable of building AI-powered applications with a focus on rapid prototyping and user-centric design. The mention of Retrieval-Augmented Generation (RAG) implies potential use cases in fraud detection, personalized financial services, or intelligent customer support. However, the term 'vibe coding' remains ambiguously defined, leading to speculation about whether it refers to low-code AI tools (e.g., Cursor, Zapier) or a niche role for product managers to create lightweight AI utilities. The technical stack aligns with modern AI infrastructure, but the job's emphasis on 'vibe' over traditional coding raises questions about Visa's approach to balancing innovation with maintainability, security, and scalability.","CommentSummary":"The community is divided on the 'vibe coder' concept, with many questioning its validity as a distinct role. Some argue it may be HR jargon masking traditional software engineering needs, while others see it as a legitimate push toward AI-augmented development. Critics highlight concerns about underpaying for data science roles and the sustainability of 'vibe coding' workflows, which they compare to chaotic 2015-era scripting. Others note the irony of Visa's CTO investing in AI agents while posting vague job requirements. A recurring theme is skepticism about the practicality of 'vibe coding'‚Äîwith one user comparing it to 10-30 prompts in Gemini 2.5 yielding minimal results. Despite the criticism, some acknowledge the technical rigor of the stack (e.g., PostgreSQL, FastAPI) and speculate about Visa's broader AI ambitions.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/","Relevance":"","IsRelevant":true},{"Title":"New GPU Acquisition and Testing Discussions","ID":"t3_1kexdgy","Summary":"A Reddit user shared their excitement about receiving a high-end GPU, sparking a discussion about potential AI workloads and model testing. While the post itself is a personal update, the comments reveal technical curiosity around running large language models (LLMs) like Llama 3.2, Qwen 3, and others. The conversation touches on hardware specifications (e.g., 'RTX Pro 6000 with 96GB VRAM'), software requirements (CUDA, PyTorch), and benchmarking interests. However, the post lacks specific technical details about the GPU model, performance metrics, or novel infrastructure developments. The community suggestions focus on testing strategies rather than announcing new AI technologies.","CommentSummary":"The comments reflect a mix of enthusiasm and technical curiosity. Users suggested running specific LLMs (e.g., 'Llama 3.3 70b at 8-bit') and benchmarking tools, while others questioned hardware capabilities (e.g., 'Can it run Crysis?'). There were discussions about VRAM constraints, model efficiency (e.g., 'Q2K_XL for 235B Qwen3'), and even humorous takes ('Hello Kitty Island Adventures'). Some users expressed concern about pricing and availability of high-end GPUs, highlighting market challenges. The conversation lacks substantive technical analysis of new AI developments, instead focusing on personal testing plans.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/","Relevance":"","IsRelevant":false}],"persona":"LocalLLaMa"}