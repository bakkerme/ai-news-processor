{"raw_input":["Title: What do I test out / run first?\nID: t3_1kexdgy\nSummary:       Just got her in the mail. Haven't had a chance to put her in yet.    submitted by    /u/Recurrents   [link]   [comments] \nImageDescription: \nComment:       Just got her in the mail. Haven't had a chance to put her in yet.    submitted by    /u/Recurrents   [link]   [comments] \nComment: First run home. Preferably safely. \nComment: llama 3.2 1b \nComment: Bro is loaded. How many kidneys did you sell for that?! \nComment: LLAMA 405B Q.000016 \nComment: all the new qwen 3 models \nComment: sexy ass card \nComment: Old School Runescape \nComment: Are they selling those already? \nComment: Download cuda and make sure your pytorch is the cuda version \nComment: Can it run Crysis? \nComment: You bought it just to benchmark it, didn't you? \nComment: Hello Kitty Island Adventures, butters would be proud of you. \nComment: Would you mind sharing or DMing retailer info? I don‚Äôt have a preferred vendor and am curious on your experience. \nComment: Llama 3.3 70b at 8-bit. Would be interesting to see how many tokens per second gives. \nComment: Try Super Mario Bros ü•∏ \nComment: you don't need it. gimme that. \nComment: Rtx pro 6000 is 96Gb it is beast. Without pro is 48gb. I really want to know how many FOPS it is. Or the t/s for a deepseek 70B or largest model it can fit. \nComment: Get some silly concurrency going on qwen 3 32b awq and run the aider benchmark. \nComment: Flux to generate pics of your dream Audi. Find out your use case and try some models that fit. I was first impressed by GLM 4 in one shot coding, but it fails to use other tools. Mistral small is my daily driver currently. It's even fluent in most languages. \nComment: That‚Äôs some expensive computer hardware. Congratulations. \nComment: That‚Äôs our serial number now \nComment: Your power connectors. \nComment: i cant imagine spending that much money on a gpu with that power connector \nComment: Houston we have lift off  https://preview.redd.it/v3z4prno2wye1.png?width=780\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=6a6156b3fc0818b93b0459a14c86a0e0dd1d70d7 \nComment: https://preview.redd.it/5bnvabxayvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=9516acddbdda888267887c823c70c25db1ba8c6e New card installed! \nComment: Is it better then a h100 performance wise? i know the vram is slightly bigger. \nComment: Quake I \nComment: Everything. In all seriousness, I would reaaally like to see the benchmarks on that thing \nComment: Old school runescape \nComment: Cancer research. \nComment: OT, but run 3Dmark and confirm if it really is faster in games than the 5090 (for once in the history of workstation cards). \nComment: dude you are so lucky congrats!! run every qwen 3 model and make videos! i hear you stream, how about a live stream using llama.cpp and testing out models, or lm studio. this card is so awesome üòç \nComment: Something like Gemma 3 27B/Mistral small-3/Qwen 3 32B with maximum context size? \nComment: Qwen3 and don‚Äôt look back \nComment: Qwen 30B A3B q8 has something around 30 GB file size. Should run very fast and have plent of room for context. \nComment: Bios \nComment: Where did you buy it from? \nComment: About $12,000 to $16,000 for the 48gb vram editions here .. not sure we can get the 96gb \nComment: What CPU are you pairing with? Linux? \nComment: https://preview.redd.it/glj9rjmk9vye1.jpeg?width=1280\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=64d6eac13d0a6aaed4b500953bfd300dcea46322 \nComment: ü•∫ü•πüò≠ \nComment: Wan 2.1 fp 32 model \nComment: Crysis. \nComment: Haha I thought it had a plaid pattern printed on it üòÖ \nComment: Dude so cheesed, could've even wait to get home \nComment: Hey, I was looking to buy one as well, how much did you pay and how long did it take to arrive. They are releasing so many cards these days I get confused. \nComment: How much \nComment: what version is it? Max‚ÄìQ? Workstation edition? Etc‚Ä¶ \nComment: GTA V \nComment: Grounding strap. \nComment: Crysis \nComment: Plex Media Server. But make sure to hack your drivers. \nComment: Mate, share some benchmarks!  I‚Äôm about ready to pull the trigger on one too, but the price gouging here is insane. They‚Äôre still selling Ampere A6000s for 6‚Äì7K AUD, and the Ada version is going for as much as 12K.  Instead of dropping prices on the older cards, they‚Äôre just marking up the new Blackwell ones way above MSRP. The server variant of this exact card is already sitting at 17K AUD (~11K USD)‚Äîabsolute piss take tbh. \nComment: Image and clip generation \nComment: I think I'll stream getting some LLMs and comfyui up tomorrow and the next few days. give a follow if you want to be notified https://twitch.tv/faustcircuits \nComment: Get that unsloth 235B Qwen3 model at Q2K_XL. It should fit. Q2 is the most efficient size when it comes to benchmark score to size ratio according to unsloths documentation. It should be fast AF too since only 22B active parameters.  \nComment: Nice! Still waiting for mine. Can you let me know if you are able to disable ECC or not? \nComment: what Audi is that? S4? \nComment: Nice. Run stuff and share stats! Would be cool to see. \nComment: https://preview.redd.it/ems9w2z6yvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=76b13f186be7cb783727c000bda533c92c1e8c56 here is the old card lol \n","Title: Visa is looking for vibe coders - thoughts?\nID: t3_1keolh9\nSummary:         submitted by    /u/eastwindtoday   [link]   [comments] \nImageDescription: ## Image Analysis: Visa \"Vibe Coder\" Job Requirements\n\nThe image presents a snippet of text outlining requirements for a position at Visa, seemingly related to \"Vibe Coding\" as indicated by the post title. Here's a detailed breakdown:\n\n**What is shown:**\n\n*   **Textual List:** The core content is a bullet-point list of desired skills and experience.\n*   **Logo:** A small, circular logo with the word \"technical\" is present in the bottom right corner. This likely indicates the source or department within Visa responsible for this role (e.g., a technical team).\n*   **Repetitive Text:** A large block of seemingly random characters (Unicode symbols) is repeated multiple times at the bottom. This appears to be filler or obfuscated text, potentially used for visual effect or as a placeholder.\n\n**Technical Details:**\n\n*   **Vector Databases:** Explicit mention of experience with vector databases ‚Äì specifically **ChromaDB and Conecone**. This is a key indicator. Vector databases are used for similarity search on embeddings, crucial in modern AI applications like recommendation systems, semantic search, and RAG (Retrieval-Augmented Generation).\n*   **Embeddings:** The requirement for experience with embedding models directly links to the vector database need.  These models transform data (text, images, etc.) into numerical vectors for efficient similarity comparisons.\n*   **Containerization:**  Experience with **Docker** is requested, signifying a need for reproducible and scalable deployments of AI/ML models.\n*   **Kubernetes:**  The inclusion of **Kubernetes** points to a production-level environment requiring orchestration and management of containerized applications.\n*   **Frontend Design Systems:**  Experience with **Lovable and V0** (likely referring to V0.dev) suggests a need for building user interfaces around AI-powered features, potentially involving component libraries and design systems.\n*   **Data Science Background:**  A \"big plus\" indicates that foundational data science knowledge is valuable.\n\n**Relation to Post Title (\"Visa is looking for vibe coders - thoughts?\"):**\n\nThe term \"vibe coding\" appears to be a colloquialism for building AI applications with a focus on user experience and rapid prototyping. The requirements strongly suggest this is *not* traditional software engineering.  The emphasis on frontend design systems, vector databases (for quick semantic search/response), and rapid prototyping tools like V0.dev supports this interpretation.  It's likely Visa is looking for engineers who can quickly build and iterate on AI-powered features, focusing on the *feel* of the application as much as its functionality.\n\n**Key Insights:**\n\n*   **RAG Focus:** The combination of vector databases and embedding models strongly suggests Visa is building applications leveraging Retrieval-Augmented Generation (RAG). This involves retrieving relevant information from a knowledge base to enhance the responses of large language models.\n*   **User-Centric AI:** The emphasis on frontend design systems and \"vibe coding\" indicates a focus on creating intuitive and engaging AI-powered user experiences.\n*   **Modern AI Stack:** The requirements showcase a modern AI stack including vector databases, containerization (Docker/Kubernetes), and rapid prototyping tools.\n*   **Potential Application Areas:**  Given Visa's business, potential applications could include fraud detection with explainable AI (using RAG to provide context), personalized financial recommendations, or intelligent customer support.\n\n\n\n\nComment:         submitted by    /u/eastwindtoday   [link]   [comments] \nComment: The way it's written with the other requirements makes me think that they want a programmer able to write vibe coding tools, not specifically hiring a vibe coder for programming. \nComment: \"Background in Data Science is a big plus\" - Be prepared to be payed less than a proper data scientist. \nComment: Dumb. Vibe coding is a bit ill-defined but what I see is a niche where people like product managers can use AI to build tiny tools that would be hard to get into a product team's priority list. In particular what I saw was using Zapier which is a user-friendly automation tool that lets you run snippets of Python or js inside workflows and now lets you generate them with AI. Great way to build little utilities without a developer but also very narrow, low stakes applications that aren't critical path. This job req wants familiarity with vector DBs and containers which means they want an actual ...\nComment: Vibe coding 2025 is same as slapping an untested, ill designed, 700-lines-a-function Python script on anything and calling it a day in the 2015s Edit: You scrolled down on some requirements tho https://www.visa.ca/fr_ca/jobs/REF061638W \u0026gt; Strong proficiency with Python, FastAPI and PostgreSQL for backend development. \nComment: These days, stupid HRs are stuffing the job requirements sections with as many tech buzzwords as possible, regardless of whether these buzzwords actually reflect the technologies used in the company. I feel like \"vibe coding\" may be one of those buzzwords. \nComment: now exactly nobody will know why credit card applications failed \nComment: Maybe we should all \"vibe-pay\" our Visa card bills. \nComment: Click click, sold all visa stocks instantly \nComment: I for one am happy my CC transactions are handled by vibed up systems. \nComment: they are starting up an ml intership for students next summer in poland too \nComment: I don't even know what to think. My general reaction of most job posts is to assume the companily already has someone they're gonna hire, and the position is only being posted as a formality and to make it appear that the company has fair hiring practices (even though requirements weed out people that are perfectly capable of being trained). \nComment: I know some folks working with such teams. They have ambitions to build vibe coding tools (like cursor) but without the dependency on letting data go out of the org (from a financial reg pov). Visa‚Äôs CTO is one of the highest paid CTOs in the US, and they are building agents replicating (gamma.ai, OpenAI deep research, Agentic Commerce) to start with. \nComment: They are even Vibe-using Caps. That's so Vibe. It gives me Bad Vibes. \nComment: That job posting is clear as mud. Title is an obvious mismatch for the skillset. \nComment: well I'm glad none of my credit cards are visa...  \nComment: what could possibly go wrong? \nComment: If they are listing the tech stacks that should be enough. \nComment: At least they're not requiring minimum 15 years of experience \nComment: Lmao \nComment: AI generated job posting ? :p \nComment: Vibe coding doesn't work. It takes me 10-20 prompts to generate anything useful with gemini 2.5.  Sometimes 30+ prompts even with explicit instructions. \nComment: And I thought it was a big deal 15 years ago when I talked NASA into letting me put ‚Äúauto-generated‚Äù guidance and control code into a satellite. That particular NASA center had never done such a thing, and I had to pop their cherry on it (as a contractor). Partly because a colleague of mine was doing the same thing on a bigger project, and he really wanted to be able to say his project wasn‚Äôt the first. The ‚Äúauto-generation‚Äù step is just compiling MatLab down into C code, and from there is just standard C compilation on the target environment. But the MatLab becomes the source code, ...\nComment: As someone already posted, these are not the full requirements, but another thing also:  Vibe-coding is expensive. Requests can quickly become $1-2 a piece. Over the course of a month, you could easily rack up $500+ if you're using it liberally. Now consider that you're already paying a regular salary, put this on top, and it becomes kind of unsustainable.  The money has to come from somewhere, and it's most probably the base pay. \nComment: I don't know much about developers as vibe coders. But, a lot of project managers \u0026amp; UI UX folks and product managers are starting to learn these tools (lovable, cursor, etc.,) and will roll out the basic screens for evaluation.  Also, this will start getting back to actual developers on justifying their timelines, complexity justifications, etc., This is mostly as nobody gives a fuck about code maintainability, reusability, tech debt, security, etc,.  Other side to this coin is 90% of code written by \u0026lt;5 years experienced devs are not following these standards anyways ü§∑‚Äç‚ôÇÔ∏è 90% o...\n","Title: UI-Tars-1.5 reasoning never fails to entertain me.\nID: t3_1keo3te\nSummary:       7B parameter computer use agent.    submitted by    /u/Impressive_Half_2819   [link]   [comments] \nImageDescription: ## Image Analysis: UI-Tars-1.5 Reasoning \u0026 LLM Interaction\n\nThe image depicts a screenshot of a user interface (UI) seemingly built to interact with a Large Language Model (LLM), likely within a macOS application environment. The UI is styled to resemble a pop-up window, possibly an extension or plugin.\n\n**Key Elements \u0026 Technical Details:**\n\n* **LLM Provider and Model Selection:** A prominent section allows the user to specify an LLM provider.  A dropdown menu is present, with \"Custom model...\" selected. Below this is a text input field labeled ‚ÄúCustom Model Name‚Äù where the string \"ByteDance-Seed/UI-Tars-1.5-7B\" is entered. This strongly suggests the user is attempting to utilize a custom LLM hosted or accessible via an API. The \"7B\" likely refers to the model size ‚Äì a 7 billion parameter LLM.\n* **UI-Tars Thoughts:** A text area displays the LLM's \"thoughts.\" The content reveals a conversational, almost stream-of-consciousness reasoning process.  The LLM is responding to an implicit prompt (likely related to Google) and expresses a desire to search for information (\"search for a repository\").  The use of the downward arrow emoji (‚Üì) suggests an intention to perform an action, possibly triggering a web search. This is indicative of the LLM having agency or access to tools.\n* **Text Input Box:** A text box labeled \"Ask me to perform tasks in a virtual macOS environment\" is present. This is crucial; it implies the LLM isn't just generating text but can *execute* commands within a simulated macOS environment. This is a significant capability, suggesting the LLM has access to APIs or tools that allow it to interact with an operating system.\n* **Provider Base URL:** A large, seemingly empty text field labeled \"Provider Base URL\" is visible. This likely specifies the API endpoint for accessing the LLM provider's services, crucial for authentication and communication. The field is filled with a long string of seemingly random characters ‚Äì potentially obfuscated or placeholder data.\n* **UI Style:** The UI is minimalist, with a clear focus on text interaction and model selection.  The use of rounded corners and a light background is typical of modern macOS applications.\n\n**Relation to Post Title (\"UI-Tars-1.5 reasoning never fails to entertain me\"):**\n\nThe post title directly references \"UI-Tars-1.5.\" The image confirms this is the name of the LLM being used.  The \"reasoning never fails to entertain me\" suggests the user is amused by the LLM's conversational style, its attempts at problem-solving (searching for a repository), and potentially its unexpected or quirky responses. The LLM's \"thoughts\" displayed in the text area support this ‚Äì it‚Äôs a verbose, self-aware reasoning process.\n\n**Key Insights:**\n\n* **Agentic LLM:** This is not a simple text completion model. The LLM exhibits agentic behavior ‚Äì it *plans* actions (searching), expresses intentions, and interacts with a virtual environment.\n* **Tool Use:** The LLM has access to tools (likely web search) and can utilize them to achieve its goals.\n* **Custom Model:** The user is leveraging a custom LLM, potentially fine-tuned for specific tasks or with unique capabilities.\n* **Virtual Environment Interaction:** The ability to perform tasks within a virtual macOS environment is a powerful feature, suggesting applications in automation, testing, or even security research.\n* **Potential for Complex Workflows:** The combination of reasoning, tool use, and environment interaction suggests this LLM can handle complex workflows beyond simple text generation.\n\n\n\n\nComment:       7B parameter computer use agent.    submitted by    /u/Impressive_Half_2819   [link]   [comments] \nComment: What's more important here is the model used - ByteDance-Seed/UI-TARS-1.5-7B the model which it is meant to be used with, so how did you make it work? Because last time I checked I haven't seen that model being converted to GGUF format, nor having vision support added into llama.cpp for it. \nComment: I guess: https://github.com/trycua/cua \nComment: When you train a model to use computers for humans and do the tiresome ToS reading, but it can't be bothered to do it either \nComment: Most probably trained on Gen-Z data. \nComment: https://preview.redd.it/4ignwtxrhuye1.jpeg?width=1079\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=e5d30270b51854c061aebdd502448897c63fed18 \nComment: On one hand, I guess I'd like the language model to read language on my behalf - on the other hand I wouldn't want the model to decide the cookies policy warrants user review or some other distraction so maybe skipping it is for the best after all. It does seem reading the pop-up falls within the scope of accessing the site to search for a repository \nComment: Try out yourself using cu/a! \nComment: I mean, fair \nComment: tiktok ai getting lazy \nComment: TARS, would you set your attention span setting to 8 for me? \nComment: It‚Äôs the defaut personality? \n","Title: Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)\nID: t3_1kepuli\nSummary:       Hey LocalLlama! We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon. We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device). Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist. We figured we'd kickstart this since we already maintain this benchmarking infra/tooling at RunLocal for our enterprise customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and TFLite to date, so a few things are still WIP for GGUF support.  Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill Tokens=512, Num Generation Tokens=128). GGUFs are from Unsloth üêê Qwen3 GGUF benchmarks on laptops Qwen3 GGUF benchmarks on phones You can see more of the benchmark data for Qwen3 here. We realize there are so many variables (devices, backends, etc.) that interpreting the data is currently h...\nImageDescription: ## Image Analysis: Qwen3 Performance Benchmarks\n\nThe image displays a screenshot of what appears to be a user interface (UI) for running and benchmarking the Qwen3 large language model (LLM) across a diverse set of devices. The UI is dark-themed with a card-based layout, likely from an application or web service designed for LLM inference.\n\n**Key UI Elements \u0026 Technical Details:**\n\n* **Model Selection:** Several Qwen3 model variants are listed, including:\n    * `DepthAnythingV2`: Suggests integration with a depth estimation model alongside Qwen3, potentially for multimodal applications.\n    * `Depth2Img-7B-08_0_gpu`: A 7 billion parameter Qwen3 variant optimized for image generation, running on a GPU. The \"08_0\" likely refers to a specific revision or training checkpoint.\n    * `Llama-3_2-1B-Instruct-08_0_gpu`: A 1 billion parameter Llama-3 variant, also GPU accelerated.  The \"Instruct\" suffix indicates fine-tuning for instruction following tasks.\n    * `Kokoro-82M`: A smaller 82 million parameter model, likely for faster inference on resource-constrained devices.\n* **Benchmark Metrics:** Each model card displays several key metrics:\n    * `toks/s`: Tokens per second ‚Äì a primary measure of inference speed.  Values are visible (e.g., 1.08 for Llama-3_2-1B).\n    * `RAM`: RAM utilization in MB.  Values are visible (e.g., 62MB for Llama-3_2-1B).\n    * `Text Generation`: Indicates the model is capable of text generation.\n    * `Depth Estimation` (for DepthAnythingV2): Indicates capability for depth estimation tasks.\n    * `View Pipeline`: A button to access detailed pipeline information, likely including hardware utilization and other performance data.\n* **Device Coverage:** The post title mentions ~50 devices (iOS, Android, Mac, Windows). While not directly visible in the screenshot, the UI is designed to support a wide range of platforms.\n* **Model Versions:** The inclusion of version numbers (e.g., \"08_0\") suggests the UI tracks and allows benchmarking of different model revisions, facilitating performance comparisons.\n\n**Relation to Post Title:**\n\nThe image directly illustrates the core content of the post: performance benchmarks for Qwen3 models. The UI provides a clear and organized way to compare the speed (toks/s) and resource usage (RAM) of different Qwen3 variants across a variety of devices. The mention of iOS, Android, Mac and Windows in the title is reflected by the UI's likely cross-platform nature.\n\n**Key Insights:**\n\n* **Scalability:** The availability of models ranging from 82M to 7B parameters suggests a focus on scalability, allowing users to choose models based on their hardware constraints and performance requirements.\n* **GPU Acceleration:** The inclusion of \"_gpu\" in some model names indicates that GPU acceleration is a key optimization strategy.\n* **Performance Trade-offs:** The UI allows for direct comparison of the trade-off between model size, inference speed (toks/s), and RAM usage.  Smaller models like Kokoro-82M likely offer faster inference but lower quality, while larger models provide better results at the cost of increased resource consumption.\n* **Ongoing Development:** The version numbers suggest active development and refinement of the Qwen3 models.\n\n\n\n\nComment:       Hey LocalLlama! We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon. We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device). Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist. We figured we'd kickstart this since we alr...\nComment: Iphone 16's Metal performance is pretty impressive for 1.6b-q8. But I do wonder why q8's performance is faster than q4 in that particular setup. \nComment: It‚Äôs interesting to see that performance in m4 is pretty similar in both cpu and gpu \nComment: There's one edge factor you missed - on Metal backend when you get OOM you get completely wrong results. For example on Qwen3 8B Q4 your results are like this: - MacBook Pro M1, 8GB = 99232.83tok/s prefill, 2133.70tok/s generation - MacBook Pro M3, 8GB = 90508.66tok/s prefill, 2507.50tok/s generation If you wouldn't get OOM the correct results for that model should be around ~100-150tok/s prefill and ~10tok/s generation. Additionally, all results for RAM usage on Apple silicon \u0026amp; Metal are not correct. In terms of your UX/UI there's tons of stuff that should be improved. but to not make thi...\nComment: How to run on metal on iphone 16 pro? I have pocketpal app and how to switch from cpu to metal? \nComment: if i'm reading this correctly the load time on cpu is better than gpu/metal for macbook pro but the gpu/metal is less memory intensive? also metal perf on iphone 16 is pretty impressive. \nComment: How do I run this on Android? Rn it just crashes \nComment: Why is Q8 faster than Q4??? \nComment: For laptops, is vulkan using the igpu ? \nComment: according to this data on iphone 16 you have 24 t/s on Q8 and 22 t/s on Q4 why so tiny models? \nComment: The iPhone 16e is listed to have the A18 Pro SoC but it actually has the A18.  https://preview.redd.it/h1dx2hgphvye1.png?width=623\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c2f8794bf27c6a9042b81040ebefa09873eae989 \n","Title: Qwen 30B A3B performance degradation with KV quantization\nID: t3_1kewkno\nSummary: I came across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4 that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization. I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens. I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens without finding any solution. Then I saw that sunpazed didn't used KV quantization and tried the same: boom! First time right. It worked with Q5_K_S and also with Q4_K_XL For who wants more details I leave here a gist https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef Do you have any report of performance degradation with long generations on Qwen3 30B A3B and KV quantization?    submitted by    /u/fakezeta   [link]   [comments]\nImageDescription: \nComment: I came across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4 that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization. I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens. I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens wit...\nComment: What KV quant level were you using? IMO on llama.cpp you shouldn‚Äôt push it past Q8_0. Q4_0 cache quant tanks quality in any model and especially models that heavily leverage GQA.  \nComment: I have one rule: I always test ALL new models without flash attention and with full 16bit KV cache. \nComment: Which KV quantization are you using? Don't have time to run this test right now, but I usually use -ctk q8_0 -ctv q5_1 (requires -DGGML_CUDA_FA_ALL_QUANTS=on) \nComment: Ive only tried KV quantization once and saw that any amount of it makes models super dumb. Not sure why anybody uses it tbh \nComment: Which quantization did you use initially? \nComment: Interested here since I'm running a q6 \nComment: Use these parameters: Thinking Mode Settings: Temperature = 0.6 Min_P = 0.0 Top_P = 0.95 TopK = 20 Non-Thinking Mode Settings: Temperature = 0.7  Min_P = 0.0 Top_P = 0.8 TopK = 20 \nComment: Could you please tell us how to disable KV cache quantisation? I'd also like to check the difference. What is the difference in the amount of memory used with KV running at fp16 in comparison with regular q4? \nComment: I‚Äôm confused. Isn‚Äôt K_M KV quantization? And yet you said Qwen 30b solved the rest with Q4 K_M? \nComment: Of course  Cache should always be fp16 even Q8 has degradation. Only flash attention is ok...ish ( as is fp16 ) \n","Title: QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.\nID: t3_1kenk4f\nSummary:       All models are from Bartowski - q4km version Test only HTML frontend. My assessment lauout quality from 0 to 10 Prompt \"Generate a beautiful website for Steve's pc repair using a single html script.\" QwQ 32b - 3/10 - poor layout but ..works , very basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b Qwen 3 32b - 6/10 - much better looks but still not too complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b272399ba8140785feb429a196ecc5173 GLM-4-32b 9/10 - looks insanely good , quality layout like sonnet 3.7 easily - 1500+ code lines https://preview.redd.it/3zj2lr2ahsye1.png?width=2469\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964 GLM-4-32b is insanely good for html code frontend. I say that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most. Other coding language like python , c , c++ or any other quality of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on the seme level but for html and JavaScript ... is GREAT.    submitte...\nImageDescription: \nComment:       All models are from Bartowski - q4km version Test only HTML frontend. My assessment lauout quality from 0 to 10 Prompt \"Generate a beautiful website for Steve's pc repair using a single html script.\" QwQ 32b - 3/10 - poor layout but ..works , very basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b Qwen 3 32b - 6/10 - much better looks but still not too complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b2...\nComment: Yeah, I have also tried to generate webpages with a couple of models, like GLM-4, Qwen3, Phi-4 Reasoning, etc. GLM-4 is so far the clear winner at these tasks. It's a gem in my model collection. \nComment: I've created a list of one-shot generated HTML pages using different models, big and small. https://blog.kekepower.com/ai/ \nComment: Ironically, so far userscript (javascript): Qwen 3 32B \u0026gt; GLM-4-0414. Don't get me wrong. I love GLM-4-0414, but it feels like it lacked the required understanding for my particular requests that Qwen 3 32B understood well. \nComment: Whats the temp? Did you rerun multiple times? \nComment: Why not try a slightly more complex task? E.g. a mini-game?  Create a single-HTML-page game using Babylon.js where the player controls a ship and moves about the open sea exploring islands to find treasure. A single, small map with 3 islands of which only one has the treasure is enough  In the first reply, the camera works, but WASD didn't. I copied-pasted the errors from the console a couple times and WASD works now. It looks terrible, but I guess that's expected without external assets. https://preview.redd.it/lf5foomq1tye1.png?width=1288\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b02ed407b3fbef31bf...\nComment: GLM falls flat on its face when I try to continue developing after the first prompt. It feels like a model trained (very well) for one-shots \nComment: Try UIGEN-T2 for html generation! There's also a react model.  https://huggingface.co/Tesslate/UIGEN-T2 \nComment: Would be interesting to see how these results compare to the recent Tesslate/UIGEN-T2-7B. It's a tuned version of Qwen 2.5 Coder 7B specifically for UI generation. \nComment: What quants what context window size? Ollama default size will kill QWQ reasoning if you don‚Äôt know how to set it up properly. \nComment: Still kind of freaked out that smaller Qwen 3 models are probably as good at website development as I was as a teenager. And a damn sight quicker too. \nComment: Thanks for posting the eval. Would be curious to see the prompt used as well. \nComment: Interesting. I ran the same input on QwQ with these settings: Temp: 0.6 Top p: 0.95 Min p: 0.0 Top k: 40 And quite a bit different output. Output: https://pastebin.com/Ntc8QQfH \nComment: GPT 4.1 is clearly the winner here in my opinion as well as claude sonnet 3.7 \n","Title: LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!\nID: t3_1keoint\nSummary:       You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B) mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert) tl;dr; I highly recommend doing a git pull and re-building your ik_llama.cpp or llama.cpp repo to take advantage of recent major performance improvements just released. The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving r/LocalLLaMA community! If you have enough VRAM to fully offload and already have an existing \"normal\" quant of Qwen3 MoE then you'll get a little more speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork! Details I spent yesterday compiling and running benhmarks on the newest versions of both ik_llama.cpp and mainline llama.cpp. For those that don't know, ikawrakow was an early contributor to mainline llama.cpp working on important features that have since trickled down into ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my understanding) the ik_llama.cpp fork was built and has a number of ...\nImageDescription: \nComment:       You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B) mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert) tl;dr; I highly recommend doing a git pull and re-building your ik_llama.cpp or llama.cpp repo to take advantage of recent major performance improvements just released. The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving r/LocalLLaMA community! If you have enough VRAM to fully offload and already have an existing \"normal\" quant of Qwen3 MoE then you'll get ...\nComment: I'm currently running ik_llama.cpp with Qwen3-235B-A22 on a Xeon E5-2680v4, that's a 10 year old CPU with 128GB ddr4 memory, and a single RTX3090. I'm getting 7 tok/s generation, very usable if you don't use reasoning. BTW the server is multi-GPU but ik_llama.cpp just crash trying to use multiple-gpus, but I don't think it would improve speed a lot, as the CPU is always the bottleneck. \nComment: Could you explain how to read your pictures? I see orange plot below red plot, so ik_llama.cpp is slower than llama.cpp? \nComment: Can you post some of the commands you use for the benchmarks? I want to tinker to see what is best for my use case \nComment: Oh, just updated. My rig is busy for running deepseek \u0026amp; ik_llama (1 week jobs). I will update after that :) \nComment: Maybe GGUF will now give same speed as MLX on Mac devices \nComment: I have a 3090. Doesn't this say it's slower, not faster? \nComment: https://preview.redd.it/0zroyhg1qsye1.png?width=3404\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b3e55128b1aac3f6d2ddfbd22597b9cd6d7dd02c In my limited testing you probably want to go with ik_llama.cpp for fully offloaded non-MoE models like the recent GLM-4 which is crazy efficient on kv-cache VRAM usage due to its GQA design. \nComment: I just pulled and rebuilt and I'm now actually going about 15 tps slower. My previous build was from about a week ago, and I was getting an eval time of about 54 tps. Now I'm only getting 39 tokens per second, so pretty significant drop. I just downloaded the latest unsloth model I'm running on 2 3090s, using this command: ``` .\\bin\\Release\\llama-server.exe -m C:\\shared-drive\\llm_models\\unsloth-2-Qwen3-30B-A3B-128K-Q8_0.gguf --host 0.0.0.0 --ctx-size 50000 --n-predict 10000 --jinja --tensor-split 14,14 --top_k 20 --min_p 0.0 --top_p 0.8 --flash-attn --n-gpu-layers 9999 --threads 24 ``` Prompt:...\nComment: How close is llamacpp to vLLM and exllama now? \nComment: Seems like it is related to CUDA only, so I guess only for people with Nvidia cards and not folks on Apple Silicon and others. \n","Title: Apparently shipping AI platforms is a thing now as per this post from the Qwen X account\nID: t3_1kebauw\nSummary:         submitted by    /u/MushroomGecko   [link]   [comments] \nImageDescription: ## Image Analysis: Qwen AI Platform Shipping Announcement\n\nThe image is a promotional graphic, likely intended for social media (given the cartoon style and celebratory elements), announcing the shipping of a Qwen AI platform. It features two anthropomorphic animal characters: a sloth and a bear, both appearing to be mascots. The bear is presenting a flower to the sloth.\n\n**Key Visual Elements \u0026 Technical Implications:**\n\n* **Mascots:** The use of mascots suggests a focus on accessibility and user-friendliness.  This is common in platforms aiming for broader adoption, particularly those targeting developers or non-technical users.\n* **\"Qwen\" Branding:** While not explicitly stated in the image, the post title confirms this is related to Qwen AI.  Qwen (ÈòøÈáåÂ∑¥Â∑¥ÈÄö‰πâÂçÉÈóÆ) is a large language model developed by Alibaba, known for its open-source releases and strong performance in Chinese NLP tasks.\n* **Shipping Implication:** The celebratory nature (flower presentation, bright colors) strongly implies a product release or significant update is being shipped.  This could refer to:\n    * **Model Weights:** The shipping of updated model weights for Qwen, allowing users to download and deploy the latest version.\n    * **Deployment Infrastructure:**  A packaged deployment solution (e.g., Docker images, cloud integrations) making it easier to run Qwen models.\n    * **SDK/API Release:**  New or updated Software Development Kits (SDKs) and Application Programming Interfaces (APIs) for interacting with Qwen models.\n    * **Platform/Tooling:** A complete platform or suite of tools built around Qwen, simplifying model fine-tuning, evaluation, and serving.\n\n**Insights:**\n\n* **Focus on User Experience:** The cartoon mascots suggest a deliberate effort to make Qwen approachable and easy to use.  This is crucial for attracting developers and fostering a community around the model.\n* **Commercialization/Accessibility:**  \"Shipping\" implies more than just open-source availability. It points towards a commercially viable product or easily deployable solution, potentially through cloud services or packaged software.\n* **Competitive Landscape:**  The announcement suggests Qwen is actively competing in the rapidly evolving LLM space, alongside models like Llama 2, Mistral, and others.  Shipping a platform is a key step in gaining market share.\n\n\n\nWithout further context (e.g., accompanying text, links), it's difficult to pinpoint the exact nature of what is being shipped. However, the image clearly communicates a positive update related to Qwen AI and its accessibility for users.\nComment:         submitted by    /u/MushroomGecko   [link]   [comments] \nComment: Ah yes, the AI romcom. \nComment: theres lore now? \nComment: -Ok, lets see, i want to make love to you-. -But...wait. no no \nComment: What do you mean by \"shipping AI platforms\"? \nComment: Can't wait for the competitions to see which model writes the best fan fiction \nComment: Should AI companies also create their own VTuber characters that use their models? \nComment: Qwen guy looks mean. His expression is like he's never felt love. \nComment: I ship it! \nComment: And those two ship often \nComment: Hell yeah \nComment: Has anyone noticed better performance for similar quants of the same model by different creators? \nComment: Oh no \nComment: Love the contribution to the OSS these guys make!! Pure awesomeness \nComment: ü§¶üèª‚Äç‚ôÇÔ∏è \nComment: Anyone feeding into this just entirely lacks a social life, why are some of you like this? \n","Title: IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models\nID: t3_1kedu0d\nSummary:         submitted by    /u/ab2377   [link]   [comments] \nImageDescription: \nComment:   \n                \n            submitted by   \n            /u/ab2377   [link]\n              [comments]\n            \nComment: Hope they\n            can release a larger one like 30b-a3b  \nComment: so a new\n            architecture, more moe goodness  \"Whereas prior generations\n            of Granite LLMs utilized a conventional transformer architecture, all models in the\n            Granite 4.0 family utilize a new hybrid Mamba-2/Transformer architecture, marrying the\n            speed and efficiency of Mamba with the precision of transformer-based self-attention.\n            Granite 4.0 Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts\n            (MoE) model, with 7B total parameters and only 1B active parameters at inference\n            time. Many of the innovat...\nComment: Please\n            look here: https://huggingface.co/ibm-granite/granite-4.0-tiny-preview/discussions/2\n            gabegoodhart IBM\n            Granite org 1\n            day ago Since this model is hot-off-the-press, we\n            don't have inference support in llama.cpp yet.\n            I'm actively working on it, but since this is one of the first major models\n            using a hybrid-recurrent architecture, there are a number of in-flight architectural\n            changes in the codebase that need to all meet up to get this supported. We'll\n            keep you posted! gabegoodhart IBM...\nComment: i hope we\n            can see some larger models too! I really want them to scale those more experimental\n            architectures and see where it leads. I think there is huge potential in combining\n            attention with hidden state models. attention to understand context, hidden state to\n            think ahead, remember key information etc. \nComment: Read the\n            full thing. It‚Äôs worth it. \nComment: Holy,\n            this actually looks really good. IBM might actually be able to catch up with Alibaba\n            with this one. \nComment: Neat but\n            unless folks really start working to help add support for mamba architectures to\n            llama.cpp it'll be dead on arrival. It would be great to see\n            the folks at /u/IBM step up and help out\n            llama.cpp to support things like this. \nComment: \n            The Granite 4.0 architecture uses no positional encoding (NoPE). Our testing\n            demonstrates convincingly that this has had no adverse effect on long-context\n            performance.   This is interesting. Are there any\n            papers that explain why this still works? \nComment: Looking\n            very promising... \nComment: Is IBM\n            going to be the silent winner? It‚Äôs impressive that their tiny model is 8b MOE and\n            likely to perform at the same level as their previous dense 8b: Granite 4.0\n            Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts (MoE) model,\n            with 7B total parameters and only 1B active parameters at inference time.\n            I hope their efforts attempt to improve in https://fiction.live/stories/Fiction-liveBench-April-6-2025/oQdzQvKHw8JyXbN87\n            and not just passkey testing. \nComment: I'm just a dreamer without much background in\n            ML stuff. Can anyone with sense comment on how likely it is we'll ever see\n            something that might be so efficient it'll run well on CPU? I mean, this model\n            already sounds pretty exciting from an efficiency perspective. Wondering if\n            we've exhausted architectural changes that would e.g. reduce memory bandwidth\n            requirements \nComment: ‚ÄúWe‚Äôre\n            excited to continue pre-training Granite 4.0 Tiny, given such promising results so early\n            in the process. We‚Äôre also excited to apply our learnings from post-training Granite\n            3.3, particularly with regard to reasoning capabilities and complex instruction\n            following, to the new models. Like its predecessors in Granite 3.2 and Granite 3.3,\n            Granite 4.0 Tiny Preview offers toggleablethinking on andthinking off functionality\n            (though its reasoning-focused post-training is very much incomplete).‚Äù\n            I hope s...\nComment: Now if\n            only we could get IBM to sell a version of their AI card to the public\n            \nComment: ibm doing\n            better work than meta theyre surprisingly becoming a big player in open source (for\n            small models) \nComment: I wonder\n            what is prompt processing speed for semi-recurrent stuff compared to transformers.\n            Transformers have fantastic prompt processing speed like 1000t/s easy even on crap like\n            3060, but slow down during token generation as context grows. This seems to be the other\n            way around, slow PP but fast TG. I might be completely\n            wrong. \nComment: Large\n            datasets: all of Harry Potter series asking questions like, what would have to change in\n            the series for Harry to end up with Hermione or for Voldemort to win. It‚Äôs a series\n            everyone knows fairly well and requires details in the story and the story\n            whole. \nComment: I\n            remember seeing this model a few days ago. There's no gguf so i cant try it out.\n            I guess there's not a lot of interest in this moe or it's not currently\n            possibly to make ggufs for it at the moment.  Webui stopped working\n            for me last year after i updated it and I've never been able to get it working\n            right since then, so been using lm studio appimages. That program runs everything good\n            for me but only runs ggufs. \nComment: Yall need\n            to learn Transformers and stop hating on llama.cpp \n"],"results":[{"Title":"LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!","ID":"t3_1keoint","Summary":"This post highlights significant performance improvements in both the mainline llama.cpp and its popular fork ik_llama.cpp, specifically for running Qwen3 MoE models. The updates enable faster inference through better GPU offloading and new quantization techniques like 'iqN_k'. For users with sufficient VRAM, mainline llama.cpp sees speed gains for fully offloaded Qwen3 MoE models, while ik_llama.cpp offers hybrid CPU+GPU support and state-of-the-art quantization. The author recommends rebuilding repositories to leverage these optimizations, noting that the competitive development between forks is benefiting the GGUF community. Benchmarks show mixed results, with some users reporting speed increases and others experiencing regressions depending on hardware configurations.","CommentSummary":"The community is divided but engaged: one user reports 7 tokens/sec on a decade-old CPU with RTX 3090, while others note slowdowns after updates. A key debate revolves around benchmarking methods and hardware bottlenecks (CPU vs GPU). Questions about quantization techniques, command-line parameters, and cross-platform compatibility (e.g., Apple Silicon vs NVIDIA) dominate discussions. Some users speculate that GGUF might rival MLX's Mac performance, but concerns about CUDA dependency limit appeal for non-NVIDIA users. The post sparks interest in comparing llama.cpp to alternatives like vLLM and exllama.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1keoint/llama_gotta_go_fast_both_ik_and_mainline_llamacpp/","Relevance":"","IsRelevant":true},{"Title":"IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models","ID":"t3_1kedu0d","Summary":"IBM has unveiled the Granite 4.0 Tiny Preview, a hybrid Mamba-2/Transformer architecture model with 7B total parameters and only 1B active parameters at inference time, leveraging a fine-grained mixture of experts (MoE) design. The model eliminates positional encoding (NoPE), demonstrating no adverse effects on long-context performance, while combining Mamba's efficiency with transformer-based precision. Early benchmarks suggest it achieves dense 8B model performance with significantly reduced inference compute, though community discussions highlight challenges in supporting its hybrid architecture via tools like llama.cpp. The preview also introduces toggleable thinking functionality, hinting at improved reasoning capabilities through post-training optimizations.","CommentSummary":"The community is enthusiastic about the hybrid architecture's potential to merge Mamba's efficiency with transformer accuracy, but concerns persist about limited tooling support (e.g., llama.cpp). Users speculate on scaling up the MoE design and question IBM's open-source strategy, while others praise the NoPE approach and efficiency gains. Discussions highlight a desire for CPU-friendly implementations and skepticism about whether architectural innovations will translate to real-world performance improvements. Some users criticize IBM's lack of direct support for popular inference frameworks, though others acknowledge their growing open-source contributions.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kedu0d/ibm_granite_40_tiny_preview_a_sneak_peek_at_the/","Relevance":"","IsRelevant":true},{"Title":"QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison","ID":"t3_1kenk4f","Summary":"This post presents a user-driven comparison of three large language models (QwQ 32b, Qwen 3 32b, and GLM-4-32B) focused on their ability to generate HTML code. The evaluation uses a specific prompt ('Generate a beautiful website for Steve's pc repair using a single html script') and assesses layout quality on a 0-10 scale. GLM-4-32B scores highest (9/10) with a 1500+ line output, while QwQ 32b scores lowest (3/10) with 250 lines. The analysis highlights GLM-4-32B's superior HTML/CSS generation capabilities compared to other models, though it notes limitations in non-HTML tasks. Technical details include model quantization (q4km) and code line counts, providing concrete metrics for comparison.","CommentSummary":"The community discussion reveals mixed sentiments. While many praise GLM-4-32B's HTML generation quality, some note its limitations in JavaScript and non-web tasks. Users compare it to other models like Qwen 3, Phi-4, and Tesslate/UIGEN-T2, with debates about specific use cases. Concerns include model quantization effects (q4km), context window sizes, and temperature settings impacting results. A few users suggest GLM-4-32B excels in one-shot tasks but struggles with iterative development. The thread also highlights interest in specialized models like UIGEN-T2 for UI generation.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kenk4f/qwq_32b_vs_qwen_3_32b_vs_glm432b_html_coding_only/","Relevance":"","IsRelevant":true},{"Title":"Apparently shipping AI platforms is a thing now as per this post from the Qwen X account","ID":"t3_1kebauw","Summary":"The post features a promotional image announcing the 'shipping' of a Qwen AI platform, likely involving model weights, deployment tools, or SDKs. The image uses anthropomorphic mascots (a sloth and bear) to emphasize accessibility, aligning with Alibaba's Qwen model focus on user-friendly NLP solutions. While the exact nature of the 'shipping' remains unspecified, the celebratory tone suggests a commercial or deployment-oriented update. The analysis highlights potential implications for broader adoption, competitive positioning against models like Llama 2, and the importance of user experience in AI platforms.","CommentSummary":"The comments mix humor and confusion about the term 'shipping AI platforms,' with users joking about AI romcoms, VTubers, and fanfiction competitions. Some question the technical specifics, while others praise Qwen's open-source contributions. The lighthearted tone contrasts with the technical analysis, reflecting community fascination with AI's cultural impact rather than deep technical scrutiny.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kebauw/apparently_shipping_ai_platforms_is_a_thing_now/","Relevance":"","IsRelevant":false},{"Title":"Qwen 30B A3B performance degradation with KV quantization","ID":"t3_1kewkno","Summary":"A user reports inconsistent performance issues with Qwen 30B when using KV quantization for long-generation tasks, particularly in solving the OpenAI cypher test. They observed repetition loops and incorrect conclusions with Q4_K_XL and Q5_K_S quantizations, but success was achieved without KV quantization. The post includes code snippets and comparisons between different quantization schemes (e.g., Q4_K_M, Q5_K_S) and highlights the impact of KV cache precision on model behavior. The user seeks confirmation if others have encountered similar degradation in Qwen 30B A3B.","CommentSummary":"The discussion reveals strong community interest in quantization trade-offs. Several users emphasize avoiding aggressive KV quantization (e.g., Q4_0) due to quality loss, particularly with GQA-based models. Recommendations include using fp16 KV caches or Q8_0 quantization for stability. Some users question the utility of KV quantization altogether, while others share parameter tweaks (e.g., temperature settings) to mitigate issues. The thread underscores technical debates around quantization levels and their impact on reasoning tasks, with a focus on practical troubleshooting.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kewkno/qwen_30b_a3b_performance_degradation_with_kv/","Relevance":"","IsRelevant":true},{"Title":"UI-Tars-1.5 reasoning never fails to entertain me.","ID":"t3_1keo3te","Summary":"This post showcases a custom UI for interacting with the ByteDance-Seed/UI-Tars-1.5-7B model, a 7 billion parameter LLM. The interface allows users to specify LLM providers, execute commands in a virtual macOS environment, and access tools like web search. The model's 'thoughts' section reveals agentic behavior, such as planning to search for repositories. Technical details include a 'Provider Base URL' field and integration with tools like cu/a, suggesting advanced automation capabilities. The 7B parameter size indicates a balance between performance and resource efficiency, while the UI's design emphasizes seamless human-machine collaboration.","CommentSummary":"The community is intrigued by the model's agentic capabilities and technical implementation. Users debated how the 7B model was adapted for specific workflows, with questions about GGUF format conversion and llama.cpp compatibility. Some joked about the model's 'lazy' tendencies, while others highlighted its potential for automation tasks like cookie policy analysis. The mention of 'cu/a' (likely a tool for LLM interaction) sparked interest in practical applications. Comments also touched on ethical concerns about AI decision-making, such as whether models should handle tedious tasks like ToS reading. Overall, the discussion blends technical curiosity with playful speculation about AI autonomy.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1keo3te/uitars15_reasoning_never_fails_to_entertain_me/","Relevance":"","IsRelevant":true},{"Title":"Visa is looking for vibe coders - thoughts?","ID":"t3_1keolh9","Summary":"This Reddit post analyzes a Visa job posting for a 'Vibe Coder' role, revealing technical requirements that suggest AI/ML integration. The position emphasizes vector databases (ChromaDB, Conecone), embeddings, Docker/Kubernetes, and frontend systems like Lovable/V0.dev. The term 'vibe coding' appears to refer to rapid AI prototyping with a focus on user experience, potentially leveraging Retrieval-Augmented Generation (RAG) for applications like fraud detection or personalized financial tools. The post highlights Visa's adoption of modern AI infrastructure, including containerization and semantic search technologies.","CommentSummary":"The community debates the meaning of 'vibe coding,' with some suggesting it's a buzzword for AI-powered rapid development. Critics argue the job post uses excessive jargon and may underpay compared to traditional roles. Discussions highlight skepticism about AI tools like Cursor and Zapier for low-stakes applications, while others note Visa's potential use of RAG and AI agents for financial systems. Humor about 'vibe-paying' and 'bad vibes' contrasts with concerns about tech debt and unsustainable AI integration costs. Some commenters question the practicality of the requirements, noting discrepancies between stated skills and actual job needs.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1keolh9/visa_is_looking_for_vibe_coders_thoughts/","Relevance":"","IsRelevant":true},{"Title":"Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS, Android, Mac, Windows)","ID":"t3_1kepuli","Summary":"This post presents an open-source initiative to benchmark Qwen3's performance across 50+ devices, including iOS, Android, Mac, and Windows. The benchmarks focus on key metrics like tokens per second (toks/s) and RAM utilization, with a emphasis on model formats such as Core ML, ONNX, and TFLite. The image shows a UI displaying Qwen3 variants (e.g., DepthAnythingV2, Llama-3_2-1B-Instruct) with metrics like 1.08 tok/s for the 1B model and 62MB RAM usage. The post highlights scalability through models ranging from 82M to 7B parameters, GPU acceleration (e.g., 'gpu' in model names), and trade-offs between speed, resource usage, and model size. Challenges like inaccurate RAM measurements on Apple silicon and OOM errors on Metal backends are noted, along with plans to expand device coverage to 100+ devices.","CommentSummary":"The community is engaged with technical discussions about Qwen3's performance. Users question why Q8 models outperform Q4 on iPhones, note similar M4 CPU/GPU performance, and highlight critical issues like Metal backend OOM errors causing incorrect results. There's interest in running models on specific devices (e.g., iPhone 16 Pro via PocketPal, Android compatibility) and clarifying hardware details (e.g., Vulkan using iGPU). While some critique the UI/UX, the post sparks debate about benchmarking methodologies and hardware-specific optimizations. The mention of versioned models ('08_0') suggests active development, and users are eager for more standardized performance data.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kepuli/qwen3_performance_benchmarks_tokss_ram/","Relevance":"","IsRelevant":true},{"Title":"What do I test out / run first?","ID":"t3_1kexdgy","Summary":"A Reddit user received new GPU hardware (likely an expensive workstation card like the RTX Pro 6000) and is seeking recommendations for initial tests. The post includes comments suggesting specific LLMs (e.g., Llama 3.2, Qwen 3), hardware comparisons (Crysis benchmarking, VRAM capacity), and technical questions about drivers, ECC settings, and model performance. The discussion highlights community interest in testing large models like 70B-parameter variants but lacks concrete technical details about the hardware itself.","CommentSummary":"The community is highly engaged, with users suggesting specific models (Llama 3.3 70b, Qwen 3 32B), benchmarking scenarios (Crysis, 3Dmark), and hardware optimizations. Concerns about pricing, availability, and technical setup (e.g., CUDA versions, ECC memory) are prominent. Humorous remarks about 'Crysis' and 'Old School Runescape' contrast with serious discussions about FLOPS metrics and model efficiency. Several users request benchmarks and retailer info, indicating a mix of curiosity and practicality.","Link":"https://www.reddit.com/r/LocalLLaMA/comments/1kexdgy/what_do_i_test_out_run_first/","Relevance":"","IsRelevant":false}],"persona":"LocalLLaMa"}