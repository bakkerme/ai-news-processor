{
  "entrySummaries": [
    {
      "rawInput": "Title: What do I test out / run first?\nID: t3_1kexdgy\nSummary:   \n               Just got her in the mail. Haven't had a chance\n            to put her in yet.    submitted by\n              \n            /u/Recurrents   [link]\n              [comments]\n            \nImageDescription: \nComment:       Just got her in the mail. Haven't had a chance to put her in yet.    submitted by    /u/Recurrents   [link]   [comments] \nComment: First run home. Preferably safely. \nComment: llama 3.2 1b \nComment: Bro is loaded. How many kidneys did you sell for that?! \nComment: LLAMA 405B Q.000016 \nComment: all the new qwen 3 models \nComment: sexy ass card \nComment: Old School Runescape \nComment: Are they selling those already? \nComment: Download cuda and make sure your pytorch is the cuda version \nComment: Can it run Crysis? \nComment: You bought it just to benchmark it, didn't you? \nComment: Hello Kitty Island Adventures, butters would be proud of you. \nComment: Would you mind sharing or DMing retailer info? I don‚Äôt have a preferred vendor and am curious on your experience. \nComment: Llama 3.3 70b at 8-bit. Would be interesting to see how many tokens per second gives. \nComment: Try Super Mario Bros ü•∏ \nComment: you don't need it. gimme that. \nComment: Rtx pro 6000 is 96Gb it is beast. Without pro is 48gb. I really want to know how many FOPS it is. Or the t/s for a deepseek 70B or largest model it can fit. \nComment: Get some silly concurrency going on qwen 3 32b awq and run the aider benchmark. \nComment: Flux to generate pics of your dream Audi. Find out your use case and try some models that fit. I was first impressed by GLM 4 in one shot coding, but it fails to use other tools. Mistral small is my daily driver currently. It's even fluent in most languages. \nComment: That‚Äôs some expensive computer hardware. Congratulations. \nComment: That‚Äôs our serial number now \nComment: Your power connectors. \nComment: i cant imagine spending that much money on a gpu with that power connector \nComment: Houston we have lift off  https://preview.redd.it/v3z4prno2wye1.png?width=780\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=6a6156b3fc0818b93b0459a14c86a0e0dd1d70d7 \nComment: https://preview.redd.it/5bnvabxayvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=9516acddbdda888267887c823c70c25db1ba8c6e New card installed! \nComment: Is it better then a h100 performance wise? i know the vram is slightly bigger. \nComment: Quake I \nComment: Everything. In all seriousness, I would reaaally like to see the benchmarks on that thing \nComment: Old school runescape \nComment: Cancer research. \nComment: OT, but run 3Dmark and confirm if it really is faster in games than the 5090 (for once in the history of workstation cards). \nComment: dude you are so lucky congrats!! run every qwen 3 model and make videos! i hear you stream, how about a live stream using llama.cpp and testing out models, or lm studio. this card is so awesome üòç \nComment: Something like Gemma 3 27B/Mistral small-3/Qwen 3 32B with maximum context size? \nComment: Qwen3 and don‚Äôt look back \nComment: Qwen 30B A3B q8 has something around 30 GB file size. Should run very fast and have plent of room for context. \nComment: Bios \nComment: Where did you buy it from? \nComment: About $12,000 to $16,000 for the 48gb vram editions here .. not sure we can get the 96gb \nComment: What CPU are you pairing with? Linux? \nComment: https://preview.redd.it/glj9rjmk9vye1.jpeg?width=1280\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=64d6eac13d0a6aaed4b500953bfd300dcea46322 \nComment: ü•∫ü•πüò≠ \nComment: Wan 2.1 fp 32 model \nComment: Crysis. \nComment: Haha I thought it had a plaid pattern printed on it üòÖ \nComment: Dude so cheesed, could've even wait to get home \nComment: Hey, I was looking to buy one as well, how much did you pay and how long did it take to arrive. They are releasing so many cards these days I get confused. \nComment: How much \nComment: what version is it? Max‚ÄìQ? Workstation edition? Etc‚Ä¶ \nComment: GTA V \nComment: Grounding strap. \nComment: Crysis \nComment: Plex Media Server. But make sure to hack your drivers. \nComment: Mate, share some benchmarks!  I‚Äôm about ready to pull the trigger on one too, but the price gouging here is insane. They‚Äôre still selling Ampere A6000s for 6‚Äì7K AUD, and the Ada version is going for as much as 12K.  Instead of dropping prices on the older cards, they‚Äôre just marking up the new Blackwell ones way above MSRP. The server variant of this exact card is already sitting at 17K AUD (~11K USD)‚Äîabsolute piss take tbh. \nComment: Image and clip generation \nComment: I think I'll stream getting some LLMs and comfyui up tomorrow and the next few days. give a follow if you want to be notified https://twitch.tv/faustcircuits \nComment: Get that unsloth 235B Qwen3 model at Q2K_XL. It should fit. Q2 is the most efficient size when it comes to benchmark score to size ratio according to unsloths documentation. It should be fast AF too since only 22B active parameters.  \nComment: Nice! Still waiting for mine. Can you let me know if you are able to disable ECC or not? \nComment: what Audi is that? S4? \nComment: Nice. Run stuff and share stats! Would be cool to see. \nComment: https://preview.redd.it/ems9w2z6yvye1.jpeg?width=3000\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=76b13f186be7cb783727c000bda533c92c1e8c56 here is the old card lol \n",
      "results": {
        "title": "What do I test out / run first?",
        "id": "t3_1kexdgy",
        "summary": "This Reddit post features a user who just received a high-end GPU (likely an RTX Pro 6000 with 96GB VRAM) and is seeking recommendations for initial tests. The community suggests running large language models like Llama 3.2, Qwen 3, and Mistral, with specific focus on benchmarking performance metrics such as tokens per second, FLOPS, and context window sizes. Comments mention testing games like Crysis and Quake I for GPU stress, as well as AI workloads like image generation with Flux and coding benchmarks. Technical discussions include CUDA setup, PyTorch versions, and comparisons to H100/V100 GPUs. While the post itself lacks formal technical details, the community-driven suggestions highlight current trends in AI hardware optimization and model evaluation.",
        "commentSummary": "The community is highly engaged, with users sharing detailed suggestions for testing the GPU's capabilities. Many focus on benchmarking large models like Qwen 30B and Llama 405B, while others joke about running classic games or asking about power requirements. Concerns about the GPU's $12k+ price tag and availability are raised, alongside technical queries about ECC memory disablement and driver hacks. The post reflects both excitement about cutting-edge hardware and practical considerations for AI/ML workloads.",
        "isRelevant": false
      },
      "processingTimeMs": 27401
    },
    {
      "rawInput": "Title: Visa is looking for vibe coders - thoughts?\nID: t3_1keolh9\nSummary:   \n                \n            submitted by    /u/eastwindtoday\n              [link]\n              [comments]\n            \nImageDescription: ## Image Analysis: Visa \"Vibe Coder\" Job Posting Detail\n\nThe image presents a screenshot of text, likely extracted from a job posting by Visa. The post is soliciting candidates with specific technical skills, framed as \"vibe coders.\" It appears to be a section detailing desired experience.\n\nThe text is formatted as a bulleted list, outlining requirements and preferred qualifications. Key technologies explicitly mentioned include:\n\n*   **Vector Databases:** Pinecone, ChromaDB are named. This indicates a focus on semantic search and retrieval using vector embeddings.\n*   **Embedding Models:** The posting specifically requests experience with embedding models, suggesting the role involves working with representations of data for similarity comparisons.\n*   **Containerization:** Docker is explicitly listed, indicating a need for experience in packaging and deploying applications within containers. Kubernetes is also mentioned, implying familiarity with container orchestration at scale.\n*   **Frontend Frameworks:** Lovable and V0 are listed, suggesting a preference for modern frontend tooling. These frameworks likely represent a specific tech stack the team utilizes.\n*   **Data Science:** Experience in Data Science is considered a \"big plus,\" indicating the role may involve data analysis or model building.\n*   **Problem Solving:** Strong problem-solving skills are explicitly requested, highlighting the need for analytical thinking and debugging capabilities.\n\nThe text also includes a large block of unreadable characters at the bottom, likely representing encoding errors or corrupted data. \n\nA Reddit logo (specifically the \"technical\" subreddit icon) is visible in the bottom right corner, indicating this screenshot was likely taken from a Reddit post discussing the job opportunity. The overall impression is that Visa is seeking engineers with experience in modern machine learning infrastructure, particularly around vector databases and embedding models, combined with frontend development skills.\nComment:         submitted by    /u/eastwindtoday   [link]   [comments] \nComment: The way it's written with the other requirements makes me think that they want a programmer able to write vibe coding tools, not specifically hiring a vibe coder for programming. \nComment: \"Background in Data Science is a big plus\" - Be prepared to be payed less than a proper data scientist. \nComment: Dumb. Vibe coding is a bit ill-defined but what I see is a niche where people like product managers can use AI to build tiny tools that would be hard to get into a product team's priority list. In particular what I saw was using Zapier which is a user-friendly automation tool that lets you run snippets of Python or js inside workflows and now lets you generate them with AI. Great way to build little utilities without a developer but also very narrow, low stakes applications that aren't critical path. This job req wants familiarity with vector DBs and containers which means they want an actual experienced software engineer. Someone who can use a coding assistant but probably doesn't need one. That's not vibe coding. \nComment: Vibe coding 2025 is same as slapping an untested, ill designed, 700-lines-a-function Python script on anything and calling it a day in the 2015s Edit: You scrolled down on some requirements tho https://www.visa.ca/fr_ca/jobs/REF061638W \u0026gt; Strong proficiency with Python, FastAPI and PostgreSQL for backend development. \nComment: These days, stupid HRs are stuffing the job requirements sections with as many tech buzzwords as possible, regardless of whether these buzzwords actually reflect the technologies used in the company. I feel like \"vibe coding\" may be one of those buzzwords. \nComment: now exactly nobody will know why credit card applications failed \nComment: Maybe we should all \"vibe-pay\" our Visa card bills. \nComment: Click click, sold all visa stocks instantly \nComment: I for one am happy my CC transactions are handled by vibed up systems. \nComment: they are starting up an ml intership for students next summer in poland too \nComment: I don't even know what to think. My general reaction of most job posts is to assume the companily already has someone they're gonna hire, and the position is only being posted as a formality and to make it appear that the company has fair hiring practices (even though requirements weed out people that are perfectly capable of being trained). \nComment: I know some folks working with such teams. They have ambitions to build vibe coding tools (like cursor) but without the dependency on letting data go out of the org (from a financial reg pov). Visa‚Äôs CTO is one of the highest paid CTOs in the US, and they are building agents replicating (gamma.ai, OpenAI deep research, Agentic Commerce) to start with. \nComment: They are even Vibe-using Caps. That's so Vibe. It gives me Bad Vibes. \nComment: That job posting is clear as mud. Title is an obvious mismatch for the skillset. \nComment: well I'm glad none of my credit cards are visa...  \nComment: what could possibly go wrong? \nComment: If they are listing the tech stacks that should be enough. \nComment: At least they're not requiring minimum 15 years of experience \nComment: Lmao \nComment: AI generated job posting ? :p \nComment: Vibe coding doesn't work. It takes me 10-20 prompts to generate anything useful with gemini 2.5.  Sometimes 30+ prompts even with explicit instructions. \nComment: And I thought it was a big deal 15 years ago when I talked NASA into letting me put ‚Äúauto-generated‚Äù guidance and control code into a satellite. That particular NASA center had never done such a thing, and I had to pop their cherry on it (as a contractor). Partly because a colleague of mine was doing the same thing on a bigger project, and he really wanted to be able to say his project wasn‚Äôt the first. The ‚Äúauto-generation‚Äù step is just compiling MatLab down into C code, and from there is just standard C compilation on the target environment. But the MatLab becomes the source code, in this case. And you have to version-control the auto-generation configuration as well.  Honestly, with all the other shit we had to deal with on that program, just telling an AI to ‚Äúwrite some code that might look like spacecraft guidance and control software, no particular requirements because we‚Äôre don‚Äôt have the money to actually test the hardware before launch, and most of it is going to fail anyway‚Äù would have been such a stress-saver. That‚Äôs a good vibe. \nComment: As someone already posted, these are not the full requirements, but another thing also:  Vibe-coding is expensive. Requests can quickly become $1-2 a piece. Over the course of a month, you could easily rack up $500+ if you're using it liberally. Now consider that you're already paying a regular salary, put this on top, and it becomes kind of unsustainable.  The money has to come from somewhere, and it's most probably the base pay. \nComment: I don't know much about developers as vibe coders. But, a lot of project managers \u0026amp; UI UX folks and product managers are starting to learn these tools (lovable, cursor, etc.,) and will roll out the basic screens for evaluation.  Also, this will start getting back to actual developers on justifying their timelines, complexity justifications, etc., This is mostly as nobody gives a fuck about code maintainability, reusability, tech debt, security, etc,.  Other side to this coin is 90% of code written by \u0026lt;5 years experienced devs are not following these standards anyways ü§∑‚Äç‚ôÇÔ∏è 90% of products from start-ups and other IT companies never see the light anyways ü§¶‚Äç‚ôÇÔ∏èü§£ \n",
      "results": {
        "title": "Visa is looking for vibe coders - thoughts?",
        "id": "t3_1keolh9",
        "summary": "This Reddit discussion revolves around Visa's job posting for 'vibe coders,' which has sparked curiosity and skepticism. The role appears to require expertise in vector databases (e.g., Pinecone, ChromaDB), embedding models, containerization (Docker/Kubernetes), and frontend frameworks like Lovable/V0. While the technical requirements are concrete, the term 'vibe coding' remains ambiguous. Comments suggest it might refer to AI-assisted development or low-code tools, but the job's actual scope is unclear. The mention of Python, FastAPI, and PostgreSQL hints at backend development, while the emphasis on data science experience raises questions about the role's priorities. The post also highlights broader concerns about HR buzzword inflation and the feasibility of AI-generated code in critical systems.",
        "commentSummary": "The community is divided on the 'vibe coder' concept, with many mocking the term as vague or overly technical jargon. Some argue it could signal Visa's interest in AI-powered development tools, while others dismiss it as a marketing ploy. Critics note the disconnect between the listed tech stack (e.g., vector databases) and the nebulous 'vibe coding' title. Humor about 'vibe-paying' and 'bad vibes' abounds, but deeper concerns include potential cost overruns from AI tool reliance and skepticism about the role's practicality. A few commenters suggest Visa might be exploring generative AI for internal tools, but the lack of clarity leaves many unconvinced.",
        "isRelevant": false
      },
      "processingTimeMs": 24218
    },
    {
      "rawInput": "Title: UI-Tars-1.5 reasoning never fails to entertain me.\nID: t3_1keo3te\nSummary:   \n             \n             7B\n            parameter computer use agent.   \n            submitted by   \n            /u/Impressive_Half_2819   [link]\n              [comments]\n            \nImageDescription: ## Image Analysis: UI-Tars-1.5 Reasoning Interface\n\nThe image displays a screenshot of a user interface (UI) seemingly built for interacting with a Large Language Model (LLM), specifically identified as \"UI-Tars-1.5\". The UI appears to be running within a macOS environment, evidenced by the window chrome and overall aesthetic.\n\nThe central portion of the screen is dominated by a panel titled \"LLM Provider and Model\". This section allows user selection of an LLM. A dropdown menu is present, currently displaying \"Select model or choose 'Custom model...'\". Below this dropdown is a text input field labeled ‚ÄúCustom Model Name‚Äù, with the placeholder text \"ByteDance-Seed/UI-Tars-1.5-7B\" pre-populated, suggesting the user is interacting with a model hosted by ByteDance. A \"Provider Base URL\" field is also visible, though its content is obscured by a long string of non-English characters.\n\nTo the right of this selection panel is a text area labeled \"UI-TARS Thoughts\". This section displays the LLM's reasoning process. The text indicates a conversational flow where the user asked Google to explain its purpose, and the LLM is responding. The response begins with \"It seems that Google wants to explain the purpose of read through all this information right now. I just want to search for a repository, so I'll go ahead and select 'All' first.\"  An arrow symbol (‚Üì) suggests further reasoning or action is pending.\n\nAt the bottom of the screen, a \"Textbox\" input field is present with the prompt ‚ÄúAsk me to perform tasks in a virtual macOS environment‚Äù. This indicates the LLM is capable of executing commands or responding to requests within a simulated operating system.\n\nThe UI design appears minimalist, utilizing a light background and clear typography. The presence of the \"Custom model...\" option suggests flexibility in selecting or specifying LLMs,\nComment:       7B parameter computer use agent.    submitted by    /u/Impressive_Half_2819   [link]   [comments] \nComment: What's more important here is the model used - ByteDance-Seed/UI-TARS-1.5-7B the model which it is meant to be used with, so how did you make it work? Because last time I checked I haven't seen that model being converted to GGUF format, nor having vision support added into llama.cpp for it. \nComment: I guess: https://github.com/trycua/cua \nComment: When you train a model to use computers for humans and do the tiresome ToS reading, but it can't be bothered to do it either \nComment: Most probably trained on Gen-Z data. \nComment: https://preview.redd.it/4ignwtxrhuye1.jpeg?width=1079\u0026amp;format=pjpg\u0026amp;auto=webp\u0026amp;s=e5d30270b51854c061aebdd502448897c63fed18 \nComment: On one hand, I guess I'd like the language model to read language on my behalf - on the other hand I wouldn't want the model to decide the cookies policy warrants user review or some other distraction so maybe skipping it is for the best after all. It does seem reading the pop-up falls within the scope of accessing the site to search for a repository \nComment: Try out yourself using cu/a! \nComment: I mean, fair \nComment: tiktok ai getting lazy \nComment: TARS, would you set your attention span setting to 8 for me? \nComment: It‚Äôs the defaut personality? \n",
      "results": {
        "title": "UI-Tars-1.5 reasoning never fails to entertain me.",
        "id": "t3_1keo3te",
        "summary": "This post shares a screenshot of a custom UI interface for interacting with ByteDance's 'UI-Tars-1.5-7B' model, a 7-billion-parameter language model. The interface allows users to select LLM providers, input custom models, and view the system's 'reasoning process' in a dedicated panel. Notably, the UI appears to simulate execution of tasks within a virtual macOS environment, suggesting capabilities beyond standard LLM interactions. The interface's design includes a 'LLM Provider and Model' selector, a 'UI-TARS Thoughts' section displaying the model's internal reasoning, and a textbox for task execution. While the post itself lacks technical specifications about the model's architecture or performance, the community discussion highlights concerns about compatibility (e.g., absence of GGUF format support in llama.cpp) and the model's apparent ability to navigate digital environments. The UI's functionality implies integration with tools like 'cua' (as suggested in comments) for executing commands, though no concrete implementation details are provided.",
        "commentSummary": "The comments reveal a mix of curiosity and technical skepticism. Users questioned how the UI-Tars-1.5-7B model operates without GGUF format support or vision capabilities in llama.cpp, suggesting potential workarounds like the 'cua' project. Others humorously noted the model's apparent reluctance to read lengthy terms of service, joking about 'Gen-Z data' training. A recurring theme was the tension between AI-assisted task automation and the risks of delegating critical decisions (e.g., cookie policies). Some users speculated about the model's 'default personality' settings, while others focused on practicality, urging experimentation with the claimed tools. The discussion underscores both fascination with the UI's capabilities and frustration over missing technical documentation.",
        "isRelevant": false
      },
      "processingTimeMs": 18551
    },
    {
      "rawInput": "Title: Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS,\n            Android, Mac, Windows)\nID: t3_1kepuli\nSummary:   \n             \n             Hey\n            LocalLlama! We've started publishing open-source model\n            performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS,\n            Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+\n            soon. We‚Äôre doing this because perf metrics determine the viability\n            of shipping models in apps to users (no end-user wants crashing/slow AI features that\n            hog up their specific device). Although benchmarks get posted in\n            threads here and there, we feel like a more consolidated and standardized hub should\n            probably exist. We figured we'd kickstart this since we\n            already maintain this benchmarking infra/tooling at RunLocal for our enterprise\n            customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and\n            TFLite to date, so a few things are still WIP for GGUF support. \n            Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill\n            Tokens=512, Num Generation Tokens=128). GGUFs\n            are from Unsloth üêê Qwen3\n            GGUF benchmarks on laptops Qwen3\n            GGUF benchmarks on phones You can see more of the benchmark\n            data for Qwen3 here.\n            We realize there are so many variables (devices, backends, etc.) that interpreting the\n            data is currently harder than it should be. We'll work on that!\n            You can also see benchmarks for a few other models here. If\n            you want to see benchmarks for any others, feel free to request them and we‚Äôll try to\n            publish ASAP! Lastly, you can run your own benchmarks on our devices\n            for free (limited to some degree to avoid our devices melting!). This\n            free/public version is a bit of a frankenstein fork of our enterprise product, so any\n            benchmarks you run would be private to your account. But if there's interest, we\n            can add a way for you to also publish them so that the public benchmarks aren‚Äôt\n            bottlenecked by us.  It‚Äôs still very early days for us with this, so\n            please let us know what would make it better/cooler for the community: https://edgemeter.runlocal.ai/public/pipelines\n            To more on-device AI in production! üí™ https://i.redd.it/aev5rgjazsye1.gif\n               submitted by    /u/intofuture \n             [link]\n              [comments]\n            \nImageDescription: ## Image Analysis: Qwen3 Performance Benchmarks\n\nThe image presents a detailed performance comparison of the Qwen3 large language model across approximately 50 different devices. The data is organized in a tabular format, likely extracted from a spreadsheet or similar tool.\n\n**Columns:** The table features the following columns: \"Backend\", \"Count\", \"Q4_0\", \"1 GB\", \"364 MB\", \"1.71 GB\", \"RAM (MB)\", \"Time (ms)\", and \"Toks/s\".\n\n*   **Backend:** Lists the hardware/software configurations used for testing. Examples include \"Metal\" (likely Apple GPUs), ‚ÄúGoogle Pixel 8 (2023)\", \"SoC: A13 Bionic\", ‚ÄúRAM: 4GB‚Äù, and various Android devices with specific chipsets.\n*   **Count:** Indicates the number of samples/runs performed for each backend configuration.\n*   **Q4\\_0, 1 GB, 364 MB, 1.71 GB:** These columns represent different quantization levels of the Qwen3 model (4-bit, 1GB quantized, 364MB quantized and 1.71 GB quantized). The values within these columns likely represent the tokens per second (Toks/s) achieved for each quantization level.\n*   **RAM (MB):** Shows the RAM usage in megabytes for each backend during inference.\n*   **Time (ms):** Represents the average latency or time taken for processing a request in milliseconds.\n*   **Toks/s:**  The primary performance metric, indicating the number of tokens generated per second.\n\n**Data Observations (Examples):**\n\n*   \"Metal\" shows Toks/s values of 48, 171, and 187 for Q4\\_0, 1 GB, and 364 MB quantization respectively\nComment:       Hey LocalLlama! We've started publishing open-source model performance benchmarks (speed, RAM utilization, etc.) across various devices (iOS, Android, Mac, Windows). We currently maintain ~50 devices and will expand this to 100+ soon. We‚Äôre doing this because perf metrics determine the viability of shipping models in apps to users (no end-user wants crashing/slow AI features that hog up their specific device). Although benchmarks get posted in threads here and there, we feel like a more consolidated and standardized hub should probably exist. We figured we'd kickstart this since we already maintain this benchmarking infra/tooling at RunLocal for our enterprise customers. Note: We‚Äôve mostly focused on supporting model formats like Core ML, ONNX and TFLite to date, so a few things are still WIP for GGUF support.  Thought it would be cool to start with benchmarks for Qwen3 (Num Prefill Tokens=512, Num Generation Tokens=128). GGUFs are from Unsloth üêê Qwen3 GGUF benchmarks on laptops Qwen3 GGUF benchmarks on phones You can see more of the benchmark data for Qwen3 here. We realize there are so many variables (devices, backends, etc.) that interpreting the data is currently harder than it should be. We'll work on that! You can also see benchmarks for a few other models here. If you want to see benchmarks for any others, feel free to request them and we‚Äôll try to publish ASAP! Lastly, you can run your own benchmarks on our devices for free (limited to some degree to avoid our devices melting!). This free/public version is a bit of a frankenstein fork of our enterprise product, so any benchmarks you run would be private to your account. But if there's interest, we can add a way for you to also publish them so that the public benchmarks aren‚Äôt bottlenecked by us.  It‚Äôs still very early days for us with this, so please let us know what would make it better/cooler for the community: https://edgemeter.runlocal.ai/public/pipelines To more on-device AI in production! üí™ https://i.redd.it/aev5rgjazsye1.gif    submitted by    /u/intofuture   [link]   [comments] \nComment: Iphone 16's Metal performance is pretty impressive for 1.6b-q8. But I do wonder why q8's performance is faster than q4 in that particular setup. \nComment: It‚Äôs interesting to see that performance in m4 is pretty similar in both cpu and gpu \nComment: There's one edge factor you missed - on Metal backend when you get OOM you get completely wrong results. For example on Qwen3 8B Q4 your results are like this: - MacBook Pro M1, 8GB = 99232.83tok/s prefill, 2133.70tok/s generation - MacBook Pro M3, 8GB = 90508.66tok/s prefill, 2507.50tok/s generation If you wouldn't get OOM the correct results for that model should be around ~100-150tok/s prefill and ~10tok/s generation. Additionally, all results for RAM usage on Apple silicon \u0026amp; Metal are not correct. In terms of your UX/UI there's tons of stuff that should be improved. but to not make this into very long post I'll write about biggest problems that can be fixed rather easily. First, add option to hide columns, there's too much redundant information that should be possible to hide with just couple of clicks. Second, decide on some naming scheme for components and stick with it. https://preview.redd.it/8idjxghteuye1.png?width=91\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=12d1d9317495b745768b9dd012288be1eb804964 I would suggest to get rid of 'Apple'/'Bionic' names altogether - it just adds to complexity and cognitive load to a table that is already very dense. There is no non-Apple M1 in Macbooks or non-Bionic A12 in iPad, so you don't need to clarify that much in a first place and additionally this page is aimed at technical people. Exact same problem with Samsung/Google vs Snapdragon. Third, if both CPU and Metal failed don't create two entries. Table is 2x longer than it should be with results that are non-comparable to anything. Just combine it into one entry. \nComment: How to run on metal on iphone 16 pro? I have pocketpal app and how to switch from cpu to metal? \nComment: if i'm reading this correctly the load time on cpu is better than gpu/metal for macbook pro but the gpu/metal is less memory intensive? also metal perf on iphone 16 is pretty impressive. \nComment: How do I run this on Android? Rn it just crashes \nComment: Why is Q8 faster than Q4??? \nComment: For laptops, is vulkan using the igpu ? \nComment: according to this data on iphone 16 you have 24 t/s on Q8 and 22 t/s on Q4 why so tiny models? \nComment: The iPhone 16e is listed to have the A18 Pro SoC but it actually has the A18.  https://preview.redd.it/h1dx2hgphvye1.png?width=623\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=c2f8794bf27c6a9042b81040ebefa09873eae989 \n",
      "results": {
        "title": "Qwen3 Performance Benchmarks Across 50+ Devices: Toks/s, RAM, and Quantization Insights",
        "id": "t3_1kepuli",
        "summary": "LocalLlama has launched an open-source initiative to benchmark Qwen3's performance across ~50 devices (iOS, Android, Mac, Windows), focusing on metrics like tokens-per-second (Toks/s), RAM utilization, and quantization efficiency. The data includes evaluations of Qwen3 in multiple formats (Core ML, ONNX, TFLite) and quantization levels (Q4_0, 1GB, 364MB, 1.71GB). For example, on Apple's Metal backend, Qwen3's Q4_0 quantization achieves 48 Toks/s, while 364MB quantization reaches 187 Toks/s on certain devices. The project aims to address the critical need for standardized performance metrics to ensure viable on-device AI deployment, as suboptimal benchmarks could lead to app crashes or poor user experiences. However, the data's complexity and variability (e.g., backend differences, OOM errors) highlight challenges in interpretation. The initiative also allows users to run custom benchmarks via a public tool, though it's currently limited to avoid overloading devices. This effort underscores the growing importance of edge AI optimization, particularly as models like Qwen3 are deployed on resource-constrained hardware.",
        "commentSummary": "The community praised the initiative's ambition but raised technical concerns. Users noted impressive Metal backend performance on iPhone 16 (24 Toks/s for Q8) but questioned why Q8 quantization outperformed Q4 in some cases. Several highlighted issues like inaccurate RAM usage reports on Apple silicon and OOM errors causing erroneous results. Others criticized the table's complexity, suggesting UI improvements like column hiding and simplified naming conventions (e.g., removing 'Apple' or 'Bionic' from device labels). Practical questions emerged, such as how to enable Metal backend on iOS devices and why Android benchmarks frequently crash. The discussion emphasized the need for clearer data presentation and standardized metrics to maximize the project's utility for developers.",
        "isRelevant": true
      },
      "processingTimeMs": 20698
    },
    {
      "rawInput": "Title: Qwen 30B A3B performance degradation with KV quantization\nID: t3_1kewkno\nSummary: \n            I came\n            across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4\n            that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M\n            quantization. I tried to replicate locally but could I was not able,\n            model sometimes entered in a repetition loop even with dry sampling or came to wrong\n            conclusion after generating lots of thinking tokens. I was using\n            Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested\n            Bartowski Q5_K_S but it had no improvement. The model didn't entered in any\n            repetition loop but generated lots of thinking tokens without finding any\n            solution. Then I saw that sunpazed didn't used KV\n            quantization and tried the same: boom! First time right. It worked\n            with Q5_K_S and also with Q4_K_XL For who wants more details I leave\n            here a gist https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef\n            Do you have any report of performance degradation with long generations on\n            Qwen3 30B A3B and KV quantization?   \n            submitted by   \n            /u/fakezeta   [link]\n              [comments]\nImageDescription: \nComment: I came across this gist https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4 that shows how Qwen 30B can solve the OpenAI cypher test with Q4_K_M quantization. I tried to replicate locally but could I was not able, model sometimes entered in a repetition loop even with dry sampling or came to wrong conclusion after generating lots of thinking tokens. I was using Unsloth Q4_K_XL quantization, so I tought it could be the Dynamic quantization. I tested Bartowski Q5_K_S but it had no improvement. The model didn't entered in any repetition loop but generated lots of thinking tokens without finding any solution. Then I saw that sunpazed didn't used KV quantization and tried the same: boom! First time right. It worked with Q5_K_S and also with Q4_K_XL For who wants more details I leave here a gist https://gist.github.com/fakezeta/eaa5602c85b421eb255e6914a816e1ef Do you have any report of performance degradation with long generations on Qwen3 30B A3B and KV quantization?    submitted by    /u/fakezeta   [link]   [comments]\nComment: What KV quant level were you using? IMO on llama.cpp you shouldn‚Äôt push it past Q8_0. Q4_0 cache quant tanks quality in any model and especially models that heavily leverage GQA.  \nComment: I have one rule: I always test ALL new models without flash attention and with full 16bit KV cache. \nComment: Which KV quantization are you using? Don't have time to run this test right now, but I usually use -ctk q8_0 -ctv q5_1 (requires -DGGML_CUDA_FA_ALL_QUANTS=on) \nComment: Ive only tried KV quantization once and saw that any amount of it makes models super dumb. Not sure why anybody uses it tbh \nComment: Which quantization did you use initially? \nComment: Interested here since I'm running a q6 \nComment: Use these parameters: Thinking Mode Settings: Temperature = 0.6 Min_P = 0.0 Top_P = 0.95 TopK = 20 Non-Thinking Mode Settings: Temperature = 0.7  Min_P = 0.0 Top_P = 0.8 TopK = 20 \nComment: Could you please tell us how to disable KV cache quantisation? I'd also like to check the difference. What is the difference in the amount of memory used with KV running at fp16 in comparison with regular q4? \nComment: I‚Äôm confused. Isn‚Äôt K_M KV quantization? And yet you said Qwen 30b solved the rest with Q4 K_M? \nComment: Of course  Cache should always be fp16 even Q8 has degradation. Only flash attention is ok...ish ( as is fp16 ) \n",
      "results": {
        "title": "Qwen 30B A3B Performance Degradation with KV Quantization",
        "id": "t3_1kewkno",
        "summary": "A Reddit user reported significant performance issues with Qwen 30B A3B when using KV (key-value) quantization during long-generation tasks, particularly in solving the OpenAI cypher test. The user initially struggled to replicate results from a gist (https://gist.github.com/sunpazed/f5220310f120e3fc7ea8c1fb978ee7a4) that claimed Qwen 30B could solve the test with Q4_K_M quantization. After testing various quantization schemes‚ÄîUnsloth Q4_K_XL, Bartowski Q5_K_S‚Äîthe model either entered repetition loops or failed to find solutions. Disabling KV quantization (using Q5_K_S and Q4_K_XL without it) resolved the issue, suggesting KV quantization directly impacts performance. The post highlights a critical tradeoff between memory efficiency and task accuracy, particularly for models relying on GQA (Grouped Query Attention) or long-context reasoning. The user‚Äôs findings imply that KV quantization may introduce information loss in attention mechanisms, degrading the model‚Äôs ability to maintain coherent reasoning over extended generations.",
        "commentSummary": "The community emphasized the risks of aggressive KV quantization, with users warning that pushing beyond Q8_0 on llama.cpp can severely degrade model quality. Several commented that fp16 KV caches are essential for maintaining performance, while others criticized KV quantization as inherently flawed for attention-heavy tasks. A debate emerged about the utility of K_M quantization, with one user clarifying that Q4_K_M is a weight quantization method, not KV-specific. Practical advice included using -ctk q8_0 -ctv q5_1 for KV quantization and avoiding it altogether for critical tasks. The discussion underscored a broader concern about balancing optimization techniques with model reliability, particularly for large-scale reasoning tasks.",
        "isRelevant": true
      },
      "processingTimeMs": 18420
    },
    {
      "rawInput": "Title: QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.\nID: t3_1kenk4f\nSummary:   \n               All models are from Bartowski - q4km version\n            Test only HTML frontend. My assessment lauout quality from 0\n            to 10 Prompt \"Generate a beautiful\n            website for Steve's pc repair using a single html script.\"\n            QwQ 32b - 3/10 - poor layout but ..works , very\n            basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b\n            Qwen 3 32b - 6/10 - much better looks but still not too\n            complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b272399ba8140785feb429a196ecc5173\n            GLM-4-32b 9/10 - looks insanely good , quality layout like\n            sonnet 3.7 easily - 1500+ code lines https://preview.redd.it/3zj2lr2ahsye1.png?width=2469\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964\n            GLM-4-32b is insanely good for html code frontend. I say\n            that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most.\n            Other coding language like python , c , c++ or any other quality\n            of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on\n            the seme level but for html and JavaScript ... is GREAT.\n               submitted by   \n            /u/Healthy-Nebula-3603   [link]\n              [comments]\n            \nImageDescription: The image presents a side-by-side comparison of HTML code generated by three large language models (LLMs): QwQ 32b, Qwen 3 32b, and GLM-4-32B. The comparison is framed within a webpage context, suggesting an evaluation of their ability to produce functional and well-structured HTML for web development tasks.\n\nThe webpage itself has a simple layout with a header titled \"Our Services\" and two distinct content blocks below. The first block, labeled ‚ÄúHTML code Repair‚Äù, contains placeholder text indicating the task is to repair existing HTML code components. The second block, ‚ÄúVirus Removal‚Äù, describes a service for malware and virus removal.\n\nCrucially, the image focuses on the *code* generated to fulfill these service descriptions. Each model‚Äôs output is presented as a block of HTML code, likely intended to render the respective service description. The generated code is visible within rectangular boxes, allowing for direct visual comparison of syntax, structure, and verbosity.\n\nThe code blocks are lengthy, suggesting the models were prompted to generate relatively complex HTML structures. The presence of numerous `\u003cp\u003e` tags and potentially other structural elements (though details are obscured by the length) indicates that the models attempted to create complete HTML snippets.\n\nNotably, there is a significant amount of seemingly random characters and symbols interspersed within the \"Virus Removal\" code block for all three models. This appears to be an artifact of the generation process, potentially indicating issues with tokenization or output filtering. The repetition and lack of semantic meaning suggest it's not intended HTML content, but rather a failure in the model‚Äôs ability to produce clean code.\n\nThe image doesn't show any performance metrics or evaluation criteria, only the raw generated HTML code. The comparison is purely visual and relies on a user‚Äôs ability to assess the quality of the code based on its structure, readability, and correctness. The presence of the gibberish in the\nComment:   \n               All models are from Bartowski - q4km version\n            Test only HTML frontend. My assessment lauout quality from 0\n            to 10 Prompt \"Generate a beautiful\n            website for Steve's pc repair using a single html script.\"\n            QwQ 32b - 3/10 - poor layout but ..works , very\n            basic - 250 line of code https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=f65d811c4859178fe80cbcb50312217ba5591c5b\n            Qwen 3 32b - 6/10 - much better looks but still not too\n            complex layout - 310 lines of the code https://preview.redd.it/z9qixbh8hsye1.png?width=2461\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=29e6bb4b272399ba8140785feb429a196ecc5173\n            GLM-4-32b 9/10 - looks insanely good , quality layout like\n            sonnet 3.7 easily - 1500+ code lines https://preview.redd.it/3zj2lr2ahsye1.png?width=2469\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=93825986cdf77778f9e7c5dcaf19b51b4a4a6964\n            GLM-4-32b is insanely good for html code frontend. I say\n            that model is VERY GOOD ONLY IN THIS FIELD and JavaScript at most.\n            Other coding language like python , c , c++ or any other quality\n            of the code will be on the level of qwen 2.5 32b coder, reasoning and math also is on\n            the seme level but for html and JavaScript ... is GREAT.\n               submitted by   \n            /u/Healthy-Nebula-3603   [link]\n              [comments]\n            \nComment: Yeah, I\n            have also tried to generate webpages with a couple of models, like GLM-4, Qwen3, Phi-4\n            Reasoning, etc. GLM-4 is so far the clear winner at these tasks. It's a gem in\n            my model collection. \nComment: I've created a list of one-shot generated HTML\n            pages using different models, big and small. https://blog.kekepower.com/ai/\n            \nComment: Ironically, so far userscript (javascript): Qwen 3 32B\n            \u0026gt; GLM-4-0414. Don't get me wrong. I love GLM-4-0414, but it feels like it\n            lacked the required understanding for my particular requests that Qwen 3 32B understood\n            well. \nComment: Whats the\n            temp? Did you rerun multiple times? \nComment: Why not\n            try a slightly more complex task? E.g. a mini-game? \n            Create a single-HTML-page game using Babylon.js where the player controls a\n            ship and moves about the open sea exploring islands to find treasure. A single, small\n            map with 3 islands of which only one has the treasure is enough\n             In the first reply, the camera works, but WASD\n            didn't. I copied-pasted the errors from the console a couple times and WASD\n            works now. It looks terrible, but I guess that's expected without external\n            assets. https://preview.redd.it/lf5foomq1tye1.png?width=1288\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b02ed407b3fbef31bf6d83aae5491cacaa93e150\n            Edit: This is my AWQ quant of Qwen-32B. \nComment: GLM falls\n            flat on its face when I try to continue developing after the first prompt.\n            It feels like a model trained (very well) for one-shots\n            \nComment: Try\n            UIGEN-T2 for html generation! There's also a react model. \n            https://huggingface.co/Tesslate/UIGEN-T2\n            \nComment: Would be\n            interesting to see how these results compare to the recent Tesslate/UIGEN-T2-7B.\n            It's a tuned version of Qwen 2.5 Coder 7B specifically for UI\n            generation. \nComment: What\n            quants what context window size? Ollama default size will kill QWQ reasoning if you\n            don‚Äôt know how to set it up properly. \nComment: Still\n            kind of freaked out that smaller Qwen 3 models are probably as good at website\n            development as I was as a teenager. And a damn sight quicker too.\n            \nComment: Thanks\n            for posting the eval. Would be curious to see the prompt used as well.\n            \nComment: Interesting. I ran the same input on QwQ with these\n            settings: Temp: 0.6 Top p: 0.95 Min p: 0.0 Top k: 40 And quite a bit different\n            output. Output: https://pastebin.com/Ntc8QQfH\n            \nComment: GPT 4.1\n            is clearly the winner here in my opinion as well as claude sonnet 3.7\n            \n",
      "results": {
        "title": "QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison",
        "id": "t3_1kenk4f",
        "summary": "This post presents a user-driven comparison of three large language models (LLMs) - QwQ 32b, Qwen 3 32b, and GLM-4-32B - focused specifically on their ability to generate HTML code for a 'Steve's PC Repair' website. The evaluation uses a standardized prompt and assesses layout quality on a 0-10 scale, with detailed code analysis. GLM-4-32B scores highest (9/10) with 1,500+ lines of high-quality HTML resembling Sonnet 3.7's output, while QwQ 32b (3/10) produces minimalistic code. The comparison highlights significant differences in frontend capabilities, with GLM-4-32B excelling in HTML/JavaScript but reportedly underperforming in other domains like Python/C++ code generation. The test uses Bartowski's q4km quantization, and the results include direct code comparisons via image links. The post sparks discussion about model specialization, with users noting GLM-4's superiority in one-shot HTML tasks but limitations in iterative development.",
        "commentSummary": "The community validates the comparison's technical value, with users sharing similar experiences of GLM-4-32B's HTML prowess. Some note that while GLM-4 excels in one-shot tasks, it struggles with iterative development. Others compare it to Qwen 3 32b and mention limitations in JavaScript tasks. A user suggests trying specialized models like UIGEN-T2 for UI generation, while another questions the impact of quantization (q4km) and context window size. The discussion also touches on model-specific quirks - Qwen 3's reliability in certain tasks vs. GLM-4's 'one-shot' focus - and curiosity about how these results compare to other models like Tesslate/UIGEN-T2-7B. Several users request more details about the exact prompt and testing parameters.",
        "isRelevant": true
      },
      "processingTimeMs": 22130
    },
    {
      "rawInput": "Title: LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!\nID: t3_1keoint\nSummary:   \n               You\n            can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B\n            and 30B) mainline\n            llama.cpp just got a boost for fully offloaded Qwen3 MoE (single\n            expert) tl;dr; I highly recommend\n            doing a git pull and re-building your\n            ik_llama.cpp or llama.cpp repo to take\n            advantage of recent major performance improvements just released. The\n            friendly competition between these amazing projects is producing delicious fruit for the\n            whole GGUF loving r/LocalLLaMA community! If\n            you have enough VRAM to fully offload and already have an existing\n            \"normal\" quant of Qwen3 MoE then you'll get a little more\n            speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take\n            advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork!\n            Details I spent yesterday compiling and running benhmarks\n            on the newest versions of both ik_llama.cpp and\n            mainline llama.cpp.\n            For those that don't know, ikawrakow was an early contributor to\n            mainline llama.cpp working on important features that have since trickled down into\n            ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my\n            understanding) the ik_llama.cpp fork was built and has a number\n            of interesting features including SotA iqN_k quantizations that\n            pack in a lot of quality for the size while retaining good speed performance. (These new\n            quants are not available in ollma, lmstudio, koboldcpp,\n            etc.) A few recent PRs made by ikawrakow to\n            ik_llama.cpp and by JohannesGaessler to mainline have\n            boosted performance across the board and especially on CUDA with\n            Flash Attention implementations for Grouped Query Attention (GQA) models and also\n            Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B\n            releases! References  ikawrakow/ik_llama.cpp/pull/370\n                submitted by    /u/VoidAlchemy \n             [link]\n              [comments]\n            \nImageDescription: The image presents two line charts comparing the performance of different implementations of LLaMA (likely Large Language Model Meta AI) inference, specifically focusing on throughput versus KV Cache size. The post title indicates improvements in both \"ik\" and \"mainline llama.cpp\".\n\n**Chart 1 (Left):** This chart plots ‚ÄúThroughput (tokens/s)‚Äù on the y-axis against \"KV Cache Size (N_KV)\" on the x-axis, ranging from 0 to approximately 30,000.  Four lines are displayed, each representing a different configuration or implementation:\n\n*   **Red Line:** Shows consistently high throughput (around 40-50 tokens/s) with minimal variation across the entire KV Cache size range. This likely represents a baseline or highly optimized configuration.\n*   **Orange Line:** Starts at similar throughput to the red line, but exhibits a gradual decline in performance as KV Cache size increases. The decrease is relatively small, dropping to around 30 tokens/s at the maximum cache size.\n*   **Light Orange Line:** Similar to the orange line, but starts with slightly lower throughput and exhibits a more pronounced decline as KV Cache size increases.\n*   **Dark Orange Line:** Starts with the lowest throughput of the four lines and shows a significant decline in performance as KV Cache size increases.\n\n**Chart 2 (Right):** This chart mirrors the structure of the first, also plotting ‚ÄúThroughput (tokens/s)‚Äù against \"KV Cache Size (N_KV)\". However, the y-axis scale is different, ranging from approximately 5 to 10 tokens/s.  Four lines are also present:\n\n*   **Red Line:** Shows a relatively stable throughput around 8-9 tokens/s.\n*   **Orange Line:** Starts at a slightly lower throughput than the red line and exhibits a gradual decline as KV Cache size increases.\n*   **\nComment:       You can't go wrong with ik_llama.cpp fork for hybrid CPU+GPU of Qwen3 MoE (both 235B and 30B) mainline llama.cpp just got a boost for fully offloaded Qwen3 MoE (single expert) tl;dr; I highly recommend doing a git pull and re-building your ik_llama.cpp or llama.cpp repo to take advantage of recent major performance improvements just released. The friendly competition between these amazing projects is producing delicious fruit for the whole GGUF loving r/LocalLLaMA community! If you have enough VRAM to fully offload and already have an existing \"normal\" quant of Qwen3 MoE then you'll get a little more speed out of mainline llama.cpp. If you are doing hybrid CPU+GPU offload or want to take advantage of the new SotA iqN_k quants, then check out ik_llama.cpp fork! Details I spent yesterday compiling and running benhmarks on the newest versions of both ik_llama.cpp and mainline llama.cpp. For those that don't know, ikawrakow was an early contributor to mainline llama.cpp working on important features that have since trickled down into ollama, lmstudio, koboldcpp etc. At some point (presumably for reasons beyond my understanding) the ik_llama.cpp fork was built and has a number of interesting features including SotA iqN_k quantizations that pack in a lot of quality for the size while retaining good speed performance. (These new quants are not available in ollma, lmstudio, koboldcpp, etc.) A few recent PRs made by ikawrakow to ik_llama.cpp and by JohannesGaessler to mainline have boosted performance across the board and especially on CUDA with Flash Attention implementations for Grouped Query Attention (GQA) models and also Mixutre of Experts (MoEs) like the recent and amazing Qwen3 235B and 30B releases! References  ikawrakow/ik_llama.cpp/pull/370     submitted by    /u/VoidAlchemy   [link]   [comments] \nComment: I'm currently running ik_llama.cpp with Qwen3-235B-A22 on a Xeon E5-2680v4, that's a 10 year old CPU with 128GB ddr4 memory, and a single RTX3090. I'm getting 7 tok/s generation, very usable if you don't use reasoning. BTW the server is multi-GPU but ik_llama.cpp just crash trying to use multiple-gpus, but I don't think it would improve speed a lot, as the CPU is always the bottleneck. \nComment: Could you explain how to read your pictures? I see orange plot below red plot, so ik_llama.cpp is slower than llama.cpp? \nComment: Can you post some of the commands you use for the benchmarks? I want to tinker to see what is best for my use case \nComment: Oh, just updated. My rig is busy for running deepseek \u0026amp; ik_llama (1 week jobs). I will update after that :) \nComment: Maybe GGUF will now give same speed as MLX on Mac devices \nComment: I have a 3090. Doesn't this say it's slower, not faster? \nComment: https://preview.redd.it/0zroyhg1qsye1.png?width=3404\u0026amp;format=png\u0026amp;auto=webp\u0026amp;s=b3e55128b1aac3f6d2ddfbd22597b9cd6d7dd02c In my limited testing you probably want to go with ik_llama.cpp for fully offloaded non-MoE models like the recent GLM-4 which is crazy efficient on kv-cache VRAM usage due to its GQA design. \nComment: I just pulled and rebuilt and I'm now actually going about 15 tps slower. My previous build was from about a week ago, and I was getting an eval time of about 54 tps. Now I'm only getting 39 tokens per second, so pretty significant drop. I just downloaded the latest unsloth model I'm running on 2 3090s, using this command: ``` .\\bin\\Release\\llama-server.exe -m C:\\shared-drive\\llm_models\\unsloth-2-Qwen3-30B-A3B-128K-Q8_0.gguf --host 0.0.0.0 --ctx-size 50000 --n-predict 10000 --jinja --tensor-split 14,14 --top_k 20 --min_p 0.0 --top_p 0.8 --flash-attn --n-gpu-layers 9999 --threads 24 ``` Prompt: \"tell me a 2 paragraph story\" \nComment: How close is llamacpp to vLLM and exllama now? \nComment: Seems like it is related to CUDA only, so I guess only for people with Nvidia cards and not folks on Apple Silicon and others. \n",
      "results": {
        "title": "LLaMA Gotta Go Fast! ik_llama.cpp and Mainline Llama.cpp See Major Performance Boosts for Qwen3 MoE",
        "id": "t3_1keoint",
        "summary": "The open-source LLaMA inference frameworks 'ik_llama.cpp' and mainline 'llama.cpp' have seen significant performance improvements, particularly for Qwen3 MoE models (235B and 30B). These updates focus on optimizing CUDA acceleration with Flash Attention for Grouped Query Attention (GQA) models and Mixture of Experts (MoE) architectures. Mainline llama.cpp now offers better throughput for fully offloaded Qwen3 MoE models, while ik_llama.cpp retains its edge with SOTA 'iqN_k' quantizations that balance model quality and speed. The changes are critical for users running large models on GPU-equipped systems, with benchmarks showing up to 50 tokens/s throughput improvements under optimal conditions. The 'ik' fork also addresses hybrid CPU+GPU workflows, which remain essential for systems with limited VRAM. These updates highlight the competitive innovation in the GGUF model format ecosystem, benefiting the r/LocalLLaMA community.",
        "commentSummary": "The post generated lively discussion about the practical implications of these updates. Some users reported significant speed gains, while others noted instability issues like crashes when using multi-GPU setups with ik_llama.cpp. A few developers expressed confusion about the benchmark visuals, with one questioning why the 'ik' implementation appeared slower in some charts. Technical debates emerged about the tradeoffs between quantization methods, with users comparing 'iqN_k' to traditional Q8_0 formats. Concerns were raised about the CUDA-specific nature of the improvements, limiting benefits for Apple Silicon users. Several posters requested detailed benchmark commands to replicate results, while others speculated about the long-term competitiveness of llama.cpp against frameworks like vLLM and Exllama. The community remains divided between those prioritizing raw speed and those valuing flexibility in hybrid CPU/GPU workflows.",
        "isRelevant": true
      },
      "processingTimeMs": 20076
    },
    {
      "rawInput": "Title: Apparently shipping AI platforms is a thing now as per this post from the Qwen X\n            account\nID: t3_1kebauw\nSummary:   \n                \n            submitted by    /u/MushroomGecko\n              [link]\n              [comments]\n            \nImageDescription: ## Image Analysis: Qwen X Shipping Platform Post\n\nThe image appears to be a promotional graphic, likely intended for social media, advertising the availability of shipping for AI platforms ‚Äì specifically referencing Qwen X. The dominant visual element is two cartoon characters: a light-brown sloth and a brown bear.\n\nThe **sloth** is depicted with large, expressive eyes and clasped hands, suggesting anticipation or excitement. It's positioned on the left side of the image.\n\nThe **bear** is shown presenting a red flower to the sloth, wearing a plain white t-shirt. It has a slightly more reserved expression than the sloth. Both characters have simplified, rounded designs typical of sticker or emoji-style artwork.\n\nA significant portion of the lower half of the image is obscured by a repeating pattern of unreadable characters. This appears to be non-English text, possibly filler or decorative elements. The repetition suggests it isn't meaningful information intended for the viewer.\n\nThe overall aesthetic is playful and approachable, likely aiming to soften the technical complexity of deploying AI models. The use of cartoon characters suggests a focus on ease-of-use and accessibility for developers or users. The flower presentation could symbolize the \"delivery\" of AI capabilities through the shipping platform.\n\nThe context provided (\"shipping AI platforms\") indicates that Qwen X likely offers a service to facilitate the deployment and access of their models, potentially through APIs or pre-packaged solutions. The image serves as a lighthearted announcement of this availability. There are no charts, code snippets or UI elements visible in the image itself; it's purely a promotional graphic.\nComment:         submitted by    /u/MushroomGecko   [link]   [comments] \nComment: Ah yes, the AI romcom. \nComment: theres lore now? \nComment: -Ok, lets see, i want to make love to you-. -But...wait. no no \nComment: What do you mean by \"shipping AI platforms\"? \nComment: Can't wait for the competitions to see which model writes the best fan fiction \nComment: Should AI companies also create their own VTuber characters that use their models? \nComment: Qwen guy looks mean. His expression is like he's never felt love. \nComment: I ship it! \nComment: And those two ship often \nComment: Hell yeah \nComment: Has anyone noticed better performance for similar quants of the same model by different creators? \nComment: Oh no \nComment: Love the contribution to the OSS these guys make!! Pure awesomeness \nComment: ü§¶üèª‚Äç‚ôÇÔ∏è \nComment: Anyone feeding into this just entirely lacks a social life, why are some of you like this? \n",
      "results": {
        "title": "Apparently shipping AI platforms is a thing now as per this post from the Qwen X account",
        "id": "t3_1kebauw",
        "summary": "This post appears to be a promotional image from Alibaba's Qwen X team announcing 'shipping AI platforms,' but it provides no technical details about the platform itself. The image features cartoon characters (a sloth and bear) in a whimsical context, with no mention of model architectures, performance metrics, deployment infrastructure, or specific AI capabilities. The phrase 'shipping AI platforms' is vague‚Äîcould refer to model deployment, API services, or pre-trained solutions‚Äîbut the post lacks concrete information about what's being shipped, how it works, or why it matters. The absence of technical specifications, benchmarks, or novel techniques makes it impossible to assess the significance of this claim. The image's decorative elements and non-English text fragments further suggest it is not a technical announcement but rather a marketing stunt or internal joke.",
        "commentSummary": "The comments highlight confusion and humor around the post's ambiguity. Users jokingly interpreted 'shipping AI platforms' as romantic pairings (e.g., 'AI romcom,' 'ship it'), while others questioned the technical substance of the claim. Some criticized the post as lacking depth ('Anyone feeding into this just entirely lacks a social life'), and others speculated about potential implications for AI model distribution. The discussion reveals a community eager for tangible technical updates but frustrated by vague marketing language. No meaningful technical analysis emerged from the comments.",
        "isRelevant": false
      },
      "processingTimeMs": 12731
    },
    {
      "rawInput": "Title: IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models\nID: t3_1kedu0d\nSummary:   \n                \n            submitted by   \n            /u/ab2377   [link]\n              [comments]\n            \nImageDescription: \nComment:   \n                \n            submitted by   \n            /u/ab2377   [link]\n              [comments]\n            \nComment: Hope they\n            can release a larger one like 30b-a3b  \nComment: so a new\n            architecture, more moe goodness  \"Whereas prior generations\n            of Granite LLMs utilized a conventional transformer architecture, all models in the\n            Granite 4.0 family utilize a new hybrid Mamba-2/Transformer architecture, marrying the\n            speed and efficiency of Mamba with the precision of transformer-based self-attention.\n            Granite 4.0 Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts\n            (MoE) model, with 7B total parameters and only 1B active parameters at inference\n            time. Many of the innovations informing the Granite 4 architecture\n            arose from IBM Research‚Äôs collaboration with the original Mamba creators on Bamba, an\n            experimental open source hybrid model whose successor (Bamba v2) was released earlier\n            this week.\" \nComment: Please\n            look here: https://huggingface.co/ibm-granite/granite-4.0-tiny-preview/discussions/2\n            gabegoodhart IBM\n            Granite org 1\n            day ago Since this model is hot-off-the-press, we\n            don't have inference support in llama.cpp yet.\n            I'm actively working on it, but since this is one of the first major models\n            using a hybrid-recurrent architecture, there are a number of in-flight architectural\n            changes in the codebase that need to all meet up to get this supported. We'll\n            keep you posted! gabegoodhart IBM\n            Granite org 1\n            day ago We definitely expect the model quality to improve\n            beyond this preview. So far, this preview checkpoint has been trained on ~2.5T tokens,\n            but it will continue to train up to ~15T tokens before final release.\n            \nComment: i hope we\n            can see some larger models too! I really want them to scale those more experimental\n            architectures and see where it leads. I think there is huge potential in combining\n            attention with hidden state models. attention to understand context, hidden state to\n            think ahead, remember key information etc. \nComment: Read the\n            full thing. It‚Äôs worth it. \nComment: Holy,\n            this actually looks really good. IBM might actually be able to catch up with Alibaba\n            with this one. \nComment: Neat but\n            unless folks really start working to help add support for mamba architectures to\n            llama.cpp it'll be dead on arrival. It would be great to see\n            the folks at /u/IBM step up and help out\n            llama.cpp to support things like this. \nComment: \n            The Granite 4.0 architecture uses no positional encoding (NoPE). Our testing\n            demonstrates convincingly that this has had no adverse effect on long-context\n            performance.   This is interesting. Are there any\n            papers that explain why this still works? \nComment: Looking\n            very promising... \nComment: Is IBM\n            going to be the silent winner? It‚Äôs impressive that their tiny model is 8b MOE and\n            likely to perform at the same level as their previous dense 8b: Granite 4.0\n            Tiny-Preview, specifically, is a fine-grained hybrid mixture of experts (MoE) model,\n            with 7B total parameters and only 1B active parameters at inference time.\n            I hope their efforts attempt to improve in https://fiction.live/stories/Fiction-liveBench-April-6-2025/oQdzQvKHw8JyXbN87\n            and not just passkey testing. \nComment: I'm just a dreamer without much background in\n            ML stuff. Can anyone with sense comment on how likely it is we'll ever see\n            something that might be so efficient it'll run well on CPU? I mean, this model\n            already sounds pretty exciting from an efficiency perspective. Wondering if\n            we've exhausted architectural changes that would e.g. reduce memory bandwidth\n            requirements \nComment: ‚ÄúWe‚Äôre\n            excited to continue pre-training Granite 4.0 Tiny, given such promising results so early\n            in the process. We‚Äôre also excited to apply our learnings from post-training Granite\n            3.3, particularly with regard to reasoning capabilities and complex instruction\n            following, to the new models. Like its predecessors in Granite 3.2 and Granite 3.3,\n            Granite 4.0 Tiny Preview offers toggleablethinking on andthinking off functionality\n            (though its reasoning-focused post-training is very much incomplete).‚Äù\n            I hope some of this involves interacting with fictional text in a creative\n            fashion: scene summaries, character profiles, plot outlining, hypothetical change\n            impacts - books are great datasets just like large code bases and just need a good set\n            of training data ‚Äî use Guttenberg public domain books that are modernized with AI and\n            then create training around the above elements. \nComment: Now if\n            only we could get IBM to sell a version of their AI card to the public\n            \nComment: ibm doing\n            better work than meta theyre surprisingly becoming a big player in open source (for\n            small models) \nComment: I wonder\n            what is prompt processing speed for semi-recurrent stuff compared to transformers.\n            Transformers have fantastic prompt processing speed like 1000t/s easy even on crap like\n            3060, but slow down during token generation as context grows. This seems to be the other\n            way around, slow PP but fast TG. I might be completely\n            wrong. \nComment: Large\n            datasets: all of Harry Potter series asking questions like, what would have to change in\n            the series for Harry to end up with Hermione or for Voldemort to win. It‚Äôs a series\n            everyone knows fairly well and requires details in the story and the story\n            whole. \nComment: I\n            remember seeing this model a few days ago. There's no gguf so i cant try it out.\n            I guess there's not a lot of interest in this moe or it's not currently\n            possibly to make ggufs for it at the moment.  Webui stopped working\n            for me last year after i updated it and I've never been able to get it working\n            right since then, so been using lm studio appimages. That program runs everything good\n            for me but only runs ggufs. \nComment: Yall need\n            to learn Transformers and stop hating on llama.cpp \n",
      "results": {
        "title": "IBM Granite 4.0 Tiny Preview: A sneak peek at the next generation of Granite models",
        "id": "t3_1kedu0d",
        "summary": "IBM has unveiled the Granite 4.0 Tiny-Preview, a groundbreaking model featuring a hybrid Mamba-2/Transformer architecture that combines the efficiency of Mamba's state-space sequences with transformer-based self-attention. This fine-grained mixture-of-experts (MoE) model boasts 7B total parameters but only activates 1B during inference, drastically reducing computational overhead. The architecture stems from IBM Research's collaboration with Mamba creators on Bamba, an experimental hybrid model. Training data includes 2.5T tokens so far, with plans to scale up to 15T before final release. Notably, the model eliminates positional encoding (NoPE) without compromising long-context performance, a design choice that challenges conventional transformer norms. Early benchmarks suggest it could match or exceed previous dense 8B models like Granite 3.3 while maintaining superior efficiency.",
        "commentSummary": "The community is energized by the technical innovation but cautious about practical adoption. Users highlight excitement over the hybrid architecture's potential to merge attention mechanisms with recurrent state management, enabling better context handling and forward-looking capabilities. However, concerns persist about limited inference support in popular frameworks like llama.cpp, with developers noting architectural changes that delay compatibility. There's strong demand for larger variants (e.g., 30B-A3B) and hopes IBM will expand open-source efforts. Some critics question whether the model's efficiency gains will translate to real-world CPU performance, while others speculate about its potential for creative tasks like fictional text analysis. The discussion underscores both enthusiasm for IBM's technical strides and frustration over ecosystem integration challenges.",
        "isRelevant": true
      },
      "processingTimeMs": 16362
    },
    {
      "rawInput": "Title: Qwen3 no reasoning vs Qwen2.5\nID: t3_1kegrce\nSummary: \n            It seems\n            evident that Qwen3 with reasoning beats Qwen2.5. But I wonder if the Qwen3 dense models\n            with reasoning turned off also outperforms Qwen2.5. Essentially what I am wondering is\n            if the improvements mostly come from the reasoning.    submitted by    /u/No-Bicycle-132\n              [link]\n              [comments]\nImageDescription: \nComment: It seems evident that Qwen3 with reasoning beats Qwen2.5. But I wonder if the Qwen3 dense models with reasoning turned off also outperforms Qwen2.5. Essentially what I am wondering is if the improvements mostly come from the reasoning.    submitted by    /u/No-Bicycle-132   [link]   [comments]\nComment: On aider polyglot there was a huge boost from 16.4% of qwen 2.5 coder whole to qwen 3 32B (no thinking) achieving 45.8% being ahead of gpt 4.5. \nComment: Don't stop at wondering. Why don't you test it and share your result with us? \nComment: https://www.reddit.com/r/LocalLLaMA/comments/1keh542/qwen3_on_dubesor_benchmark/ \nComment: Depends on the task. For code autocomplete Qwen/Qwen3-14B-AWQ nothink is awful. I like Qwen2.5-coder:14b. Additionally: some quants might be broken. \nComment: The 2.5 coders are better at complex one shots. 3.0 seems to generalize better and retains logic over a multiturn edit. My work involves updating lots of legacy Fortran and cobol that is written with very specific formatting and comment practices. 3.0 is the first open model that can run reasonably at 48gb vram that can reliably port my code. Also I think, for coding one shot diffs, reasoning turned off produces better results. \nComment: I have compared them far too little to be able to draw a serious conclusion, but from the very few comparisons I have made in coding, Qwen3 (no thinking) outputs better code, more accordingly to the prompt, than Qwen2.5. \nComment: The short answer is it entirely depends on your use case. In my limited testing, their overall performance was pretty close, with Qwen 3 probably being better overall.  I know the benchmarks say otherwise, but when translating Japanese to English, I found Qwen 2.5 to sound more natural.  However, when summarizing short stories, Qwen 2.5 dissected the story like a technical manual, whereas Qwen 3 wrote (or tried to write) in the tone of the original story.  Qwen 3 seems to lose less when quantized than Qwen 2.5. I was shocked at how well Qwen 3 32b functioned even down to IQ2 (except for factual retrieval which as usual takes a big hit). Coding, logical puzzles, and problem-solving seemed like a toss up. They both did it with more or less the same success; although, enabling reason will likely give Qwen 3 the edge. \nComment: Yes from what I have seen for apples to apples. But the 2.5 coding models will probably still hold tier own vs regular 3 models with thinking off. \nComment: They do. Qwen3 8b outperforms 7b 2.5; at least because of that extra 1b. \n",
      "results": {
        "title": "Qwen3 reasoning vs Qwen2.5: Are improvements from reasoning or architecture?",
        "id": "t3_1kegrce",
        "summary": "This discussion explores whether Qwen3's performance gains over Qwen2.5 stem from its reasoning capabilities or architectural improvements. Users note that Qwen3's 'no thinking' mode outperforms Qwen2.5 in specific tasks like code generation (e.g., 45.8% accuracy on Aider Polyglot vs 16.4% for Qwen2.5) and retains better logic in multi-turn editing scenarios. Technical details include comparisons of model sizes (32B vs 14B), quantization effects (IQ2 performance), and task-specific benchmarks. While some report Qwen3's reasoning mode excelling in problem-solving, others find Qwen2.5-coder 14B superior for complex one-shot coding tasks. The debate highlights tradeoffs between reasoning overhead, quantization fidelity, and task-specific optimization.",
        "commentSummary": "The community is split on Qwen3's advantages, with users emphasizing context-dependent performance. Some praise Qwen3's generalization and quantization resilience, while others prefer Qwen2.5 for coding-specific tasks. Critics question whether reasoning is the primary driver, noting that Qwen3's larger size (e.g., 8B vs 7B) might explain some gains. A recurring theme is the need for task-specific evaluation, as Qwen3 excels in storytelling summarization but struggles with Japanese translation nuances compared to Qwen2.5. The discussion underscores the importance of benchmarking across diverse workloads.",
        "isRelevant": true
      },
      "processingTimeMs": 17322
    }
  ],
  "imageSummaries": [
    {
      "imageURL": "https://preview.redd.it/gefvhv84qsye1.png?width=640\u0026crop=smart\u0026auto=webp\u0026s=235b3e1de1b7df4bd1bc1f7519f84b5259303d05",
      "imageDescription": "## Image Analysis: Visa \"Vibe Coder\" Job Posting Detail\n\nThe image presents a screenshot of text, likely extracted from a job posting by Visa. The post is soliciting candidates with specific technical skills, framed as \"vibe coders.\" It appears to be a section detailing desired experience.\n\nThe text is formatted as a bulleted list, outlining requirements and preferred qualifications. Key technologies explicitly mentioned include:\n\n*   **Vector Databases:** Pinecone, ChromaDB are named. This indicates a focus on semantic search and retrieval using vector embeddings.\n*   **Embedding Models:** The posting specifically requests experience with embedding models, suggesting the role involves working with representations of data for similarity comparisons.\n*   **Containerization:** Docker is explicitly listed, indicating a need for experience in packaging and deploying applications within containers. Kubernetes is also mentioned, implying familiarity with container orchestration at scale.\n*   **Frontend Frameworks:** Lovable and V0 are listed, suggesting a preference for modern frontend tooling. These frameworks likely represent a specific tech stack the team utilizes.\n*   **Data Science:** Experience in Data Science is considered a \"big plus,\" indicating the role may involve data analysis or model building.\n*   **Problem Solving:** Strong problem-solving skills are explicitly requested, highlighting the need for analytical thinking and debugging capabilities.\n\nThe text also includes a large block of unreadable characters at the bottom, likely representing encoding errors or corrupted data. \n\nA Reddit logo (specifically the \"technical\" subreddit icon) is visible in the bottom right corner, indicating this screenshot was likely taken from a Reddit post discussing the job opportunity. The overall impression is that Visa is seeking engineers with experience in modern machine learning infrastructure, particularly around vector databases and embedding models, combined with frontend development skills.",
      "title": "Visa is looking for vibe coders - thoughts?",
      "entryID": "t3_1keolh9",
      "processingTimeMs": 35146
    },
    {
      "imageURL": "https://preview.redd.it/627wnr5emsye1.jpeg?width=640\u0026crop=smart\u0026auto=webp\u0026s=b896f5165e878160c1e104137518ab1d80b3addc",
      "imageDescription": "## Image Analysis: UI-Tars-1.5 Reasoning Interface\n\nThe image displays a screenshot of a user interface (UI) seemingly built for interacting with a Large Language Model (LLM), specifically identified as \"UI-Tars-1.5\". The UI appears to be running within a macOS environment, evidenced by the window chrome and overall aesthetic.\n\nThe central portion of the screen is dominated by a panel titled \"LLM Provider and Model\". This section allows user selection of an LLM. A dropdown menu is present, currently displaying \"Select model or choose 'Custom model...'\". Below this dropdown is a text input field labeled ‚ÄúCustom Model Name‚Äù, with the placeholder text \"ByteDance-Seed/UI-Tars-1.5-7B\" pre-populated, suggesting the user is interacting with a model hosted by ByteDance. A \"Provider Base URL\" field is also visible, though its content is obscured by a long string of non-English characters.\n\nTo the right of this selection panel is a text area labeled \"UI-TARS Thoughts\". This section displays the LLM's reasoning process. The text indicates a conversational flow where the user asked Google to explain its purpose, and the LLM is responding. The response begins with \"It seems that Google wants to explain the purpose of read through all this information right now. I just want to search for a repository, so I'll go ahead and select 'All' first.\"  An arrow symbol (‚Üì) suggests further reasoning or action is pending.\n\nAt the bottom of the screen, a \"Textbox\" input field is present with the prompt ‚ÄúAsk me to perform tasks in a virtual macOS environment‚Äù. This indicates the LLM is capable of executing commands or responding to requests within a simulated operating system.\n\nThe UI design appears minimalist, utilizing a light background and clear typography. The presence of the \"Custom model...\" option suggests flexibility in selecting or specifying LLMs,",
      "title": "UI-Tars-1.5 reasoning never fails to entertain me.",
      "entryID": "t3_1keo3te",
      "processingTimeMs": 31048
    },
    {
      "imageURL": "https://preview.redd.it/z5qxhpc1zsye1.png?width=913\u0026format=png\u0026auto=webp\u0026s=c48aa6c5c753c7dc74c4397aac34f92383d17afe",
      "imageDescription": "## Image Analysis: Qwen3 Performance Benchmarks\n\nThe image presents a detailed performance comparison of the Qwen3 large language model across approximately 50 different devices. The data is organized in a tabular format, likely extracted from a spreadsheet or similar tool.\n\n**Columns:** The table features the following columns: \"Backend\", \"Count\", \"Q4_0\", \"1 GB\", \"364 MB\", \"1.71 GB\", \"RAM (MB)\", \"Time (ms)\", and \"Toks/s\".\n\n*   **Backend:** Lists the hardware/software configurations used for testing. Examples include \"Metal\" (likely Apple GPUs), ‚ÄúGoogle Pixel 8 (2023)\", \"SoC: A13 Bionic\", ‚ÄúRAM: 4GB‚Äù, and various Android devices with specific chipsets.\n*   **Count:** Indicates the number of samples/runs performed for each backend configuration.\n*   **Q4\\_0, 1 GB, 364 MB, 1.71 GB:** These columns represent different quantization levels of the Qwen3 model (4-bit, 1GB quantized, 364MB quantized and 1.71 GB quantized). The values within these columns likely represent the tokens per second (Toks/s) achieved for each quantization level.\n*   **RAM (MB):** Shows the RAM usage in megabytes for each backend during inference.\n*   **Time (ms):** Represents the average latency or time taken for processing a request in milliseconds.\n*   **Toks/s:**  The primary performance metric, indicating the number of tokens generated per second.\n\n**Data Observations (Examples):**\n\n*   \"Metal\" shows Toks/s values of 48, 171, and 187 for Q4\\_0, 1 GB, and 364 MB quantization respectively",
      "title": "Qwen3 performance benchmarks (toks/s, RAM utilization, etc.) on ~50 devices (iOS,\n            Android, Mac, Windows)",
      "entryID": "t3_1kepuli",
      "processingTimeMs": 31193
    },
    {
      "imageURL": "https://preview.redd.it/6rol9pc6hsye1.png?width=2461\u0026format=png\u0026auto=webp\u0026s=f65d811c4859178fe80cbcb50312217ba5591c5b",
      "imageDescription": "The image presents a side-by-side comparison of HTML code generated by three large language models (LLMs): QwQ 32b, Qwen 3 32b, and GLM-4-32B. The comparison is framed within a webpage context, suggesting an evaluation of their ability to produce functional and well-structured HTML for web development tasks.\n\nThe webpage itself has a simple layout with a header titled \"Our Services\" and two distinct content blocks below. The first block, labeled ‚ÄúHTML code Repair‚Äù, contains placeholder text indicating the task is to repair existing HTML code components. The second block, ‚ÄúVirus Removal‚Äù, describes a service for malware and virus removal.\n\nCrucially, the image focuses on the *code* generated to fulfill these service descriptions. Each model‚Äôs output is presented as a block of HTML code, likely intended to render the respective service description. The generated code is visible within rectangular boxes, allowing for direct visual comparison of syntax, structure, and verbosity.\n\nThe code blocks are lengthy, suggesting the models were prompted to generate relatively complex HTML structures. The presence of numerous `\u003cp\u003e` tags and potentially other structural elements (though details are obscured by the length) indicates that the models attempted to create complete HTML snippets.\n\nNotably, there is a significant amount of seemingly random characters and symbols interspersed within the \"Virus Removal\" code block for all three models. This appears to be an artifact of the generation process, potentially indicating issues with tokenization or output filtering. The repetition and lack of semantic meaning suggest it's not intended HTML content, but rather a failure in the model‚Äôs ability to produce clean code.\n\nThe image doesn't show any performance metrics or evaluation criteria, only the raw generated HTML code. The comparison is purely visual and relies on a user‚Äôs ability to assess the quality of the code based on its structure, readability, and correctness. The presence of the gibberish in the",
      "title": "QwQ 32b vs Qwen 3 32b vs GLM-4-32B - HTML coding ONLY comparison.",
      "entryID": "t3_1kenk4f",
      "processingTimeMs": 30986
    },
    {
      "imageURL": "https://preview.redd.it/3bwwfd4epsye1.png?width=3404\u0026format=png\u0026auto=webp\u0026s=adbb0bce2c13bc560499b0d3459329d16d0a3291",
      "imageDescription": "The image presents two line charts comparing the performance of different implementations of LLaMA (likely Large Language Model Meta AI) inference, specifically focusing on throughput versus KV Cache size. The post title indicates improvements in both \"ik\" and \"mainline llama.cpp\".\n\n**Chart 1 (Left):** This chart plots ‚ÄúThroughput (tokens/s)‚Äù on the y-axis against \"KV Cache Size (N_KV)\" on the x-axis, ranging from 0 to approximately 30,000.  Four lines are displayed, each representing a different configuration or implementation:\n\n*   **Red Line:** Shows consistently high throughput (around 40-50 tokens/s) with minimal variation across the entire KV Cache size range. This likely represents a baseline or highly optimized configuration.\n*   **Orange Line:** Starts at similar throughput to the red line, but exhibits a gradual decline in performance as KV Cache size increases. The decrease is relatively small, dropping to around 30 tokens/s at the maximum cache size.\n*   **Light Orange Line:** Similar to the orange line, but starts with slightly lower throughput and exhibits a more pronounced decline as KV Cache size increases.\n*   **Dark Orange Line:** Starts with the lowest throughput of the four lines and shows a significant decline in performance as KV Cache size increases.\n\n**Chart 2 (Right):** This chart mirrors the structure of the first, also plotting ‚ÄúThroughput (tokens/s)‚Äù against \"KV Cache Size (N_KV)\". However, the y-axis scale is different, ranging from approximately 5 to 10 tokens/s.  Four lines are also present:\n\n*   **Red Line:** Shows a relatively stable throughput around 8-9 tokens/s.\n*   **Orange Line:** Starts at a slightly lower throughput than the red line and exhibits a gradual decline as KV Cache size increases.\n*   **",
      "title": "LLaMA gotta go fast! Both ik and mainline llama.cpp just got faster!",
      "entryID": "t3_1keoint",
      "processingTimeMs": 31338
    },
    {
      "imageURL": "https://preview.redd.it/fjze9by1yoye1.jpeg?width=640\u0026crop=smart\u0026auto=webp\u0026s=7e0eb0e11cb1eca743ebbe8cd75e69b96fffd960",
      "imageDescription": "## Image Analysis: Qwen X Shipping Platform Post\n\nThe image appears to be a promotional graphic, likely intended for social media, advertising the availability of shipping for AI platforms ‚Äì specifically referencing Qwen X. The dominant visual element is two cartoon characters: a light-brown sloth and a brown bear.\n\nThe **sloth** is depicted with large, expressive eyes and clasped hands, suggesting anticipation or excitement. It's positioned on the left side of the image.\n\nThe **bear** is shown presenting a red flower to the sloth, wearing a plain white t-shirt. It has a slightly more reserved expression than the sloth. Both characters have simplified, rounded designs typical of sticker or emoji-style artwork.\n\nA significant portion of the lower half of the image is obscured by a repeating pattern of unreadable characters. This appears to be non-English text, possibly filler or decorative elements. The repetition suggests it isn't meaningful information intended for the viewer.\n\nThe overall aesthetic is playful and approachable, likely aiming to soften the technical complexity of deploying AI models. The use of cartoon characters suggests a focus on ease-of-use and accessibility for developers or users. The flower presentation could symbolize the \"delivery\" of AI capabilities through the shipping platform.\n\nThe context provided (\"shipping AI platforms\") indicates that Qwen X likely offers a service to facilitate the deployment and access of their models, potentially through APIs or pre-packaged solutions. The image serves as a lighthearted announcement of this availability. There are no charts, code snippets or UI elements visible in the image itself; it's purely a promotional graphic.",
      "title": "Apparently shipping AI platforms is a thing now as per this post from the Qwen X\n            account",
      "entryID": "t3_1kebauw",
      "processingTimeMs": 26969
    }
  ],
  "overallSummary": {
    "keyDevelopments": [
      {
        "text": "Qwen3 performance benchmarks across 50+ devices highlight critical insights for on-device AI deployment, with detailed metrics on speed, RAM utilization, and quantization levels (Q4_0, 1GB, 364MB, 1.71GB). The initiative addresses the need for standardized benchmarking infrastructure to evaluate model viability on diverse hardware (iOS, Android, Mac, Windows).",
        "itemID": "t3_1kepuli"
      },
      {
        "text": "LLaMA.cpp forks (ik_llama.cpp and mainline) achieve major speed boosts for Qwen3 MoE models, particularly with CUDA Flash Attention optimizations. The integration of SOTA iqN_k quantizations and hybrid CPU+GPU support underscores advancements in efficient inference for large-scale models like Qwen3 235B and 30B.",
        "itemID": "t3_1keoint"
      },
      {
        "text": "IBM Granite 4.0 introduces a hybrid Mamba-2/Transformer architecture, combining recurrent efficiency with transformer precision. The Tiny Preview model uses fine-grained MoE (7B total parameters, 1B active) and eliminates positional encoding (NoPE), demonstrating potential for long-context tasks while maintaining efficiency.",
        "itemID": "t3_1kedu0d"
      },
      {
        "text": "GLM-4-32B outperforms Qwen3 32B and QwQ 32B in HTML code generation, achieving a 9/10 layout quality score vs. 6/10 and 3/10 respectively. This highlights specialized model training for frontend development, with GLM-4-32B showing superior scalability in code complexity (1500+ lines vs. 250-310 lines).",
        "itemID": "t3_1kenk4f"
      },
      {
        "text": "Qwen3 30B exhibits performance degradation with KV quantization, revealing risks of repetition loops and quality loss. Users report better results without KV quantization, emphasizing the need for careful optimization strategies when deploying large models with memory constraints.",
        "itemID": "t3_1kewkno"
      },
      {
        "text": "Qwen3's reasoning capabilities appear to drive significant improvements over Qwen2.5, particularly in code porting, logical puzzles, and multi-turn tasks. However, dense Qwen3 models without reasoning still outperform Qwen2.5 in specific scenarios like code autocomplete, indicating mixed performance tradeoffs.",
        "itemID": "t3_1kegrce"
      }
    ]
  },
  "persona": {
    "name": "LocalLLaMa",
    "feedUrl": "https://reddit.com/r/localllama.rss",
    "topic": "AI Technology and Large Language Models",
    "personaIdentity": "an AI researcher and enthusiast who loves diving deep into technical details.",
    "basePromptTask": "Your job is to curate the best AI Technology and Large Language Models news items and create an engaging newsletter.",
    "summaryPromptTask": "Your task is to analyze multiple AI Technology and Large Language Models news items and create a comprehensive newsletter-style overview.",
    "focusAreas": [
      "New LLM models, runners or other infrastructure being released or open sourced",
      "Big AI lab news (OpenAI, Anthropic, etc.)",
      "Security news"
    ],
    "relevanceCriteria": [
      "Contains specific technical details or specifications",
      "Explains the significance and impact of the development",
      "Includes performance metrics, benchmarks, or comparisons",
      "Discusses novel approaches or techniques"
    ],
    "summaryAnalysis": [
      "Major advances in model architecture or capabilities",
      "Emerging trends in open-source LLM development",
      "Notable performance breakthroughs",
      "Industry impact and adoption patterns"
    ],
    "exclusionCriteria": [
      "Content entirely unrelated to AI/ML technology",
      "Questions that would be better suited for tech support",
      "Duplicate posts of already covered developments",
      "Random complaints without technical content",
      "Humor posts"
    ]
  },
  "runDate": "2025-05-21T20:15:47.127077853+10:00",
  "totalProcessingTime": 384599,
  "entryTotalProcessingTime": 197915,
  "imageTotalProcessingTime": 186684,
  "successRate": 1
}